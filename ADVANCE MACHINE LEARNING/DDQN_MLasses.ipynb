{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "HTxXel2knS5N",
    "outputId": "68ee0e39-8729-402d-b70e-ae97a97ab426"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting gym-super-mario-bros\n",
      "  Downloading gym_super_mario_bros-7.3.2-py2.py3-none-any.whl (198 kB)\n",
      "\u001b[K     |████████████████████████████████| 198 kB 4.2 MB/s \n",
      "\u001b[?25hCollecting nes-py>=8.1.2\n",
      "  Downloading nes_py-8.1.8.tar.gz (76 kB)\n",
      "\u001b[K     |████████████████████████████████| 76 kB 4.1 MB/s \n",
      "\u001b[?25hRequirement already satisfied: gym>=0.17.2 in /usr/local/lib/python3.7/dist-packages (from nes-py>=8.1.2->gym-super-mario-bros) (0.17.3)\n",
      "Requirement already satisfied: numpy>=1.18.5 in /usr/local/lib/python3.7/dist-packages (from nes-py>=8.1.2->gym-super-mario-bros) (1.21.6)\n",
      "Requirement already satisfied: pyglet<=1.5.11,>=1.4.0 in /usr/local/lib/python3.7/dist-packages (from nes-py>=8.1.2->gym-super-mario-bros) (1.5.0)\n",
      "Requirement already satisfied: tqdm>=4.48.2 in /usr/local/lib/python3.7/dist-packages (from nes-py>=8.1.2->gym-super-mario-bros) (4.64.0)\n",
      "Requirement already satisfied: scipy in /usr/local/lib/python3.7/dist-packages (from gym>=0.17.2->nes-py>=8.1.2->gym-super-mario-bros) (1.4.1)\n",
      "Requirement already satisfied: cloudpickle<1.7.0,>=1.2.0 in /usr/local/lib/python3.7/dist-packages (from gym>=0.17.2->nes-py>=8.1.2->gym-super-mario-bros) (1.3.0)\n",
      "Requirement already satisfied: future in /usr/local/lib/python3.7/dist-packages (from pyglet<=1.5.11,>=1.4.0->nes-py>=8.1.2->gym-super-mario-bros) (0.16.0)\n",
      "Building wheels for collected packages: nes-py\n",
      "  Building wheel for nes-py (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "  Created wheel for nes-py: filename=nes_py-8.1.8-cp37-cp37m-linux_x86_64.whl size=438321 sha256=7bda511ad4f1e642fa7696191acced05ddbd2cc728ca1f258ad05ec9e38dbb7b\n",
      "  Stored in directory: /root/.cache/pip/wheels/f2/05/1f/608f15ab43187096eb5f3087506419c2d9772e97000f3ba025\n",
      "Successfully built nes-py\n",
      "Installing collected packages: nes-py, gym-super-mario-bros\n",
      "Successfully installed gym-super-mario-bros-7.3.2 nes-py-8.1.8\n"
     ]
    }
   ],
   "source": [
    "!pip install gym-super-mario-bros"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "YHzxywsKk6h4"
   },
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "qhrVVgYvoHqo"
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "onzhJKGpnbuB"
   },
   "outputs": [],
   "source": [
    "# Gym is an OpenAI toolkit for RL\n",
    "import gym\n",
    "from gym.spaces import Box\n",
    "from gym.wrappers import FrameStack\n",
    "\n",
    "# Super Mario environment for OpenAI Gym\n",
    "import gym_super_mario_bros\n",
    "from gym_super_mario_bros.actions import SIMPLE_MOVEMENT\n",
    "\n",
    "# NES Emulator for OpenAI Gym\n",
    "from nes_py.wrappers import JoypadSpace\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "from torchvision import transforms as T\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "from collections import deque\n",
    "import random, datetime, os, copy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "FEb5N_Utzoa8",
    "outputId": "641175a8-1810-44a0-f06f-3131f2b316a6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(240, 256, 3),\n",
      " 0,\n",
      " False,\n",
      " {'coins': 0, 'flag_get': False, 'life': 2, 'score': 0, 'stage': 1, 'status': 'small', 'time': 400, 'world': 1, 'x_pos': 40, 'y_pos': 79}\n"
     ]
    }
   ],
   "source": [
    "# Initialize Super Mario environment\n",
    "env = gym_super_mario_bros.make('SuperMarioBros-v0')\n",
    "\n",
    "# Limit the action-space to\n",
    "env = JoypadSpace(env, SIMPLE_MOVEMENT)\n",
    "#   0. walk right\n",
    "#   1. jump right\n",
    "\n",
    "env.reset()\n",
    "next_state, reward, done, info = env.step(action=0)\n",
    "print(f\"{next_state.shape},\\n {reward},\\n {done},\\n {info}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oPgICa5XPeDw"
   },
   "source": [
    "Preprocess Environment\n",
    "------------------------\n",
    "\n",
    "Environment data is returned to the agent in ``next_state``. As you saw\n",
    "above, each state is represented by a ``[3, 240, 256]`` size array.\n",
    "Often that is more information than our agent needs; for instance,\n",
    "Mario’s actions do not depend on the color of the pipes or the sky!\n",
    "\n",
    "We use **Wrappers** to preprocess environment data before sending it to\n",
    "the agent.\n",
    "\n",
    "``GrayScaleObservation`` is a common wrapper to transform an RGB image\n",
    "to grayscale; doing so reduces the size of the state representation\n",
    "without losing useful information. Now the size of each state:\n",
    "``[1, 240, 256]``\n",
    "\n",
    "``ResizeObservation`` downsamples each observation into a square image.\n",
    "New size: ``[1, 84, 84]``\n",
    "\n",
    "``SkipFrame`` is a custom wrapper that inherits from ``gym.Wrapper`` and\n",
    "implements the ``step()`` function. Because consecutive frames don’t\n",
    "vary much, we can skip n-intermediate frames without losing much\n",
    "information. The n-th frame aggregates rewards accumulated over each\n",
    "skipped frame.\n",
    "\n",
    "``FrameStack`` is a wrapper that allows us to squash consecutive frames\n",
    "of the environment into a single observation point to feed to our\n",
    "learning model. This way, we can identify if Mario was landing or\n",
    "jumping based on the direction of his movement in the previous several\n",
    "frames.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8MFOcropPeEB"
   },
   "outputs": [],
   "source": [
    "class SkipFrame(gym.Wrapper):\n",
    "    def __init__(self, env, skip):\n",
    "        \"\"\"Return only every `skip`-th frame\"\"\"\n",
    "        super().__init__(env)\n",
    "        self._skip = skip\n",
    "\n",
    "    def step(self, action):\n",
    "        \"\"\"Repeat action, and sum reward\"\"\"\n",
    "        total_reward = 0.0\n",
    "        done = False\n",
    "        for i in range(self._skip):\n",
    "            # Accumulate reward and repeat the same action\n",
    "            obs, reward, done, info = self.env.step(action)\n",
    "            total_reward += reward\n",
    "            if done:\n",
    "                break\n",
    "        return obs, total_reward, done, info\n",
    "\n",
    "\n",
    "class GrayScaleObservation(gym.ObservationWrapper):\n",
    "    def __init__(self, env):\n",
    "        super().__init__(env)\n",
    "        obs_shape = self.observation_space.shape[:2]\n",
    "        self.observation_space = Box(low=0, high=255, shape=obs_shape, dtype=np.uint8)\n",
    "\n",
    "    def permute_orientation(self, observation):\n",
    "        # permute [H, W, C] array to [C, H, W] tensor\n",
    "        observation = np.transpose(observation, (2, 0, 1))\n",
    "        observation = torch.tensor(observation.copy(), dtype=torch.float)\n",
    "        return observation\n",
    "\n",
    "    def observation(self, observation):\n",
    "        observation = self.permute_orientation(observation)\n",
    "        transform = T.Grayscale()\n",
    "        observation = transform(observation)\n",
    "        return observation\n",
    "\n",
    "\n",
    "class ResizeObservation(gym.ObservationWrapper):\n",
    "    def __init__(self, env, shape):\n",
    "        super().__init__(env)\n",
    "        if isinstance(shape, int):\n",
    "            self.shape = (shape, shape)\n",
    "        else:\n",
    "            self.shape = tuple(shape)\n",
    "\n",
    "        obs_shape = self.shape + self.observation_space.shape[2:]\n",
    "        self.observation_space = Box(low=0, high=255, shape=obs_shape, dtype=np.uint8)\n",
    "\n",
    "    def observation(self, observation):\n",
    "        transforms = T.Compose(\n",
    "            [T.Resize(self.shape), T.Normalize(0, 255)]\n",
    "        )\n",
    "        observation = transforms(observation).squeeze(0)\n",
    "        return observation\n",
    "\n",
    "\n",
    "# Apply Wrappers to environment\n",
    "env = SkipFrame(env, skip=4)\n",
    "env = GrayScaleObservation(env)\n",
    "env = ResizeObservation(env, shape=84)\n",
    "env = FrameStack(env, num_stack=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SU-7CNipPeEN"
   },
   "source": [
    "After applying the above wrappers to the environment, the final wrapped\n",
    "state consists of 4 gray-scaled consecutive frames stacked together, as\n",
    "shown above in the image on the left. Each time Mario makes an action,\n",
    "the environment responds with a state of this structure. The structure\n",
    "is represented by a 3-D array of size ``[4, 84, 84]``.\n",
    "\n",
    ".. figure:: /_static/img/mario_env.png\n",
    "   :alt: picture\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uKSyjrPpPeEn"
   },
   "source": [
    "Agent\n",
    "\"\"\"\"\"\"\"\"\"\n",
    "\n",
    "We create a class ``Mario`` to represent our agent in the game. Mario\n",
    "should be able to:\n",
    "\n",
    "-  **Act** according to the optimal action policy based on the current\n",
    "   state (of the environment).\n",
    "\n",
    "-  **Remember** experiences. Experience = (current state, current\n",
    "   action, reward, next state). Mario *caches* and later *recalls* his\n",
    "   experiences to update his action policy.\n",
    "\n",
    "-  **Learn** a better action policy over time\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "zqO-4Lo3PeEx"
   },
   "outputs": [],
   "source": [
    "class Mario:\n",
    "    def __init__():\n",
    "        pass\n",
    "\n",
    "    def act(self, state):\n",
    "        \"\"\"Given a state, choose an epsilon-greedy action\"\"\"\n",
    "        pass\n",
    "\n",
    "    def cache(self, experience):\n",
    "        \"\"\"Add the experience to memory\"\"\"\n",
    "        pass\n",
    "\n",
    "    def recall(self):\n",
    "        \"\"\"Sample experiences from memory\"\"\"\n",
    "        pass\n",
    "\n",
    "    def learn(self):\n",
    "        \"\"\"Update online action value (Q) function with a batch of experiences\"\"\"\n",
    "        pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iDTRYjCIPeE1"
   },
   "source": [
    "In the following sections, we will populate Mario’s parameters and\n",
    "define his functions.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WdTcamAjPeE1"
   },
   "source": [
    "Act\n",
    "--------------\n",
    "\n",
    "For any given state, an agent can choose to do the most optimal action\n",
    "(**exploit**) or a random action (**explore**).\n",
    "\n",
    "Mario randomly explores with a chance of ``self.exploration_rate``; when\n",
    "he chooses to exploit, he relies on ``MarioNet`` (implemented in\n",
    "``Learn`` section) to provide the most optimal action.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "xetYvGsVPeE7"
   },
   "outputs": [],
   "source": [
    "class Mario:\n",
    "    def __init__(self, state_dim, action_dim, save_dir):\n",
    "        self.state_dim = state_dim\n",
    "        self.action_dim = action_dim\n",
    "        self.save_dir = save_dir\n",
    "\n",
    "        self.use_cuda = torch.cuda.is_available()\n",
    "\n",
    "        # Mario's DNN to predict the most optimal action - we implement this in the Learn section\n",
    "        self.net = MarioNet(self.state_dim, self.action_dim).float()\n",
    "        if self.use_cuda:\n",
    "            self.net = self.net.to(device=\"cuda\")\n",
    "\n",
    "        self.exploration_rate = 1\n",
    "        self.exploration_rate_decay = 0.99999975\n",
    "        self.exploration_rate_min = 0.1\n",
    "        self.curr_step = 0\n",
    "\n",
    "        self.save_every = 5e5  # no. of experiences between saving Mario Net\n",
    "\n",
    "    def act(self, state):\n",
    "        \"\"\"\n",
    "    Given a state, choose an epsilon-greedy action and update value of step.\n",
    "\n",
    "    Inputs:\n",
    "    state(LazyFrame): A single observation of the current state, dimension is (state_dim)\n",
    "    Outputs:\n",
    "    action_idx (int): An integer representing which action Mario will perform\n",
    "    \"\"\"\n",
    "        # EXPLORE\n",
    "        if np.random.rand() < self.exploration_rate:\n",
    "            action_idx = np.random.randint(self.action_dim)\n",
    "\n",
    "        # EXPLOIT\n",
    "        else:\n",
    "            state = state.__array__()\n",
    "            if self.use_cuda:\n",
    "                state = torch.tensor(state).cuda()\n",
    "            else:\n",
    "                state = torch.tensor(state)\n",
    "            state = state.unsqueeze(0)\n",
    "            action_values = self.net(state, model=\"online\")\n",
    "            action_idx = torch.argmax(action_values, axis=1).item()\n",
    "\n",
    "        # decrease exploration_rate\n",
    "        self.exploration_rate *= self.exploration_rate_decay\n",
    "        self.exploration_rate = max(self.exploration_rate_min, self.exploration_rate)\n",
    "\n",
    "        # increment step\n",
    "        self.curr_step += 1\n",
    "        return action_idx"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AKTTDWioPeE8"
   },
   "source": [
    "Cache and Recall\n",
    "----------------------\n",
    "\n",
    "These two functions serve as Mario’s “memory” process.\n",
    "\n",
    "``cache()``: Each time Mario performs an action, he stores the\n",
    "``experience`` to his memory. His experience includes the current\n",
    "*state*, *action* performed, *reward* from the action, the *next state*,\n",
    "and whether the game is *done*.\n",
    "\n",
    "``recall()``: Mario randomly samples a batch of experiences from his\n",
    "memory, and uses that to learn the game.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7IUiX2dIPeE9"
   },
   "outputs": [],
   "source": [
    "class Mario(Mario):  # subclassing for continuity\n",
    "    def __init__(self, state_dim, action_dim, save_dir):\n",
    "        super().__init__(state_dim, action_dim, save_dir)\n",
    "        self.memory = deque(maxlen=10000)\n",
    "        self.batch_size = 16\n",
    "\n",
    "    def cache(self, state, next_state, action, reward, done):\n",
    "        \"\"\"\n",
    "        Store the experience to self.memory (replay buffer)\n",
    "\n",
    "        Inputs:\n",
    "        state (LazyFrame),\n",
    "        next_state (LazyFrame),\n",
    "        action (int),\n",
    "        reward (float),\n",
    "        done(bool))\n",
    "        \"\"\"\n",
    "        state = state.__array__()\n",
    "        next_state = next_state.__array__()\n",
    "\n",
    "        if self.use_cuda:\n",
    "            state = torch.tensor(state).cuda()\n",
    "            next_state = torch.tensor(next_state).cuda()\n",
    "            action = torch.tensor([action]).cuda()\n",
    "            reward = torch.tensor([reward]).cuda()\n",
    "            done = torch.tensor([done]).cuda()\n",
    "        else:\n",
    "            state = torch.tensor(state)\n",
    "            next_state = torch.tensor(next_state)\n",
    "            action = torch.tensor([action])\n",
    "            reward = torch.tensor([reward])\n",
    "            done = torch.tensor([done])\n",
    "\n",
    "        self.memory.append((state, next_state, action, reward, done,))\n",
    "\n",
    "    def recall(self):\n",
    "        \"\"\"\n",
    "        Retrieve a batch of experiences from memory\n",
    "        \"\"\"\n",
    "        batch = random.sample(self.memory, self.batch_size)\n",
    "        state, next_state, action, reward, done = map(torch.stack, zip(*batch))\n",
    "        return state, next_state, action.squeeze(), reward.squeeze(), done.squeeze()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-QHSShiJPeE-"
   },
   "source": [
    "Learn\n",
    "--------------\n",
    "\n",
    "Mario uses the `DDQN algorithm <https://arxiv.org/pdf/1509.06461>`__\n",
    "under the hood. DDQN uses two ConvNets - $Q_{online}$ and\n",
    "$Q_{target}$ - that independently approximate the optimal\n",
    "action-value function.\n",
    "\n",
    "In our implementation, we share feature generator ``features`` across\n",
    "$Q_{online}$ and $Q_{target}$, but maintain separate FC\n",
    "classifiers for each. $\\theta_{target}$ (the parameters of\n",
    "$Q_{target}$) is frozen to prevent updation by backprop. Instead,\n",
    "it is periodically synced with $\\theta_{online}$ (more on this\n",
    "later).\n",
    "\n",
    "Neural Network\n",
    "~~~~~~~~~~~~~~~~~~\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "H4ey5CGgPeE_"
   },
   "outputs": [],
   "source": [
    "class MarioNet(nn.Module):\n",
    "    \"\"\"mini cnn structure\n",
    "  input -> (conv2d + relu) x 3 -> flatten -> (dense + relu) x 2 -> output\n",
    "  \"\"\"\n",
    "\n",
    "    def __init__(self, input_dim, output_dim):\n",
    "        super().__init__()\n",
    "        c, h, w = input_dim\n",
    "\n",
    "        if h != 84:\n",
    "            raise ValueError(f\"Expecting input height: 84, got: {h}\")\n",
    "        if w != 84:\n",
    "            raise ValueError(f\"Expecting input width: 84, got: {w}\")\n",
    "\n",
    "        self.online = nn.Sequential(\n",
    "            nn.Conv2d(in_channels=c, out_channels=32, kernel_size=8, stride=4),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(in_channels=32, out_channels=64, kernel_size=4, stride=2),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(in_channels=64, out_channels=64, kernel_size=3, stride=1),\n",
    "            nn.ReLU(),\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(3136, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512, output_dim),\n",
    "        )\n",
    "\n",
    "        self.target = copy.deepcopy(self.online)\n",
    "\n",
    "        # Q_target parameters are frozen.\n",
    "        for p in self.target.parameters():\n",
    "            p.requires_grad = False\n",
    "\n",
    "    def forward(self, input, model):\n",
    "        if model == \"online\":\n",
    "            return self.online(input)\n",
    "        elif model == \"target\":\n",
    "            return self.target(input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1Aom8m6iPeFC"
   },
   "outputs": [],
   "source": [
    "class Mario(Mario):\n",
    "    def __init__(self, state_dim, action_dim, save_dir):\n",
    "        super().__init__(state_dim, action_dim, save_dir)\n",
    "        self.gamma = 0.9\n",
    "\n",
    "    def td_estimate(self, state, action):\n",
    "        current_Q = self.net(state, model=\"online\")[\n",
    "            np.arange(0, self.batch_size), action\n",
    "        ]  # Q_online(s,a)\n",
    "        return current_Q\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def td_target(self, reward, next_state, done):\n",
    "        next_state_Q = self.net(next_state, model=\"online\")\n",
    "        best_action = torch.argmax(next_state_Q, axis=1)\n",
    "        next_Q = self.net(next_state, model=\"target\")[\n",
    "            np.arange(0, self.batch_size), best_action\n",
    "        ]\n",
    "        return (reward + (1 - done.float()) * self.gamma * next_Q).float()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "zubYsGyJPeFE"
   },
   "outputs": [],
   "source": [
    "class Mario(Mario):\n",
    "    def __init__(self, state_dim, action_dim, save_dir):\n",
    "        super().__init__(state_dim, action_dim, save_dir)\n",
    "        self.optimizer = torch.optim.Adam(self.net.parameters(), lr=0.00025)\n",
    "        self.loss_fn = torch.nn.SmoothL1Loss()\n",
    "\n",
    "    def update_Q_online(self, td_estimate, td_target):\n",
    "        loss = self.loss_fn(td_estimate, td_target)\n",
    "        self.optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        self.optimizer.step()\n",
    "        return loss.item()\n",
    "\n",
    "    def sync_Q_target(self):\n",
    "        self.net.target.load_state_dict(self.net.online.state_dict())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7jW__P_zPeFF"
   },
   "outputs": [],
   "source": [
    "class Mario(Mario):\n",
    "    def save(self):\n",
    "        save_path = (\n",
    "            self.save_dir / f\"mario_net_{int(self.curr_step // self.save_every)}.chkpt\"\n",
    "        )\n",
    "        torch.save(\n",
    "            dict(model=self.net.state_dict(), exploration_rate=self.exploration_rate),\n",
    "            save_path,\n",
    "        )\n",
    "        print(f\"MarioNet saved to {save_path} at step {self.curr_step}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "jffQ4t36PeFK"
   },
   "outputs": [],
   "source": [
    "class Mario(Mario):\n",
    "    def __init__(self, state_dim, action_dim, save_dir):\n",
    "        super().__init__(state_dim, action_dim, save_dir)\n",
    "        self.burnin = 1e4  # min. experiences before training\n",
    "        self.learn_every = 3  # no. of experiences between updates to Q_online\n",
    "        self.sync_every = 1e4  # no. of experiences between Q_target & Q_online sync\n",
    "\n",
    "    def learn(self):\n",
    "        if self.curr_step % self.sync_every == 0:\n",
    "            self.sync_Q_target()\n",
    "\n",
    "        if self.curr_step % self.save_every == 0:\n",
    "            self.save()\n",
    "\n",
    "        if self.curr_step < self.burnin:\n",
    "            return None, None\n",
    "\n",
    "        if self.curr_step % self.learn_every != 0:\n",
    "            return None, None\n",
    "\n",
    "        # Sample from memory\n",
    "        state, next_state, action, reward, done = self.recall()\n",
    "\n",
    "        # Get TD Estimate\n",
    "        td_est = self.td_estimate(state, action)\n",
    "\n",
    "        # Get TD Target\n",
    "        td_tgt = self.td_target(reward, next_state, done)\n",
    "\n",
    "        # Backpropagate loss through Q_online\n",
    "        loss = self.update_Q_online(td_est, td_tgt)\n",
    "\n",
    "        return (td_est.mean().item(), loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Sbp0w8qyPeFK"
   },
   "source": [
    "Logging\n",
    "--------------\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "yAATLnUpPeFL"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import time, datetime\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "class MetricLogger:\n",
    "    def __init__(self, save_dir):\n",
    "        self.save_log = save_dir / \"log\"\n",
    "        with open(self.save_log, \"w\") as f:\n",
    "            f.write(\n",
    "                f\"{'Episode':>8}{'Step':>8}{'Epsilon':>10}{'MeanReward':>15}\"\n",
    "                f\"{'MeanLength':>15}{'MeanLoss':>15}{'MeanQValue':>15}\"\n",
    "                f\"{'TimeDelta':>15}{'Time':>20}\\n\"\n",
    "            )\n",
    "        self.ep_rewards_plot = save_dir / \"reward_plot.jpg\"\n",
    "        self.ep_lengths_plot = save_dir / \"length_plot.jpg\"\n",
    "        self.ep_avg_losses_plot = save_dir / \"loss_plot.jpg\"\n",
    "        self.ep_avg_qs_plot = save_dir / \"q_plot.jpg\"\n",
    "\n",
    "        # History metrics\n",
    "        self.ep_rewards = []\n",
    "        self.ep_lengths = []\n",
    "        self.ep_avg_losses = []\n",
    "        self.ep_avg_qs = []\n",
    "\n",
    "        # Moving averages, added for every call to record()\n",
    "        self.moving_avg_ep_rewards = []\n",
    "        self.moving_avg_ep_lengths = []\n",
    "        self.moving_avg_ep_avg_losses = []\n",
    "        self.moving_avg_ep_avg_qs = []\n",
    "\n",
    "        # Current episode metric\n",
    "        self.init_episode()\n",
    "\n",
    "        # Timing\n",
    "        self.record_time = time.time()\n",
    "\n",
    "    def log_step(self, reward, loss, q):\n",
    "        self.curr_ep_reward += reward\n",
    "        self.curr_ep_length += 1\n",
    "        if loss:\n",
    "            self.curr_ep_loss += loss\n",
    "            self.curr_ep_q += q\n",
    "            self.curr_ep_loss_length += 1\n",
    "\n",
    "    def log_episode(self):\n",
    "        \"Mark end of episode\"\n",
    "        self.ep_rewards.append(self.curr_ep_reward)\n",
    "        self.ep_lengths.append(self.curr_ep_length)\n",
    "        if self.curr_ep_loss_length == 0:\n",
    "            ep_avg_loss = 0\n",
    "            ep_avg_q = 0\n",
    "        else:\n",
    "            ep_avg_loss = np.round(self.curr_ep_loss / self.curr_ep_loss_length, 5)\n",
    "            ep_avg_q = np.round(self.curr_ep_q / self.curr_ep_loss_length, 5)\n",
    "        self.ep_avg_losses.append(ep_avg_loss)\n",
    "        self.ep_avg_qs.append(ep_avg_q)\n",
    "\n",
    "        self.init_episode()\n",
    "\n",
    "    def init_episode(self):\n",
    "        self.curr_ep_reward = 0.0\n",
    "        self.curr_ep_length = 0\n",
    "        self.curr_ep_loss = 0.0\n",
    "        self.curr_ep_q = 0.0\n",
    "        self.curr_ep_loss_length = 0\n",
    "\n",
    "    def record(self, episode, epsilon, step):\n",
    "        mean_ep_reward = np.round(np.mean(self.ep_rewards[-100:]), 3)\n",
    "        mean_ep_length = np.round(np.mean(self.ep_lengths[-100:]), 3)\n",
    "        mean_ep_loss = np.round(np.mean(self.ep_avg_losses[-100:]), 3)\n",
    "        mean_ep_q = np.round(np.mean(self.ep_avg_qs[-100:]), 3)\n",
    "        self.moving_avg_ep_rewards.append(mean_ep_reward)\n",
    "        self.moving_avg_ep_lengths.append(mean_ep_length)\n",
    "        self.moving_avg_ep_avg_losses.append(mean_ep_loss)\n",
    "        self.moving_avg_ep_avg_qs.append(mean_ep_q)\n",
    "\n",
    "        last_record_time = self.record_time\n",
    "        self.record_time = time.time()\n",
    "        time_since_last_record = np.round(self.record_time - last_record_time, 3)\n",
    "\n",
    "        print(\n",
    "            f\"Episode {episode} - \"\n",
    "            f\"Step {step} - \"\n",
    "            f\"Epsilon {epsilon} - \\n\"\n",
    "            f\"Mean Reward {mean_ep_reward} - \"\n",
    "            f\"Mean Length {mean_ep_length} - \"\n",
    "            f\"Mean Loss {mean_ep_loss} - \"\n",
    "            f\"Mean Q Value {mean_ep_q} - \"\n",
    "            #f\"Time Delta {time_since_last_record} - \"\n",
    "            #f\"Time {datetime.datetime.now().strftime('%Y-%m-%dT%H:%M:%S')}\"\n",
    "        )\n",
    "\n",
    "        with open(self.save_log, \"a\") as f:\n",
    "            f.write(\n",
    "                f\"{episode:8d}{step:8d}{epsilon:10.3f}\"\n",
    "                f\"{mean_ep_reward:15.3f}{mean_ep_length:15.3f}{mean_ep_loss:15.3f}{mean_ep_q:15.3f}\\n\"\n",
    "                #f\"{time_since_last_record:15.3f}\"\n",
    "                #f\"{datetime.datetime.now().strftime('%Y-%m-%dT%H:%M:%S'):>20}\\n\"\n",
    "            )\n",
    "\n",
    "        for metric in [\"ep_rewards\", \"ep_lengths\", \"ep_avg_losses\", \"ep_avg_qs\"]:\n",
    "            plt.plot(getattr(self, f\"moving_avg_{metric}\"))\n",
    "            plt.savefig(getattr(self, f\"{metric}_plot\"))\n",
    "            plt.clf()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Axj_vosRPeFM"
   },
   "source": [
    "Let’s play!\n",
    "\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\n",
    "\n",
    "In this example we run the training loop for 10 episodes, but for Mario to truly learn the ways of\n",
    "his world, we suggest running the loop for at least 40,000 episodes!\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "quzZm58pPeFM",
    "outputId": "b71130c0-98e8-437b-ab06-463012870136"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using CUDA: True\n",
      "\n",
      "Episode 0 - Step 304 - Epsilon 0.9999240028784168 - \n",
      "Mean Reward 1450.0 - Mean Length 304.0 - Mean Loss 0.0 - Mean Q Value 0.0 - \n",
      "Episode 1 - Step 1916 - Epsilon 0.9995211146422713 - \n",
      "Mean Reward 1851.5 - Mean Length 958.0 - Mean Loss 0.0 - Mean Q Value 0.0 - \n",
      "Episode 2 - Step 2704 - Epsilon 0.9993242283519851 - \n",
      "Mean Reward 1680.667 - Mean Length 901.333 - Mean Loss 0.0 - Mean Q Value 0.0 - \n",
      "Episode 3 - Step 3349 - Epsilon 0.9991631002912994 - \n",
      "Mean Reward 1666.5 - Mean Length 837.25 - Mean Loss 0.0 - Mean Q Value 0.0 - \n",
      "Episode 4 - Step 3552 - Epsilon 0.9991123940442962 - \n",
      "Mean Reward 1547.6 - Mean Length 710.4 - Mean Loss 0.0 - Mean Q Value 0.0 - \n",
      "Episode 5 - Step 4911 - Epsilon 0.9987730032232289 - \n",
      "Mean Reward 1711.833 - Mean Length 818.5 - Mean Loss 0.0 - Mean Q Value 0.0 - \n",
      "Episode 6 - Step 5372 - Epsilon 0.9986579012530823 - \n",
      "Mean Reward 1724.143 - Mean Length 767.429 - Mean Loss 0.0 - Mean Q Value 0.0 - \n",
      "Episode 7 - Step 5825 - Epsilon 0.9985448096355465 - \n",
      "Mean Reward 1737.25 - Mean Length 728.125 - Mean Loss 0.0 - Mean Q Value 0.0 - \n",
      "Episode 8 - Step 7925 - Epsilon 0.9980207111327951 - \n",
      "Mean Reward 1621.444 - Mean Length 880.556 - Mean Loss 0.0 - Mean Q Value 0.0 - \n",
      "Episode 9 - Step 8756 - Epsilon 0.9978133938399416 - \n",
      "Mean Reward 1649.4 - Mean Length 875.6 - Mean Loss 0.0 - Mean Q Value 0.0 - \n",
      "Episode 10 - Step 11607 - Epsilon 0.9971024556452128 - \n",
      "Mean Reward 1676.0 - Mean Length 1055.182 - Mean Loss 0.172 - Mean Q Value 0.026 - \n",
      "Episode 11 - Step 14164 - Epsilon 0.9964652615055785 - \n",
      "Mean Reward 1688.667 - Mean Length 1180.333 - Mean Loss 0.252 - Mean Q Value 0.133 - \n",
      "Episode 12 - Step 14822 - Epsilon 0.9963013564311114 - \n",
      "Mean Reward 1743.154 - Mean Length 1140.154 - Mean Loss 0.3 - Mean Q Value 0.201 - \n",
      "Episode 13 - Step 15662 - Epsilon 0.9960921550869916 - \n",
      "Mean Reward 1725.0 - Mean Length 1118.714 - Mean Loss 0.341 - Mean Q Value 0.265 - \n",
      "Episode 14 - Step 17394 - Epsilon 0.9956609404945732 - \n",
      "Mean Reward 1693.133 - Mean Length 1159.6 - Mean Loss 0.376 - Mean Q Value 0.313 - \n",
      "Episode 15 - Step 18409 - Epsilon 0.9954083235514413 - \n",
      "Mean Reward 1667.625 - Mean Length 1150.562 - Mean Loss 0.409 - Mean Q Value 0.358 - \n",
      "Episode 16 - Step 19686 - Epsilon 0.995090590125282 - \n",
      "Mean Reward 1693.235 - Mean Length 1158.0 - Mean Loss 0.438 - Mean Q Value 0.399 - \n",
      "Episode 17 - Step 19818 - Epsilon 0.9950577526735194 - \n",
      "Mean Reward 1637.278 - Mean Length 1101.0 - Mean Loss 0.474 - Mean Q Value 0.442 - \n",
      "Episode 18 - Step 21390 - Epsilon 0.9946667717605737 - \n",
      "Mean Reward 1644.526 - Mean Length 1125.789 - Mean Loss 0.505 - Mean Q Value 0.564 - \n",
      "Episode 19 - Step 21823 - Epsilon 0.9945591048966311 - \n",
      "Mean Reward 1612.6 - Mean Length 1091.15 - Mean Loss 0.532 - Mean Q Value 0.699 - \n",
      "Episode 20 - Step 22469 - Epsilon 0.9943984965505643 - \n",
      "Mean Reward 1609.762 - Mean Length 1069.952 - Mean Loss 0.557 - Mean Q Value 0.818 - \n",
      "Episode 21 - Step 22747 - Epsilon 0.994329388247947 - \n",
      "Mean Reward 1601.227 - Mean Length 1033.955 - Mean Loss 0.58 - Mean Q Value 0.928 - \n",
      "Episode 22 - Step 22971 - Epsilon 0.9942737073543166 - \n",
      "Mean Reward 1579.0 - Mean Length 998.739 - Mean Loss 0.599 - Mean Q Value 1.031 - \n",
      "Episode 23 - Step 23371 - Epsilon 0.9941742849423428 - \n",
      "Mean Reward 1574.208 - Mean Length 973.792 - Mean Loss 0.62 - Mean Q Value 1.152 - \n",
      "Episode 24 - Step 23916 - Epsilon 0.9940388379066079 - \n",
      "Mean Reward 1566.24 - Mean Length 956.64 - Mean Loss 0.641 - Mean Q Value 1.272 - \n",
      "Episode 25 - Step 24304 - Epsilon 0.9939424208035708 - \n",
      "Mean Reward 1562.5 - Mean Length 934.769 - Mean Loss 0.662 - Mean Q Value 1.399 - \n",
      "Episode 26 - Step 25454 - Epsilon 0.9936567033956799 - \n",
      "Mean Reward 1589.111 - Mean Length 942.741 - Mean Loss 0.682 - Mean Q Value 1.516 - \n",
      "Episode 27 - Step 27701 - Epsilon 0.9930986734240596 - \n",
      "Mean Reward 1596.464 - Mean Length 989.321 - Mean Loss 0.699 - Mean Q Value 1.621 - \n",
      "Episode 28 - Step 28958 - Epsilon 0.9927866411575275 - \n",
      "Mean Reward 1608.241 - Mean Length 998.552 - Mean Loss 0.714 - Mean Q Value 1.718 - \n",
      "Episode 29 - Step 29444 - Epsilon 0.9926660248931194 - \n",
      "Mean Reward 1615.533 - Mean Length 981.467 - Mean Loss 0.73 - Mean Q Value 1.816 - \n",
      "Episode 30 - Step 29589 - Epsilon 0.992630041397419 - \n",
      "Mean Reward 1584.645 - Mean Length 954.484 - Mean Loss 0.743 - Mean Q Value 1.907 - \n",
      "Episode 31 - Step 32483 - Epsilon 0.991912133207231 - \n",
      "Mean Reward 1591.938 - Mean Length 1015.094 - Mean Loss 0.759 - Mean Q Value 2.016 - \n",
      "Episode 32 - Step 32624 - Epsilon 0.9918771689164093 - \n",
      "Mean Reward 1563.879 - Mean Length 988.606 - Mean Loss 0.771 - Mean Q Value 2.112 - \n",
      "Episode 33 - Step 33192 - Epsilon 0.9917363323404322 - \n",
      "Mean Reward 1558.5 - Mean Length 976.235 - Mean Loss 0.779 - Mean Q Value 2.186 - \n",
      "Episode 34 - Step 34072 - Epsilon 0.991518174318279 - \n",
      "Mean Reward 1564.4 - Mean Length 973.486 - Mean Loss 0.788 - Mean Q Value 2.254 - \n",
      "Episode 35 - Step 34867 - Epsilon 0.991321129638439 - \n",
      "Mean Reward 1570.194 - Mean Length 968.528 - Mean Loss 0.795 - Mean Q Value 2.311 - \n",
      "Episode 36 - Step 35699 - Epsilon 0.9911149562604434 - \n",
      "Mean Reward 1582.838 - Mean Length 964.838 - Mean Loss 0.801 - Mean Q Value 2.367 - \n",
      "Episode 37 - Step 36342 - Epsilon 0.990955647316088 - \n",
      "Mean Reward 1596.263 - Mean Length 956.368 - Mean Loss 0.808 - Mean Q Value 2.429 - \n",
      "Episode 38 - Step 37404 - Epsilon 0.9906925834820752 - \n",
      "Mean Reward 1609.256 - Mean Length 959.077 - Mean Loss 0.816 - Mean Q Value 2.501 - \n",
      "Episode 39 - Step 38957 - Epsilon 0.990308021696302 - \n",
      "Mean Reward 1591.825 - Mean Length 973.925 - Mean Loss 0.823 - Mean Q Value 2.57 - \n",
      "Episode 40 - Step 40946 - Epsilon 0.9898157133812038 - \n",
      "Mean Reward 1624.463 - Mean Length 998.683 - Mean Loss 0.831 - Mean Q Value 2.642 - \n",
      "Episode 41 - Step 41700 - Epsilon 0.9896291506799725 - \n",
      "Mean Reward 1621.333 - Mean Length 992.857 - Mean Loss 0.844 - Mean Q Value 2.736 - \n",
      "Episode 42 - Step 44441 - Epsilon 0.9889512395154716 - \n",
      "Mean Reward 1617.558 - Mean Length 1033.512 - Mean Loss 0.852 - Mean Q Value 2.817 - \n",
      "Episode 43 - Step 46474 - Epsilon 0.9884487326954683 - \n",
      "Mean Reward 1619.068 - Mean Length 1056.227 - Mean Loss 0.856 - Mean Q Value 2.867 - \n",
      "Episode 44 - Step 49634 - Epsilon 0.9876681664632162 - \n",
      "Mean Reward 1620.689 - Mean Length 1102.978 - Mean Loss 0.856 - Mean Q Value 2.882 - \n",
      "Episode 45 - Step 49925 - Epsilon 0.9875963162087021 - \n",
      "Mean Reward 1607.196 - Mean Length 1085.326 - Mean Loss 0.853 - Mean Q Value 2.892 - \n",
      "Episode 46 - Step 51196 - Epsilon 0.9872825572910512 - \n",
      "Mean Reward 1603.064 - Mean Length 1089.277 - Mean Loss 0.856 - Mean Q Value 2.922 - \n",
      "Episode 47 - Step 51723 - Epsilon 0.9871524913661329 - \n",
      "Mean Reward 1601.062 - Mean Length 1077.562 - Mean Loss 0.858 - Mean Q Value 2.932 - \n",
      "Episode 48 - Step 52559 - Epsilon 0.9869461980280263 - \n",
      "Mean Reward 1597.735 - Mean Length 1072.633 - Mean Loss 0.862 - Mean Q Value 2.957 - \n",
      "Episode 49 - Step 52889 - Epsilon 0.9868647783151093 - \n",
      "Mean Reward 1594.8 - Mean Length 1057.78 - Mean Loss 0.864 - Mean Q Value 2.994 - \n",
      "Episode 50 - Step 53316 - Epsilon 0.9867594361095815 - \n",
      "Mean Reward 1586.51 - Mean Length 1045.412 - Mean Loss 0.868 - Mean Q Value 3.033 - \n",
      "Episode 51 - Step 54160 - Epsilon 0.9865512518067265 - \n",
      "Mean Reward 1585.327 - Mean Length 1041.538 - Mean Loss 0.872 - Mean Q Value 3.072 - \n",
      "Episode 52 - Step 56606 - Epsilon 0.9859481600548715 - \n",
      "Mean Reward 1592.981 - Mean Length 1068.038 - Mean Loss 0.876 - Mean Q Value 3.119 - \n",
      "Episode 53 - Step 56745 - Epsilon 0.9859138989473125 - \n",
      "Mean Reward 1576.519 - Mean Length 1050.833 - Mean Loss 0.879 - Mean Q Value 3.16 - \n",
      "Episode 54 - Step 58893 - Epsilon 0.9853846052454193 - \n",
      "Mean Reward 1584.236 - Mean Length 1070.782 - Mean Loss 0.884 - Mean Q Value 3.218 - \n",
      "Episode 55 - Step 59799 - Epsilon 0.9851614408787235 - \n",
      "Mean Reward 1582.036 - Mean Length 1067.839 - Mean Loss 0.888 - Mean Q Value 3.273 - \n",
      "Episode 56 - Step 61202 - Epsilon 0.9848159560530452 - \n",
      "Mean Reward 1590.368 - Mean Length 1073.719 - Mean Loss 0.896 - Mean Q Value 3.344 - \n",
      "Episode 57 - Step 62115 - Epsilon 0.9845911974345029 - \n",
      "Mean Reward 1604.069 - Mean Length 1070.948 - Mean Loss 0.902 - Mean Q Value 3.411 - \n",
      "Episode 58 - Step 62503 - Epsilon 0.9844956967082612 - \n",
      "Mean Reward 1600.322 - Mean Length 1059.373 - Mean Loss 0.908 - Mean Q Value 3.487 - \n",
      "Episode 59 - Step 62698 - Epsilon 0.98444770370688 - \n",
      "Mean Reward 1584.683 - Mean Length 1044.967 - Mean Loss 0.915 - Mean Q Value 3.562 - \n",
      "Episode 60 - Step 63702 - Epsilon 0.9842006383103363 - \n",
      "Mean Reward 1593.098 - Mean Length 1044.295 - Mean Loss 0.921 - Mean Q Value 3.63 - \n",
      "Episode 61 - Step 64096 - Epsilon 0.984103699309657 - \n",
      "Mean Reward 1594.113 - Mean Length 1033.806 - Mean Loss 0.927 - Mean Q Value 3.699 - \n",
      "Episode 62 - Step 65531 - Epsilon 0.9837507153836358 - \n",
      "Mean Reward 1601.524 - Mean Length 1040.175 - Mean Loss 0.934 - Mean Q Value 3.773 - \n",
      "Episode 63 - Step 66488 - Epsilon 0.9835153811485109 - \n",
      "Mean Reward 1608.078 - Mean Length 1038.875 - Mean Loss 0.939 - Mean Q Value 3.85 - \n",
      "Episode 64 - Step 67829 - Epsilon 0.9831857128394651 - \n",
      "Mean Reward 1610.662 - Mean Length 1043.523 - Mean Loss 0.944 - Mean Q Value 3.926 - \n",
      "Episode 65 - Step 67961 - Epsilon 0.9831532682422203 - \n",
      "Mean Reward 1596.061 - Mean Length 1029.712 - Mean Loss 0.946 - Mean Q Value 3.995 - \n",
      "Episode 66 - Step 69853 - Epsilon 0.9826883466507867 - \n",
      "Mean Reward 1614.567 - Mean Length 1042.582 - Mean Loss 0.952 - Mean Q Value 4.072 - \n",
      "Episode 67 - Step 70439 - Epsilon 0.9825443933348251 - \n",
      "Mean Reward 1619.235 - Mean Length 1035.868 - Mean Loss 0.962 - Mean Q Value 4.161 - \n",
      "Episode 68 - Step 70895 - Epsilon 0.9824323896443001 - \n",
      "Mean Reward 1616.754 - Mean Length 1027.464 - Mean Loss 0.973 - Mean Q Value 4.263 - \n",
      "Episode 69 - Step 71601 - Epsilon 0.9822590056074243 - \n",
      "Mean Reward 1619.029 - Mean Length 1022.871 - Mean Loss 0.983 - Mean Q Value 4.364 - \n",
      "Episode 70 - Step 72157 - Epsilon 0.9821224810772348 - \n",
      "Mean Reward 1618.155 - Mean Length 1016.296 - Mean Loss 0.994 - Mean Q Value 4.463 - \n",
      "Episode 71 - Step 72439 - Epsilon 0.9820532438742944 - \n",
      "Mean Reward 1609.986 - Mean Length 1006.097 - Mean Loss 1.005 - Mean Q Value 4.569 - \n",
      "Episode 72 - Step 73999 - Epsilon 0.9816703177367152 - \n",
      "Mean Reward 1614.849 - Mean Length 1013.685 - Mean Loss 1.012 - Mean Q Value 4.653 - \n",
      "Episode 73 - Step 74815 - Epsilon 0.9814700773920484 - \n",
      "Mean Reward 1616.257 - Mean Length 1011.014 - Mean Loss 1.021 - Mean Q Value 4.73 - \n",
      "Episode 74 - Step 75926 - Epsilon 0.981197511898229 - \n",
      "Mean Reward 1611.52 - Mean Length 1012.347 - Mean Loss 1.029 - Mean Q Value 4.808 - \n",
      "Episode 75 - Step 77085 - Epsilon 0.9809132510678599 - \n",
      "Mean Reward 1609.408 - Mean Length 1014.276 - Mean Loss 1.037 - Mean Q Value 4.881 - \n",
      "Episode 76 - Step 77520 - Epsilon 0.9808065825386646 - \n",
      "Mean Reward 1612.636 - Mean Length 1006.753 - Mean Loss 1.043 - Mean Q Value 4.949 - \n",
      "Episode 77 - Step 79281 - Epsilon 0.9803748774227403 - \n",
      "Mean Reward 1613.308 - Mean Length 1016.423 - Mean Loss 1.05 - Mean Q Value 5.017 - \n",
      "Episode 78 - Step 81721 - Epsilon 0.9797770310343843 - \n",
      "Mean Reward 1615.405 - Mean Length 1034.443 - Mean Loss 1.055 - Mean Q Value 5.073 - \n",
      "Episode 79 - Step 82983 - Epsilon 0.9794679601009189 - \n",
      "Mean Reward 1611.375 - Mean Length 1037.287 - Mean Loss 1.058 - Mean Q Value 5.103 - \n",
      "Episode 80 - Step 83349 - Epsilon 0.9793783428714056 - \n",
      "Mean Reward 1610.852 - Mean Length 1029.0 - Mean Loss 1.061 - Mean Q Value 5.14 - \n",
      "Episode 81 - Step 85745 - Epsilon 0.9787918708365446 - \n",
      "Mean Reward 1608.89 - Mean Length 1045.671 - Mean Loss 1.064 - Mean Q Value 5.162 - \n",
      "Episode 82 - Step 87078 - Epsilon 0.9784657427488901 - \n",
      "Mean Reward 1611.88 - Mean Length 1049.133 - Mean Loss 1.066 - Mean Q Value 5.183 - \n",
      "Episode 83 - Step 88986 - Epsilon 0.9779991258281899 - \n",
      "Mean Reward 1631.155 - Mean Length 1059.357 - Mean Loss 1.068 - Mean Q Value 5.198 - \n",
      "Episode 84 - Step 90683 - Epsilon 0.977584297648795 - \n",
      "Mean Reward 1638.165 - Mean Length 1066.859 - Mean Loss 1.07 - Mean Q Value 5.221 - \n",
      "Episode 85 - Step 92129 - Epsilon 0.9772309647497476 - \n",
      "Mean Reward 1642.605 - Mean Length 1071.267 - Mean Loss 1.073 - Mean Q Value 5.257 - \n",
      "Episode 86 - Step 92510 - Epsilon 0.9771378879215624 - \n",
      "Mean Reward 1639.839 - Mean Length 1063.333 - Mean Loss 1.077 - Mean Q Value 5.3 - \n",
      "Episode 87 - Step 93137 - Epsilon 0.9769847335422553 - \n",
      "Mean Reward 1637.193 - Mean Length 1058.375 - Mean Loss 1.082 - Mean Q Value 5.342 - \n",
      "Episode 88 - Step 94430 - Epsilon 0.9766689742249729 - \n",
      "Mean Reward 1641.09 - Mean Length 1061.011 - Mean Loss 1.086 - Mean Q Value 5.384 - \n",
      "Episode 89 - Step 95350 - Epsilon 0.9764443661637113 - \n",
      "Mean Reward 1646.911 - Mean Length 1059.444 - Mean Loss 1.09 - Mean Q Value 5.438 - \n",
      "Episode 90 - Step 96366 - Epsilon 0.9761963807591522 - \n",
      "Mean Reward 1642.835 - Mean Length 1058.967 - Mean Loss 1.095 - Mean Q Value 5.491 - \n",
      "Episode 91 - Step 97952 - Epsilon 0.9758093955706373 - \n",
      "Mean Reward 1639.565 - Mean Length 1064.696 - Mean Loss 1.099 - Mean Q Value 5.542 - \n",
      "Episode 92 - Step 98618 - Epsilon 0.9756469368110111 - \n",
      "Mean Reward 1633.656 - Mean Length 1060.409 - Mean Loss 1.102 - Mean Q Value 5.59 - \n",
      "Episode 93 - Step 99329 - Epsilon 0.9754735309581942 - \n",
      "Mean Reward 1630.894 - Mean Length 1056.691 - Mean Loss 1.106 - Mean Q Value 5.637 - \n",
      "Episode 94 - Step 99893 - Epsilon 0.9753359988693585 - \n",
      "Mean Reward 1627.926 - Mean Length 1051.505 - Mean Loss 1.111 - Mean Q Value 5.685 - \n",
      "Episode 95 - Step 100925 - Epsilon 0.9750843946085125 - \n",
      "Mean Reward 1637.896 - Mean Length 1051.302 - Mean Loss 1.119 - Mean Q Value 5.758 - \n",
      "Episode 96 - Step 101415 - Epsilon 0.9749649540711083 - \n",
      "Mean Reward 1638.526 - Mean Length 1045.515 - Mean Loss 1.127 - Mean Q Value 5.832 - \n",
      "Episode 97 - Step 104381 - Epsilon 0.9742422854297402 - \n",
      "Mean Reward 1647.949 - Mean Length 1065.112 - Mean Loss 1.134 - Mean Q Value 5.899 - \n",
      "Episode 98 - Step 105184 - Epsilon 0.9740467258964122 - \n",
      "Mean Reward 1646.939 - Mean Length 1062.465 - Mean Loss 1.139 - Mean Q Value 5.951 - \n",
      "Episode 99 - Step 105715 - Epsilon 0.9739174297595913 - \n",
      "Mean Reward 1644.9 - Mean Length 1057.15 - Mean Loss 1.144 - Mean Q Value 5.999 - \n",
      "Episode 100 - Step 107126 - Epsilon 0.973573940929664 - \n",
      "Mean Reward 1645.37 - Mean Length 1068.22 - Mean Loss 1.161 - Mean Q Value 6.112 - \n",
      "Episode 101 - Step 108982 - Epsilon 0.9731223073516493 - \n",
      "Mean Reward 1645.5 - Mean Length 1070.66 - Mean Loss 1.178 - Mean Q Value 6.231 - \n",
      "Episode 102 - Step 110634 - Epsilon 0.9727204907692666 - \n",
      "Mean Reward 1647.41 - Mean Length 1079.3 - Mean Loss 1.194 - Mean Q Value 6.338 - \n",
      "Episode 103 - Step 111747 - Epsilon 0.9724498689108555 - \n",
      "Mean Reward 1651.26 - Mean Length 1083.98 - Mean Loss 1.21 - Mean Q Value 6.44 - \n",
      "Episode 104 - Step 112241 - Epsilon 0.9723297787527369 - \n",
      "Mean Reward 1659.71 - Mean Length 1086.89 - Mean Loss 1.226 - Mean Q Value 6.532 - \n",
      "Episode 105 - Step 113646 - Epsilon 0.9719883078495561 - \n",
      "Mean Reward 1654.22 - Mean Length 1087.35 - Mean Loss 1.244 - Mean Q Value 6.647 - \n",
      "Episode 106 - Step 113771 - Epsilon 0.9719579336857337 - \n",
      "Mean Reward 1643.14 - Mean Length 1083.99 - Mean Loss 1.262 - Mean Q Value 6.763 - \n",
      "Episode 107 - Step 114359 - Epsilon 0.9718150663526098 - \n",
      "Mean Reward 1644.38 - Mean Length 1085.34 - Mean Loss 1.279 - Mean Q Value 6.889 - \n",
      "Episode 108 - Step 115130 - Epsilon 0.9716277670266861 - \n",
      "Mean Reward 1656.29 - Mean Length 1072.05 - Mean Loss 1.299 - Mean Q Value 7.016 - \n",
      "Episode 109 - Step 116115 - Epsilon 0.9713885331160026 - \n",
      "Mean Reward 1657.1 - Mean Length 1073.59 - Mean Loss 1.318 - Mean Q Value 7.148 - \n",
      "Episode 110 - Step 116860 - Epsilon 0.9712076288263064 - \n",
      "Mean Reward 1659.64 - Mean Length 1052.53 - Mean Loss 1.318 - Mean Q Value 7.28 - \n",
      "Episode 111 - Step 117902 - Epsilon 0.9709546621576793 - \n",
      "Mean Reward 1654.84 - Mean Length 1037.38 - Mean Loss 1.326 - Mean Q Value 7.405 - \n",
      "Episode 112 - Step 118841 - Epsilon 0.9707267572736011 - \n",
      "Mean Reward 1649.17 - Mean Length 1040.19 - Mean Loss 1.337 - Mean Q Value 7.542 - \n",
      "Episode 113 - Step 119458 - Epsilon 0.9705770342002442 - \n",
      "Mean Reward 1656.45 - Mean Length 1037.96 - Mean Loss 1.347 - Mean Q Value 7.683 - \n",
      "Episode 114 - Step 119840 - Epsilon 0.9704843485076917 - \n",
      "Mean Reward 1658.66 - Mean Length 1024.46 - Mean Loss 1.361 - Mean Q Value 7.83 - \n",
      "Episode 115 - Step 120595 - Epsilon 0.970301186850415 - \n",
      "Mean Reward 1660.13 - Mean Length 1021.86 - Mean Loss 1.377 - Mean Q Value 7.991 - \n",
      "Episode 116 - Step 121797 - Epsilon 0.9700096551121222 - \n",
      "Mean Reward 1658.18 - Mean Length 1021.11 - Mean Loss 1.392 - Mean Q Value 8.15 - \n",
      "Episode 117 - Step 121941 - Epsilon 0.9699747353887271 - \n",
      "Mean Reward 1657.68 - Mean Length 1021.23 - Mean Loss 1.402 - Mean Q Value 8.302 - \n",
      "Episode 118 - Step 122577 - Epsilon 0.9698205216468148 - \n",
      "Mean Reward 1653.33 - Mean Length 1011.87 - Mean Loss 1.414 - Mean Q Value 8.441 - \n",
      "Episode 119 - Step 123660 - Epsilon 0.9695579782511408 - \n",
      "Mean Reward 1659.78 - Mean Length 1018.37 - Mean Loss 1.424 - Mean Q Value 8.568 - \n",
      "Episode 120 - Step 123911 - Epsilon 0.9694971403891999 - \n",
      "Mean Reward 1655.99 - Mean Length 1014.42 - Mean Loss 1.436 - Mean Q Value 8.7 - \n",
      "Episode 121 - Step 125655 - Epsilon 0.9690745317184899 - \n",
      "Mean Reward 1656.76 - Mean Length 1029.08 - Mean Loss 1.447 - Mean Q Value 8.825 - \n",
      "Episode 122 - Step 127418 - Epsilon 0.9686475061778644 - \n",
      "Mean Reward 1666.1 - Mean Length 1044.47 - Mean Loss 1.457 - Mean Q Value 8.932 - \n",
      "Episode 123 - Step 128025 - Epsilon 0.9685005250528833 - \n",
      "Mean Reward 1668.92 - Mean Length 1046.54 - Mean Loss 1.464 - Mean Q Value 9.032 - \n",
      "Episode 124 - Step 128495 - Epsilon 0.968386732912369 - \n",
      "Mean Reward 1669.33 - Mean Length 1045.79 - Mean Loss 1.471 - Mean Q Value 9.135 - \n",
      "Episode 125 - Step 128882 - Epsilon 0.968293046016412 - \n",
      "Mean Reward 1672.5 - Mean Length 1045.78 - Mean Loss 1.479 - Mean Q Value 9.24 - \n",
      "Episode 126 - Step 130660 - Epsilon 0.9678627353472397 - \n",
      "Mean Reward 1671.75 - Mean Length 1052.06 - Mean Loss 1.486 - Mean Q Value 9.333 - \n",
      "Episode 127 - Step 132912 - Epsilon 0.9673179819215468 - \n",
      "Mean Reward 1674.91 - Mean Length 1052.11 - Mean Loss 1.492 - Mean Q Value 9.419 - \n",
      "Episode 128 - Step 133943 - Epsilon 0.9670686878096688 - \n",
      "Mean Reward 1673.43 - Mean Length 1049.85 - Mean Loss 1.496 - Mean Q Value 9.496 - \n",
      "Episode 129 - Step 134227 - Epsilon 0.967000028361682 - \n",
      "Mean Reward 1665.44 - Mean Length 1047.83 - Mean Loss 1.5 - Mean Q Value 9.553 - \n",
      "Episode 130 - Step 134980 - Epsilon 0.9668180077167958 - \n",
      "Mean Reward 1670.54 - Mean Length 1053.91 - Mean Loss 1.507 - Mean Q Value 9.63 - \n",
      "Episode 131 - Step 136299 - Epsilon 0.9664992519966007 - \n",
      "Mean Reward 1670.98 - Mean Length 1038.16 - Mean Loss 1.514 - Mean Q Value 9.712 - \n",
      "Episode 132 - Step 137398 - Epsilon 0.966233742769885 - \n",
      "Mean Reward 1687.37 - Mean Length 1047.74 - Mean Loss 1.521 - Mean Q Value 9.793 - \n",
      "Episode 133 - Step 137711 - Epsilon 0.9661581379281302 - \n",
      "Mean Reward 1687.53 - Mean Length 1045.19 - Mean Loss 1.53 - Mean Q Value 9.892 - \n",
      "Episode 134 - Step 138035 - Epsilon 0.966079882278562 - \n",
      "Mean Reward 1681.91 - Mean Length 1039.63 - Mean Loss 1.54 - Mean Q Value 9.994 - \n",
      "Episode 135 - Step 138911 - Epsilon 0.9658683339232605 - \n",
      "Mean Reward 1681.77 - Mean Length 1040.44 - Mean Loss 1.55 - Mean Q Value 10.1 - \n",
      "Episode 136 - Step 139963 - Epsilon 0.9656143439208034 - \n",
      "Mean Reward 1681.34 - Mean Length 1042.64 - Mean Loss 1.561 - Mean Q Value 10.195 - \n",
      "Episode 137 - Step 141088 - Epsilon 0.9653428030398211 - \n",
      "Mean Reward 1688.41 - Mean Length 1047.46 - Mean Loss 1.572 - Mean Q Value 10.292 - \n",
      "Episode 138 - Step 142740 - Epsilon 0.9649441987297396 - \n",
      "Mean Reward 1687.67 - Mean Length 1053.36 - Mean Loss 1.583 - Mean Q Value 10.396 - \n",
      "Episode 139 - Step 144090 - Epsilon 0.9646185849723332 - \n",
      "Mean Reward 1699.73 - Mean Length 1051.33 - Mean Loss 1.595 - Mean Q Value 10.504 - \n",
      "Episode 140 - Step 145229 - Epsilon 0.9643439488990608 - \n",
      "Mean Reward 1684.96 - Mean Length 1042.83 - Mean Loss 1.606 - Mean Q Value 10.598 - \n",
      "Episode 141 - Step 146119 - Epsilon 0.9641294062123426 - \n",
      "Mean Reward 1680.5 - Mean Length 1044.19 - Mean Loss 1.615 - Mean Q Value 10.684 - \n",
      "Episode 142 - Step 146524 - Epsilon 0.9640317930394986 - \n",
      "Mean Reward 1677.97 - Mean Length 1020.83 - Mean Loss 1.625 - Mean Q Value 10.777 - \n",
      "Episode 143 - Step 147701 - Epsilon 0.9637481683792285 - \n",
      "Mean Reward 1675.28 - Mean Length 1012.27 - Mean Loss 1.636 - Mean Q Value 10.877 - \n",
      "Episode 144 - Step 148469 - Epsilon 0.9635631464704183 - \n",
      "Mean Reward 1676.2 - Mean Length 988.35 - Mean Loss 1.647 - Mean Q Value 10.976 - \n",
      "Episode 145 - Step 149486 - Epsilon 0.9633181916509754 - \n",
      "Mean Reward 1677.25 - Mean Length 995.61 - Mean Loss 1.66 - Mean Q Value 11.075 - \n",
      "Episode 146 - Step 151149 - Epsilon 0.962917775305004 - \n",
      "Mean Reward 1682.63 - Mean Length 999.53 - Mean Loss 1.669 - Mean Q Value 11.16 - \n",
      "Episode 147 - Step 153356 - Epsilon 0.9623866318986731 - \n",
      "Mean Reward 1681.72 - Mean Length 1016.33 - Mean Loss 1.677 - Mean Q Value 11.244 - \n",
      "Episode 148 - Step 154043 - Epsilon 0.9622213561674213 - \n",
      "Mean Reward 1683.28 - Mean Length 1014.84 - Mean Loss 1.685 - Mean Q Value 11.315 - \n",
      "Episode 149 - Step 155243 - Epsilon 0.961932733020089 - \n",
      "Mean Reward 1690.32 - Mean Length 1023.54 - Mean Loss 1.694 - Mean Q Value 11.386 - \n",
      "Episode 150 - Step 155653 - Epsilon 0.9618341399555981 - \n",
      "Mean Reward 1688.41 - Mean Length 1023.37 - Mean Loss 1.699 - Mean Q Value 11.449 - \n",
      "Episode 151 - Step 156338 - Epsilon 0.9616694399413616 - \n",
      "Mean Reward 1696.79 - Mean Length 1021.78 - Mean Loss 1.707 - Mean Q Value 11.527 - \n",
      "Episode 152 - Step 157424 - Epsilon 0.9614083820959544 - \n",
      "Mean Reward 1708.31 - Mean Length 1008.18 - Mean Loss 1.715 - Mean Q Value 11.599 - \n",
      "Episode 153 - Step 157765 - Epsilon 0.9613264255145731 - \n",
      "Mean Reward 1715.69 - Mean Length 1010.2 - Mean Loss 1.723 - Mean Q Value 11.681 - \n",
      "Episode 154 - Step 159053 - Epsilon 0.9610169281985703 - \n",
      "Mean Reward 1714.21 - Mean Length 1001.6 - Mean Loss 1.731 - Mean Q Value 11.761 - \n",
      "Episode 155 - Step 160934 - Epsilon 0.960565116172175 - \n",
      "Mean Reward 1724.8 - Mean Length 1011.35 - Mean Loss 1.74 - Mean Q Value 11.847 - \n",
      "Episode 156 - Step 162412 - Epsilon 0.9602102528823723 - \n",
      "Mean Reward 1724.6 - Mean Length 1012.1 - Mean Loss 1.748 - Mean Q Value 11.928 - \n",
      "Episode 157 - Step 162850 - Epsilon 0.9601051156028961 - \n",
      "Mean Reward 1718.85 - Mean Length 1007.35 - Mean Loss 1.758 - Mean Q Value 12.024 - \n",
      "Episode 158 - Step 163418 - Epsilon 0.9599687903387434 - \n",
      "Mean Reward 1725.03 - Mean Length 1009.15 - Mean Loss 1.771 - Mean Q Value 12.126 - \n",
      "Episode 159 - Step 163994 - Epsilon 0.9598305647681165 - \n",
      "Mean Reward 1733.13 - Mean Length 1012.96 - Mean Loss 1.782 - Mean Q Value 12.233 - \n",
      "Episode 160 - Step 164370 - Epsilon 0.9597403449241364 - \n",
      "Mean Reward 1726.68 - Mean Length 1006.68 - Mean Loss 1.792 - Mean Q Value 12.346 - \n",
      "Episode 161 - Step 164705 - Epsilon 0.9596599700259373 - \n",
      "Mean Reward 1724.72 - Mean Length 1006.09 - Mean Loss 1.803 - Mean Q Value 12.462 - \n",
      "Episode 162 - Step 165845 - Epsilon 0.959386505870751 - \n",
      "Mean Reward 1717.64 - Mean Length 1003.14 - Mean Loss 1.815 - Mean Q Value 12.58 - \n",
      "Episode 163 - Step 166396 - Epsilon 0.9592543594648233 - \n",
      "Mean Reward 1718.09 - Mean Length 999.08 - Mean Loss 1.826 - Mean Q Value 12.679 - \n",
      "Episode 164 - Step 167601 - Epsilon 0.9589654275754255 - \n",
      "Mean Reward 1713.2 - Mean Length 997.72 - Mean Loss 1.836 - Mean Q Value 12.773 - \n",
      "Episode 165 - Step 168456 - Epsilon 0.9587604705951905 - \n",
      "Mean Reward 1727.07 - Mean Length 1004.95 - Mean Loss 1.846 - Mean Q Value 12.86 - \n",
      "Episode 166 - Step 168704 - Epsilon 0.958701029281275 - \n",
      "Mean Reward 1709.79 - Mean Length 988.51 - Mean Loss 1.857 - Mean Q Value 12.967 - \n",
      "Episode 167 - Step 168844 - Epsilon 0.9586674753282488 - \n",
      "Mean Reward 1697.0 - Mean Length 984.05 - Mean Loss 1.865 - Mean Q Value 13.057 - \n",
      "Episode 168 - Step 169595 - Epsilon 0.9584875023827233 - \n",
      "Mean Reward 1702.44 - Mean Length 987.0 - Mean Loss 1.872 - Mean Q Value 13.136 - \n",
      "Episode 169 - Step 171071 - Epsilon 0.9581338856963815 - \n",
      "Mean Reward 1702.39 - Mean Length 994.7 - Mean Loss 1.881 - Mean Q Value 13.219 - \n",
      "Episode 170 - Step 172115 - Epsilon 0.9578838453526097 - \n",
      "Mean Reward 1697.46 - Mean Length 999.58 - Mean Loss 1.889 - Mean Q Value 13.291 - \n",
      "Episode 171 - Step 173247 - Epsilon 0.9576128025448235 - \n",
      "Mean Reward 1697.69 - Mean Length 1008.08 - Mean Loss 1.894 - Mean Q Value 13.348 - \n",
      "Episode 172 - Step 174404 - Epsilon 0.9573358530627796 - \n",
      "Mean Reward 1706.74 - Mean Length 1004.05 - Mean Loss 1.901 - Mean Q Value 13.42 - \n",
      "Episode 173 - Step 174679 - Epsilon 0.9572700384770478 - \n",
      "Mean Reward 1699.58 - Mean Length 998.64 - Mean Loss 1.906 - Mean Q Value 13.485 - \n",
      "Episode 174 - Step 175543 - Epsilon 0.9570632904524546 - \n",
      "Mean Reward 1700.11 - Mean Length 996.17 - Mean Loss 1.912 - Mean Q Value 13.542 - \n",
      "Episode 175 - Step 176198 - Epsilon 0.9569065841497117 - \n",
      "Mean Reward 1701.73 - Mean Length 991.13 - Mean Loss 1.92 - Mean Q Value 13.611 - \n",
      "Episode 176 - Step 176951 - Epsilon 0.9567264634171017 - \n",
      "Mean Reward 1699.52 - Mean Length 994.31 - Mean Loss 1.928 - Mean Q Value 13.683 - \n",
      "Episode 177 - Step 178622 - Epsilon 0.9563268743569819 - \n",
      "Mean Reward 1705.31 - Mean Length 993.41 - Mean Loss 1.936 - Mean Q Value 13.75 - \n",
      "Episode 178 - Step 178982 - Epsilon 0.9562408088005273 - \n",
      "Mean Reward 1705.23 - Mean Length 972.61 - Mean Loss 1.943 - Mean Q Value 13.82 - \n",
      "Episode 179 - Step 180486 - Epsilon 0.9558813297977211 - \n",
      "Mean Reward 1711.5 - Mean Length 975.03 - Mean Loss 1.952 - Mean Q Value 13.907 - \n",
      "Episode 180 - Step 180926 - Epsilon 0.9557761886211568 - \n",
      "Mean Reward 1713.68 - Mean Length 975.77 - Mean Loss 1.961 - Mean Q Value 13.987 - \n",
      "Episode 181 - Step 182438 - Epsilon 0.9554149734506238 - \n",
      "Mean Reward 1721.6 - Mean Length 966.93 - Mean Loss 1.971 - Mean Q Value 14.094 - \n",
      "Episode 182 - Step 182727 - Epsilon 0.955345947203757 - \n",
      "Mean Reward 1717.11 - Mean Length 956.49 - Mean Loss 1.985 - Mean Q Value 14.227 - \n",
      "Episode 183 - Step 184321 - Epsilon 0.9549653176416424 - \n",
      "Mean Reward 1701.06 - Mean Length 953.35 - Mean Loss 1.996 - Mean Q Value 14.343 - \n",
      "Episode 184 - Step 184823 - Epsilon 0.9548454769994394 - \n",
      "Mean Reward 1694.34 - Mean Length 941.4 - Mean Loss 2.005 - Mean Q Value 14.437 - \n",
      "Episode 185 - Step 185313 - Epsilon 0.9547285155779031 - \n",
      "Mean Reward 1691.34 - Mean Length 931.84 - Mean Loss 2.011 - Mean Q Value 14.532 - \n",
      "Episode 186 - Step 187704 - Epsilon 0.9541579970670292 - \n",
      "Mean Reward 1693.75 - Mean Length 951.94 - Mean Loss 2.018 - Mean Q Value 14.604 - \n",
      "Episode 187 - Step 189519 - Epsilon 0.9537251460321998 - \n",
      "Mean Reward 1700.91 - Mean Length 963.82 - Mean Loss 2.024 - Mean Q Value 14.663 - \n",
      "Episode 188 - Step 190748 - Epsilon 0.9534321589568617 - \n",
      "Mean Reward 1699.69 - Mean Length 963.18 - Mean Loss 2.029 - Mean Q Value 14.71 - \n",
      "Episode 189 - Step 192257 - Epsilon 0.9530725444664417 - \n",
      "Mean Reward 1694.31 - Mean Length 969.07 - Mean Loss 2.031 - Mean Q Value 14.728 - \n",
      "Episode 190 - Step 193857 - Epsilon 0.9526913916366045 - \n",
      "Mean Reward 1696.63 - Mean Length 974.91 - Mean Loss 2.033 - Mean Q Value 14.737 - \n",
      "Episode 191 - Step 195281 - Epsilon 0.9523522938217397 - \n",
      "Mean Reward 1704.29 - Mean Length 973.29 - Mean Loss 2.035 - Mean Q Value 14.748 - \n",
      "Episode 192 - Step 195876 - Epsilon 0.9522106419359277 - \n",
      "Mean Reward 1708.9 - Mean Length 972.58 - Mean Loss 2.035 - Mean Q Value 14.743 - \n",
      "Episode 193 - Step 196931 - Epsilon 0.9519595294646085 - \n",
      "Mean Reward 1711.01 - Mean Length 976.02 - Mean Loss 2.037 - Mean Q Value 14.758 - \n",
      "Episode 194 - Step 199035 - Epsilon 0.9514589303585719 - \n",
      "Mean Reward 1715.67 - Mean Length 991.42 - Mean Loss 2.038 - Mean Q Value 14.771 - \n",
      "Episode 195 - Step 199237 - Epsilon 0.9514108828897849 - \n",
      "Mean Reward 1700.11 - Mean Length 983.12 - Mean Loss 2.037 - Mean Q Value 14.761 - \n",
      "Episode 196 - Step 200888 - Epsilon 0.9510182690299996 - \n",
      "Mean Reward 1695.3 - Mean Length 994.73 - Mean Loss 2.034 - Mean Q Value 14.748 - \n",
      "Episode 197 - Step 201461 - Epsilon 0.9508820454031641 - \n",
      "Mean Reward 1683.52 - Mean Length 970.8 - Mean Loss 2.034 - Mean Q Value 14.735 - \n",
      "Episode 198 - Step 203741 - Epsilon 0.9503401970109376 - \n",
      "Mean Reward 1690.4 - Mean Length 985.57 - Mean Loss 2.036 - Mean Q Value 14.746 - \n",
      "Episode 199 - Step 205836 - Epsilon 0.9498425865935852 - \n",
      "Mean Reward 1683.9 - Mean Length 1001.21 - Mean Loss 2.037 - Mean Q Value 14.748 - \n",
      "Episode 200 - Step 206756 - Epsilon 0.9496241478927465 - \n",
      "Mean Reward 1697.59 - Mean Length 996.3 - Mean Loss 2.036 - Mean Q Value 14.737 - \n",
      "Episode 201 - Step 208862 - Epsilon 0.9491243023122258 - \n",
      "Mean Reward 1683.8 - Mean Length 998.8 - Mean Loss 2.035 - Mean Q Value 14.727 - \n",
      "Episode 202 - Step 209526 - Epsilon 0.9489667607346405 - \n",
      "Mean Reward 1684.15 - Mean Length 988.92 - Mean Loss 2.034 - Mean Q Value 14.711 - \n",
      "Episode 203 - Step 211168 - Epsilon 0.948577289775003 - \n",
      "Mean Reward 1682.14 - Mean Length 994.21 - Mean Loss 2.035 - Mean Q Value 14.719 - \n",
      "Episode 204 - Step 212314 - Epsilon 0.9483055612744425 - \n",
      "Mean Reward 1679.5 - Mean Length 1000.73 - Mean Loss 2.035 - Mean Q Value 14.731 - \n",
      "Episode 205 - Step 213310 - Epsilon 0.9480694625556504 - \n",
      "Mean Reward 1678.44 - Mean Length 996.64 - Mean Loss 2.035 - Mean Q Value 14.721 - \n",
      "Episode 206 - Step 215268 - Epsilon 0.9476054960607313 - \n",
      "Mean Reward 1691.21 - Mean Length 1014.97 - Mean Loss 2.034 - Mean Q Value 14.715 - \n",
      "Episode 207 - Step 216749 - Epsilon 0.94725471002519 - \n",
      "Mean Reward 1690.3 - Mean Length 1023.9 - Mean Loss 2.035 - Mean Q Value 14.705 - \n",
      "Episode 208 - Step 217160 - Epsilon 0.9471573845917356 - \n",
      "Mean Reward 1685.65 - Mean Length 1020.3 - Mean Loss 2.033 - Mean Q Value 14.689 - \n",
      "Episode 209 - Step 217417 - Epsilon 0.9470965316770814 - \n",
      "Mean Reward 1676.58 - Mean Length 1013.02 - Mean Loss 2.032 - Mean Q Value 14.665 - \n",
      "Episode 210 - Step 218128 - Epsilon 0.9469282002084131 - \n",
      "Mean Reward 1670.38 - Mean Length 1012.68 - Mean Loss 2.031 - Mean Q Value 14.656 - \n",
      "Episode 211 - Step 218570 - Epsilon 0.9468235704100996 - \n",
      "Mean Reward 1674.98 - Mean Length 1006.68 - Mean Loss 2.033 - Mean Q Value 14.672 - \n",
      "Episode 212 - Step 218841 - Epsilon 0.9467594252781194 - \n",
      "Mean Reward 1667.42 - Mean Length 1000.0 - Mean Loss 2.036 - Mean Q Value 14.682 - \n",
      "Episode 213 - Step 219766 - Epsilon 0.9465405124464009 - \n",
      "Mean Reward 1661.07 - Mean Length 1003.08 - Mean Loss 2.037 - Mean Q Value 14.694 - \n",
      "Episode 214 - Step 219948 - Epsilon 0.9464974458274683 - \n",
      "Mean Reward 1657.32 - Mean Length 1001.08 - Mean Loss 2.04 - Mean Q Value 14.702 - \n",
      "Episode 215 - Step 221812 - Epsilon 0.9460564807153816 - \n",
      "Mean Reward 1663.09 - Mean Length 1012.17 - Mean Loss 2.038 - Mean Q Value 14.696 - \n",
      "Episode 216 - Step 222588 - Epsilon 0.9458729635369 - \n",
      "Mean Reward 1662.01 - Mean Length 1007.91 - Mean Loss 2.035 - Mean Q Value 14.694 - \n",
      "Episode 217 - Step 222873 - Epsilon 0.9458055724806496 - \n",
      "Mean Reward 1670.04 - Mean Length 1009.32 - Mean Loss 2.037 - Mean Q Value 14.693 - \n",
      "Episode 218 - Step 223024 - Epsilon 0.9457698689897284 - \n",
      "Mean Reward 1663.28 - Mean Length 1004.47 - Mean Loss 2.042 - Mean Q Value 14.708 - \n",
      "Episode 219 - Step 227574 - Epsilon 0.9446946672661107 - \n",
      "Mean Reward 1667.65 - Mean Length 1039.14 - Mean Loss 2.042 - Mean Q Value 14.721 - \n",
      "Episode 220 - Step 228185 - Epsilon 0.9445503761581437 - \n",
      "Mean Reward 1671.73 - Mean Length 1042.74 - Mean Loss 2.039 - Mean Q Value 14.717 - \n",
      "Episode 221 - Step 229815 - Epsilon 0.9441655502453074 - \n",
      "Mean Reward 1678.95 - Mean Length 1041.6 - Mean Loss 2.036 - Mean Q Value 14.702 - \n",
      "Episode 222 - Step 230143 - Epsilon 0.9440881318346976 - \n",
      "Mean Reward 1674.15 - Mean Length 1027.25 - Mean Loss 2.034 - Mean Q Value 14.684 - \n",
      "Episode 223 - Step 230903 - Epsilon 0.9439087721069178 - \n",
      "Mean Reward 1682.38 - Mean Length 1028.78 - Mean Loss 2.035 - Mean Q Value 14.682 - \n",
      "Episode 224 - Step 232864 - Epsilon 0.9434461341868853 - \n",
      "Mean Reward 1678.92 - Mean Length 1043.69 - Mean Loss 2.036 - Mean Q Value 14.674 - \n",
      "Episode 225 - Step 233228 - Epsilon 0.9433602844841521 - \n",
      "Mean Reward 1675.62 - Mean Length 1043.46 - Mean Loss 2.033 - Mean Q Value 14.644 - \n",
      "Episode 226 - Step 233671 - Epsilon 0.9432558131047802 - \n",
      "Mean Reward 1667.6 - Mean Length 1030.11 - Mean Loss 2.033 - Mean Q Value 14.631 - \n",
      "Episode 227 - Step 234444 - Epsilon 0.9430735465081688 - \n",
      "Mean Reward 1657.68 - Mean Length 1015.32 - Mean Loss 2.034 - Mean Q Value 14.643 - \n",
      "Episode 228 - Step 234893 - Epsilon 0.942967692430498 - \n",
      "Mean Reward 1654.33 - Mean Length 1009.5 - Mean Loss 2.04 - Mean Q Value 14.671 - \n",
      "Episode 229 - Step 235510 - Epsilon 0.9428222508632091 - \n",
      "Mean Reward 1660.28 - Mean Length 1012.83 - Mean Loss 2.045 - Mean Q Value 14.723 - \n",
      "Episode 230 - Step 236576 - Epsilon 0.9425710221796253 - \n",
      "Mean Reward 1658.52 - Mean Length 1015.96 - Mean Loss 2.05 - Mean Q Value 14.76 - \n",
      "Episode 231 - Step 237072 - Epsilon 0.9424541506044373 - \n",
      "Mean Reward 1657.55 - Mean Length 1007.73 - Mean Loss 2.054 - Mean Q Value 14.8 - \n",
      "Episode 232 - Step 237515 - Epsilon 0.9423497795738501 - \n",
      "Mean Reward 1653.11 - Mean Length 1001.17 - Mean Loss 2.061 - Mean Q Value 14.86 - \n",
      "Episode 233 - Step 237795 - Epsilon 0.9422838173897291 - \n",
      "Mean Reward 1649.96 - Mean Length 1000.84 - Mean Loss 2.066 - Mean Q Value 14.891 - \n",
      "Episode 234 - Step 238629 - Epsilon 0.9420873716693984 - \n",
      "Mean Reward 1657.54 - Mean Length 1005.94 - Mean Loss 2.072 - Mean Q Value 14.938 - \n",
      "Episode 235 - Step 241923 - Epsilon 0.9413118819734937 - \n",
      "Mean Reward 1655.71 - Mean Length 1030.12 - Mean Loss 2.076 - Mean Q Value 14.965 - \n",
      "Episode 236 - Step 242519 - Epsilon 0.9411716369340447 - \n",
      "Mean Reward 1647.63 - Mean Length 1025.56 - Mean Loss 2.079 - Mean Q Value 14.997 - \n",
      "Episode 237 - Step 243595 - Epsilon 0.9409184957810428 - \n",
      "Mean Reward 1632.5 - Mean Length 1025.07 - Mean Loss 2.081 - Mean Q Value 15.018 - \n",
      "Episode 238 - Step 244229 - Epsilon 0.9407693719991724 - \n",
      "Mean Reward 1631.47 - Mean Length 1014.89 - Mean Loss 2.082 - Mean Q Value 15.017 - \n",
      "Episode 239 - Step 245808 - Epsilon 0.9403980765323768 - \n",
      "Mean Reward 1623.61 - Mean Length 1017.18 - Mean Loss 2.08 - Mean Q Value 15.002 - \n",
      "Episode 240 - Step 246208 - Epsilon 0.9403040414147904 - \n",
      "Mean Reward 1624.83 - Mean Length 1009.79 - Mean Loss 2.08 - Mean Q Value 15.009 - \n",
      "Episode 241 - Step 246518 - Epsilon 0.94023117066624 - \n",
      "Mean Reward 1629.85 - Mean Length 1003.99 - Mean Loss 2.079 - Mean Q Value 15.001 - \n",
      "Episode 242 - Step 246757 - Epsilon 0.9401749935250711 - \n",
      "Mean Reward 1629.85 - Mean Length 1002.33 - Mean Loss 2.076 - Mean Q Value 15.006 - \n",
      "Episode 243 - Step 249543 - Epsilon 0.9395203895524606 - \n",
      "Mean Reward 1636.65 - Mean Length 1018.42 - Mean Loss 2.073 - Mean Q Value 14.983 - \n",
      "Episode 244 - Step 250835 - Epsilon 0.939216973433007 - \n",
      "Mean Reward 1643.91 - Mean Length 1023.66 - Mean Loss 2.073 - Mean Q Value 14.98 - \n",
      "Episode 245 - Step 251328 - Epsilon 0.9391012220598711 - \n",
      "Mean Reward 1650.22 - Mean Length 1018.42 - Mean Loss 2.073 - Mean Q Value 14.997 - \n",
      "Episode 246 - Step 253343 - Epsilon 0.9386282688951517 - \n",
      "Mean Reward 1641.56 - Mean Length 1021.94 - Mean Loss 2.075 - Mean Q Value 15.014 - \n",
      "Episode 247 - Step 254372 - Epsilon 0.9383868377981726 - \n",
      "Mean Reward 1649.04 - Mean Length 1010.16 - Mean Loss 2.076 - Mean Q Value 15.034 - \n",
      "Episode 248 - Step 254794 - Epsilon 0.9382878431964541 - \n",
      "Mean Reward 1647.62 - Mean Length 1007.51 - Mean Loss 2.078 - Mean Q Value 15.069 - \n",
      "Episode 249 - Step 256625 - Epsilon 0.9378584401696136 - \n",
      "Mean Reward 1642.08 - Mean Length 1013.82 - Mean Loss 2.08 - Mean Q Value 15.097 - \n",
      "Episode 250 - Step 259448 - Epsilon 0.9371967800034586 - \n",
      "Mean Reward 1650.0 - Mean Length 1037.95 - Mean Loss 2.082 - Mean Q Value 15.099 - \n",
      "Episode 251 - Step 261222 - Epsilon 0.9367812253355841 - \n",
      "Mean Reward 1642.15 - Mean Length 1048.84 - Mean Loss 2.079 - Mean Q Value 15.082 - \n",
      "Episode 252 - Step 261665 - Epsilon 0.9366774825467593 - \n",
      "Mean Reward 1624.68 - Mean Length 1042.41 - Mean Loss 2.073 - Mean Q Value 15.051 - \n",
      "Episode 253 - Step 263040 - Epsilon 0.9363555549563234 - \n",
      "Mean Reward 1628.8 - Mean Length 1052.75 - Mean Loss 2.069 - Mean Q Value 15.027 - \n",
      "Episode 254 - Step 263258 - Epsilon 0.9363045249627724 - \n",
      "Mean Reward 1620.54 - Mean Length 1042.05 - Mean Loss 2.065 - Mean Q Value 15.009 - \n",
      "Episode 255 - Step 263540 - Epsilon 0.9362385178122817 - \n",
      "Mean Reward 1607.47 - Mean Length 1026.06 - Mean Loss 2.062 - Mean Q Value 14.991 - \n",
      "Episode 256 - Step 265481 - Epsilon 0.935784318223758 - \n",
      "Mean Reward 1618.73 - Mean Length 1030.69 - Mean Loss 2.058 - Mean Q Value 14.959 - \n",
      "Episode 257 - Step 266737 - Epsilon 0.9354905280385438 - \n",
      "Mean Reward 1617.88 - Mean Length 1038.87 - Mean Loss 2.051 - Mean Q Value 14.921 - \n",
      "Episode 258 - Step 267641 - Epsilon 0.9352791310415133 - \n",
      "Mean Reward 1615.84 - Mean Length 1042.23 - Mean Loss 2.041 - Mean Q Value 14.866 - \n",
      "Episode 259 - Step 268907 - Epsilon 0.9349831619990727 - \n",
      "Mean Reward 1624.23 - Mean Length 1049.13 - Mean Loss 2.035 - Mean Q Value 14.822 - \n",
      "Episode 260 - Step 270487 - Epsilon 0.9346139165346565 - \n",
      "Mean Reward 1623.71 - Mean Length 1061.17 - Mean Loss 2.031 - Mean Q Value 14.793 - \n",
      "Episode 261 - Step 270912 - Epsilon 0.9345146190688703 - \n",
      "Mean Reward 1624.91 - Mean Length 1062.07 - Mean Loss 2.03 - Mean Q Value 14.785 - \n",
      "Episode 262 - Step 272385 - Epsilon 0.9341705473734313 - \n",
      "Mean Reward 1632.03 - Mean Length 1065.4 - Mean Loss 2.027 - Mean Q Value 14.766 - \n",
      "Episode 263 - Step 272774 - Epsilon 0.9340797036936765 - \n",
      "Mean Reward 1626.0 - Mean Length 1063.78 - Mean Loss 2.026 - Mean Q Value 14.765 - \n",
      "Episode 264 - Step 273056 - Epsilon 0.934013853387576 - \n",
      "Mean Reward 1625.51 - Mean Length 1054.55 - Mean Loss 2.027 - Mean Q Value 14.771 - \n",
      "Episode 265 - Step 273975 - Epsilon 0.9337992883270153 - \n",
      "Mean Reward 1624.66 - Mean Length 1055.19 - Mean Loss 2.029 - Mean Q Value 14.792 - \n",
      "Episode 266 - Step 274444 - Epsilon 0.9336898067652393 - \n",
      "Mean Reward 1630.57 - Mean Length 1057.4 - Mean Loss 2.027 - Mean Q Value 14.786 - \n",
      "Episode 267 - Step 275887 - Epsilon 0.933353038873467 - \n",
      "Mean Reward 1637.05 - Mean Length 1070.43 - Mean Loss 2.025 - Mean Q Value 14.792 - \n",
      "Episode 268 - Step 276655 - Epsilon 0.9331738522700447 - \n",
      "Mean Reward 1631.81 - Mean Length 1070.6 - Mean Loss 2.022 - Mean Q Value 14.79 - \n",
      "Episode 269 - Step 279216 - Epsilon 0.9325765788589382 - \n",
      "Mean Reward 1640.87 - Mean Length 1081.45 - Mean Loss 2.017 - Mean Q Value 14.775 - \n",
      "Episode 270 - Step 279581 - Epsilon 0.9324914851179298 - \n",
      "Mean Reward 1648.75 - Mean Length 1074.66 - Mean Loss 2.014 - Mean Q Value 14.783 - \n",
      "Episode 271 - Step 279883 - Epsilon 0.932421084659644 - \n",
      "Mean Reward 1652.38 - Mean Length 1066.36 - Mean Loss 2.014 - Mean Q Value 14.797 - \n",
      "Episode 272 - Step 280805 - Epsilon 0.9322061863407212 - \n",
      "Mean Reward 1638.88 - Mean Length 1064.01 - Mean Loss 2.016 - Mean Q Value 14.813 - \n",
      "Episode 273 - Step 281359 - Epsilon 0.9320770847082519 - \n",
      "Mean Reward 1643.54 - Mean Length 1066.8 - Mean Loss 2.014 - Mean Q Value 14.827 - \n",
      "Episode 274 - Step 282273 - Epsilon 0.931864129398817 - \n",
      "Mean Reward 1650.79 - Mean Length 1067.3 - Mean Loss 2.013 - Mean Q Value 14.863 - \n",
      "Episode 275 - Step 282914 - Epsilon 0.9317148101179225 - \n",
      "Mean Reward 1656.95 - Mean Length 1067.16 - Mean Loss 2.012 - Mean Q Value 14.881 - \n",
      "Episode 276 - Step 285653 - Epsilon 0.931077036704476 - \n",
      "Mean Reward 1656.44 - Mean Length 1087.02 - Mean Loss 2.009 - Mean Q Value 14.876 - \n",
      "Episode 277 - Step 285868 - Epsilon 0.9310269926524368 - \n",
      "Mean Reward 1644.11 - Mean Length 1072.46 - Mean Loss 2.007 - Mean Q Value 14.88 - \n",
      "Episode 278 - Step 286525 - Epsilon 0.9308740840077253 - \n",
      "Mean Reward 1638.16 - Mean Length 1075.43 - Mean Loss 2.005 - Mean Q Value 14.885 - \n",
      "Episode 279 - Step 290397 - Epsilon 0.9299734337667448 - \n",
      "Mean Reward 1634.18 - Mean Length 1099.11 - Mean Loss 2.001 - Mean Q Value 14.875 - \n",
      "Episode 280 - Step 291898 - Epsilon 0.9296245266598503 - \n",
      "Mean Reward 1637.63 - Mean Length 1109.72 - Mean Loss 1.995 - Mean Q Value 14.817 - \n",
      "Episode 281 - Step 293132 - Epsilon 0.9293377816901789 - \n",
      "Mean Reward 1628.61 - Mean Length 1106.94 - Mean Loss 1.986 - Mean Q Value 14.736 - \n",
      "Episode 282 - Step 295332 - Epsilon 0.9288267863828896 - \n",
      "Mean Reward 1627.48 - Mean Length 1126.05 - Mean Loss 1.974 - Mean Q Value 14.62 - \n",
      "Episode 283 - Step 297759 - Epsilon 0.9282633915969266 - \n",
      "Mean Reward 1630.13 - Mean Length 1134.38 - Mean Loss 1.964 - Mean Q Value 14.516 - \n",
      "Episode 284 - Step 299105 - Epsilon 0.9279510834753607 - \n",
      "Mean Reward 1630.02 - Mean Length 1142.82 - Mean Loss 1.956 - Mean Q Value 14.437 - \n",
      "Episode 285 - Step 299461 - Epsilon 0.9278684994936387 - \n",
      "Mean Reward 1626.93 - Mean Length 1141.48 - Mean Loss 1.949 - Mean Q Value 14.346 - \n",
      "Episode 286 - Step 301209 - Epsilon 0.9274631094927949 - \n",
      "Mean Reward 1633.04 - Mean Length 1135.05 - Mean Loss 1.945 - Mean Q Value 14.298 - \n",
      "Episode 287 - Step 302327 - Epsilon 0.9272039197447107 - \n",
      "Mean Reward 1632.75 - Mean Length 1128.08 - Mean Loss 1.94 - Mean Q Value 14.264 - \n",
      "Episode 288 - Step 302822 - Epsilon 0.9270891853446221 - \n",
      "Mean Reward 1629.84 - Mean Length 1120.74 - Mean Loss 1.937 - Mean Q Value 14.249 - \n",
      "Episode 289 - Step 303468 - Epsilon 0.9269394725120902 - \n",
      "Mean Reward 1627.72 - Mean Length 1112.11 - Mean Loss 1.936 - Mean Q Value 14.254 - \n",
      "Episode 290 - Step 303816 - Epsilon 0.9268588322757914 - \n",
      "Mean Reward 1631.36 - Mean Length 1099.59 - Mean Loss 1.935 - Mean Q Value 14.261 - \n",
      "Episode 291 - Step 304346 - Epsilon 0.9267360316008726 - \n",
      "Mean Reward 1630.27 - Mean Length 1090.65 - Mean Loss 1.937 - Mean Q Value 14.297 - \n",
      "Episode 292 - Step 304920 - Epsilon 0.9266030545050322 - \n",
      "Mean Reward 1630.61 - Mean Length 1090.44 - Mean Loss 1.943 - Mean Q Value 14.369 - \n",
      "Episode 293 - Step 305774 - Epsilon 0.9264052458449671 - \n",
      "Mean Reward 1629.97 - Mean Length 1088.43 - Mean Loss 1.946 - Mean Q Value 14.432 - \n",
      "Episode 294 - Step 306543 - Epsilon 0.9262271615330722 - \n",
      "Mean Reward 1630.66 - Mean Length 1075.08 - Mean Loss 1.951 - Mean Q Value 14.506 - \n",
      "Episode 295 - Step 307476 - Epsilon 0.9260111442146105 - \n",
      "Mean Reward 1630.8 - Mean Length 1082.39 - Mean Loss 1.958 - Mean Q Value 14.587 - \n",
      "Episode 296 - Step 308858 - Epsilon 0.9256912625869881 - \n",
      "Mean Reward 1637.26 - Mean Length 1079.7 - Mean Loss 1.961 - Mean Q Value 14.659 - \n",
      "Episode 297 - Step 309819 - Epsilon 0.925468891946662 - \n",
      "Mean Reward 1639.18 - Mean Length 1083.58 - Mean Loss 1.966 - Mean Q Value 14.745 - \n",
      "Episode 298 - Step 310320 - Epsilon 0.9253529842123147 - \n",
      "Mean Reward 1636.6 - Mean Length 1065.79 - Mean Loss 1.969 - Mean Q Value 14.81 - \n",
      "Episode 299 - Step 310428 - Epsilon 0.9253280000159025 - \n",
      "Mean Reward 1635.93 - Mean Length 1045.92 - Mean Loss 1.976 - Mean Q Value 14.905 - \n",
      "Episode 300 - Step 310898 - Epsilon 0.9252192803497011 - \n",
      "Mean Reward 1622.01 - Mean Length 1041.42 - Mean Loss 1.984 - Mean Q Value 15.013 - \n",
      "Episode 301 - Step 311248 - Epsilon 0.9251383271942919 - \n",
      "Mean Reward 1628.51 - Mean Length 1023.86 - Mean Loss 1.993 - Mean Q Value 15.127 - \n",
      "Episode 302 - Step 313641 - Epsilon 0.924585028642739 - \n",
      "Mean Reward 1632.98 - Mean Length 1041.15 - Mean Loss 2.001 - Mean Q Value 15.24 - \n",
      "Episode 303 - Step 313880 - Epsilon 0.9245297863307447 - \n",
      "Mean Reward 1625.45 - Mean Length 1027.12 - Mean Loss 2.007 - Mean Q Value 15.319 - \n",
      "Episode 304 - Step 314083 - Epsilon 0.9244828676287892 - \n",
      "Mean Reward 1619.71 - Mean Length 1017.69 - Mean Loss 2.013 - Mean Q Value 15.408 - \n",
      "Episode 305 - Step 316354 - Epsilon 0.9239581413857849 - \n",
      "Mean Reward 1618.44 - Mean Length 1030.44 - Mean Loss 2.016 - Mean Q Value 15.475 - \n",
      "Episode 306 - Step 317273 - Epsilon 0.9237458863599695 - \n",
      "Mean Reward 1608.9 - Mean Length 1020.05 - Mean Loss 2.016 - Mean Q Value 15.519 - \n",
      "Episode 307 - Step 318290 - Epsilon 0.9235110537933329 - \n",
      "Mean Reward 1610.43 - Mean Length 1015.41 - Mean Loss 2.017 - Mean Q Value 15.562 - \n",
      "Episode 308 - Step 320451 - Epsilon 0.9230122616324673 - \n",
      "Mean Reward 1615.63 - Mean Length 1032.91 - Mean Loss 2.019 - Mean Q Value 15.607 - \n",
      "Episode 309 - Step 321126 - Epsilon 0.922856516435197 - \n",
      "Mean Reward 1620.24 - Mean Length 1037.09 - Mean Loss 2.019 - Mean Q Value 15.626 - \n",
      "Episode 310 - Step 322762 - Epsilon 0.9224791452504179 - \n",
      "Mean Reward 1616.13 - Mean Length 1046.34 - Mean Loss 2.018 - Mean Q Value 15.62 - \n",
      "Episode 311 - Step 323071 - Epsilon 0.9224078864799354 - \n",
      "Mean Reward 1612.3 - Mean Length 1045.01 - Mean Loss 2.012 - Mean Q Value 15.593 - \n",
      "Episode 312 - Step 323718 - Epsilon 0.9222586990514867 - \n",
      "Mean Reward 1621.65 - Mean Length 1048.77 - Mean Loss 2.007 - Mean Q Value 15.572 - \n",
      "Episode 313 - Step 324407 - Epsilon 0.9220998536516504 - \n",
      "Mean Reward 1626.82 - Mean Length 1046.41 - Mean Loss 2.003 - Mean Q Value 15.538 - \n",
      "Episode 314 - Step 325621 - Epsilon 0.9218200387751085 - \n",
      "Mean Reward 1636.47 - Mean Length 1056.73 - Mean Loss 1.996 - Mean Q Value 15.507 - \n",
      "Episode 315 - Step 326177 - Epsilon 0.9216919146785162 - \n",
      "Mean Reward 1631.27 - Mean Length 1043.65 - Mean Loss 1.994 - Mean Q Value 15.49 - \n",
      "Episode 316 - Step 326451 - Epsilon 0.9216287809368158 - \n",
      "Mean Reward 1624.11 - Mean Length 1038.63 - Mean Loss 1.991 - Mean Q Value 15.481 - \n",
      "Episode 317 - Step 326673 - Epsilon 0.9215776319524704 - \n",
      "Mean Reward 1621.95 - Mean Length 1038.0 - Mean Loss 1.988 - Mean Q Value 15.49 - \n",
      "Episode 318 - Step 328436 - Epsilon 0.9211715360603262 - \n",
      "Mean Reward 1627.18 - Mean Length 1054.12 - Mean Loss 1.982 - Mean Q Value 15.475 - \n",
      "Episode 319 - Step 331304 - Epsilon 0.9205112927118733 - \n",
      "Mean Reward 1623.95 - Mean Length 1037.3 - Mean Loss 1.979 - Mean Q Value 15.447 - \n",
      "Episode 320 - Step 331742 - Epsilon 0.9204105022310877 - \n",
      "Mean Reward 1623.82 - Mean Length 1035.57 - Mean Loss 1.978 - Mean Q Value 15.439 - \n",
      "Episode 321 - Step 333041 - Epsilon 0.9201116474122676 - \n",
      "Mean Reward 1618.29 - Mean Length 1032.26 - Mean Loss 1.977 - Mean Q Value 15.446 - \n",
      "Episode 322 - Step 334205 - Epsilon 0.9198439338435823 - \n",
      "Mean Reward 1617.72 - Mean Length 1040.62 - Mean Loss 1.977 - Mean Q Value 15.461 - \n",
      "Episode 323 - Step 336418 - Epsilon 0.9193351708730503 - \n",
      "Mean Reward 1605.55 - Mean Length 1055.15 - Mean Loss 1.974 - Mean Q Value 15.447 - \n",
      "Episode 324 - Step 336839 - Epsilon 0.9192384159260265 - \n",
      "Mean Reward 1609.8 - Mean Length 1039.75 - Mean Loss 1.967 - Mean Q Value 15.415 - \n",
      "Episode 325 - Step 337101 - Epsilon 0.9191782077740872 - \n",
      "Mean Reward 1606.23 - Mean Length 1038.73 - Mean Loss 1.967 - Mean Q Value 15.394 - \n",
      "Episode 326 - Step 337267 - Epsilon 0.9191400626652078 - \n",
      "Mean Reward 1597.97 - Mean Length 1035.96 - Mean Loss 1.962 - Mean Q Value 15.378 - \n",
      "Episode 327 - Step 337636 - Epsilon 0.9190552758946667 - \n",
      "Mean Reward 1601.41 - Mean Length 1031.92 - Mean Loss 1.959 - Mean Q Value 15.365 - \n",
      "Episode 328 - Step 338303 - Epsilon 0.9189020361849514 - \n",
      "Mean Reward 1602.56 - Mean Length 1034.1 - Mean Loss 1.956 - Mean Q Value 15.347 - \n",
      "Episode 329 - Step 338741 - Epsilon 0.9188014219081299 - \n",
      "Mean Reward 1604.22 - Mean Length 1032.31 - Mean Loss 1.95 - Mean Q Value 15.336 - \n",
      "Episode 330 - Step 339294 - Epsilon 0.9186744063758076 - \n",
      "Mean Reward 1614.08 - Mean Length 1027.18 - Mean Loss 1.945 - Mean Q Value 15.333 - \n",
      "Episode 331 - Step 340573 - Epsilon 0.9183807071552794 - \n",
      "Mean Reward 1608.9 - Mean Length 1035.01 - Mean Loss 1.942 - Mean Q Value 15.322 - \n",
      "Episode 332 - Step 340844 - Epsilon 0.9183184889622453 - \n",
      "Mean Reward 1601.22 - Mean Length 1033.29 - Mean Loss 1.938 - Mean Q Value 15.308 - \n",
      "Episode 333 - Step 341026 - Epsilon 0.918276706416329 - \n",
      "Mean Reward 1601.27 - Mean Length 1032.31 - Mean Loss 1.934 - Mean Q Value 15.326 - \n",
      "Episode 334 - Step 342795 - Epsilon 0.9178706882794857 - \n",
      "Mean Reward 1594.26 - Mean Length 1041.66 - Mean Loss 1.926 - Mean Q Value 15.309 - \n",
      "Episode 335 - Step 344267 - Epsilon 0.9175329739671828 - \n",
      "Mean Reward 1590.5 - Mean Length 1023.44 - Mean Loss 1.921 - Mean Q Value 15.293 - \n",
      "Episode 336 - Step 344944 - Epsilon 0.9173776946327912 - \n",
      "Mean Reward 1593.22 - Mean Length 1024.25 - Mean Loss 1.918 - Mean Q Value 15.292 - \n",
      "Episode 337 - Step 346741 - Epsilon 0.9169656552131985 - \n",
      "Mean Reward 1600.7 - Mean Length 1031.46 - Mean Loss 1.913 - Mean Q Value 15.305 - \n",
      "Episode 338 - Step 349016 - Epsilon 0.9164442792119033 - \n",
      "Mean Reward 1594.13 - Mean Length 1047.87 - Mean Loss 1.906 - Mean Q Value 15.281 - \n",
      "Episode 339 - Step 350019 - Epsilon 0.916214509588708 - \n",
      "Mean Reward 1605.23 - Mean Length 1042.11 - Mean Loss 1.899 - Mean Q Value 15.244 - \n",
      "Episode 340 - Step 352796 - Epsilon 0.915578648334732 - \n",
      "Mean Reward 1607.67 - Mean Length 1065.88 - Mean Loss 1.891 - Mean Q Value 15.193 - \n",
      "Episode 341 - Step 353609 - Epsilon 0.9153925758614289 - \n",
      "Mean Reward 1607.69 - Mean Length 1070.91 - Mean Loss 1.884 - Mean Q Value 15.144 - \n",
      "Episode 342 - Step 354471 - Epsilon 0.9151953299906536 - \n",
      "Mean Reward 1609.91 - Mean Length 1077.14 - Mean Loss 1.88 - Mean Q Value 15.086 - \n",
      "Episode 343 - Step 355005 - Epsilon 0.9150731595538694 - \n",
      "Mean Reward 1604.82 - Mean Length 1054.62 - Mean Loss 1.876 - Mean Q Value 15.067 - \n",
      "Episode 344 - Step 356303 - Epsilon 0.9147762664499531 - \n",
      "Mean Reward 1592.27 - Mean Length 1054.68 - Mean Loss 1.872 - Mean Q Value 15.035 - \n",
      "Episode 345 - Step 358082 - Epsilon 0.9143695101136926 - \n",
      "Mean Reward 1583.88 - Mean Length 1067.54 - Mean Loss 1.867 - Mean Q Value 14.991 - \n",
      "Episode 346 - Step 361086 - Epsilon 0.9136830763138535 - \n",
      "Mean Reward 1586.1 - Mean Length 1077.43 - Mean Loss 1.86 - Mean Q Value 14.945 - \n",
      "Episode 347 - Step 361832 - Epsilon 0.9135126902877898 - \n",
      "Mean Reward 1584.34 - Mean Length 1074.6 - Mean Loss 1.856 - Mean Q Value 14.902 - \n",
      "Episode 348 - Step 363172 - Epsilon 0.9132067147520165 - \n",
      "Mean Reward 1588.8 - Mean Length 1083.78 - Mean Loss 1.85 - Mean Q Value 14.863 - \n",
      "Episode 349 - Step 366173 - Epsilon 0.9125218382749841 - \n",
      "Mean Reward 1591.73 - Mean Length 1095.48 - Mean Loss 1.842 - Mean Q Value 14.808 - \n",
      "Episode 350 - Step 368584 - Epsilon 0.911971981397919 - \n",
      "Mean Reward 1599.88 - Mean Length 1091.36 - Mean Loss 1.837 - Mean Q Value 14.777 - \n",
      "Episode 351 - Step 369449 - Epsilon 0.911774788754489 - \n",
      "Mean Reward 1603.8 - Mean Length 1082.27 - Mean Loss 1.836 - Mean Q Value 14.78 - \n",
      "Episode 352 - Step 369799 - Epsilon 0.9116950119407762 - \n",
      "Mean Reward 1604.14 - Mean Length 1081.34 - Mean Loss 1.839 - Mean Q Value 14.814 - \n",
      "Episode 353 - Step 370380 - Epsilon 0.9115625978405284 - \n",
      "Mean Reward 1600.72 - Mean Length 1073.4 - Mean Loss 1.84 - Mean Q Value 14.847 - \n",
      "Episode 354 - Step 373433 - Epsilon 0.9108671130488423 - \n",
      "Mean Reward 1608.45 - Mean Length 1101.75 - Mean Loss 1.84 - Mean Q Value 14.853 - \n",
      "Episode 355 - Step 374591 - Epsilon 0.9106034551529425 - \n",
      "Mean Reward 1616.94 - Mean Length 1110.51 - Mean Loss 1.837 - Mean Q Value 14.848 - \n",
      "Episode 356 - Step 377074 - Epsilon 0.9100383733928398 - \n",
      "Mean Reward 1605.83 - Mean Length 1115.93 - Mean Loss 1.836 - Mean Q Value 14.866 - \n",
      "Episode 357 - Step 377952 - Epsilon 0.9098386418662232 - \n",
      "Mean Reward 1606.61 - Mean Length 1112.15 - Mean Loss 1.835 - Mean Q Value 14.87 - \n",
      "Episode 358 - Step 378132 - Epsilon 0.9097977000434136 - \n",
      "Mean Reward 1599.31 - Mean Length 1104.91 - Mean Loss 1.834 - Mean Q Value 14.88 - \n",
      "Episode 359 - Step 378508 - Epsilon 0.9097121830682686 - \n",
      "Mean Reward 1588.67 - Mean Length 1096.01 - Mean Loss 1.83 - Mean Q Value 14.873 - \n",
      "Episode 360 - Step 378801 - Epsilon 0.9096455490830201 - \n",
      "Mean Reward 1585.55 - Mean Length 1083.14 - Mean Loss 1.824 - Mean Q Value 14.842 - \n",
      "Episode 361 - Step 381526 - Epsilon 0.9090260640112485 - \n",
      "Mean Reward 1580.67 - Mean Length 1106.14 - Mean Loss 1.819 - Mean Q Value 14.785 - \n",
      "Episode 362 - Step 382367 - Epsilon 0.9088349613477469 - \n",
      "Mean Reward 1577.34 - Mean Length 1099.82 - Mean Loss 1.812 - Mean Q Value 14.722 - \n",
      "Episode 363 - Step 382765 - Epsilon 0.9087445367564748 - \n",
      "Mean Reward 1574.61 - Mean Length 1099.91 - Mean Loss 1.806 - Mean Q Value 14.674 - \n",
      "Episode 364 - Step 383985 - Epsilon 0.908467411901774 - \n",
      "Mean Reward 1590.65 - Mean Length 1109.29 - Mean Loss 1.801 - Mean Q Value 14.632 - \n",
      "Episode 365 - Step 384666 - Epsilon 0.9083127584707915 - \n",
      "Mean Reward 1591.95 - Mean Length 1106.91 - Mean Loss 1.796 - Mean Q Value 14.597 - \n",
      "Episode 366 - Step 385024 - Epsilon 0.9082314681065335 - \n",
      "Mean Reward 1586.93 - Mean Length 1105.8 - Mean Loss 1.793 - Mean Q Value 14.561 - \n",
      "Episode 367 - Step 386708 - Epsilon 0.9078491830872646 - \n",
      "Mean Reward 1591.88 - Mean Length 1108.21 - Mean Loss 1.792 - Mean Q Value 14.536 - \n",
      "Episode 368 - Step 388883 - Epsilon 0.9073556742170888 - \n",
      "Mean Reward 1593.36 - Mean Length 1122.28 - Mean Loss 1.791 - Mean Q Value 14.511 - \n",
      "Episode 369 - Step 389664 - Epsilon 0.9071785302937687 - \n",
      "Mean Reward 1580.32 - Mean Length 1104.48 - Mean Loss 1.787 - Mean Q Value 14.47 - \n",
      "Episode 370 - Step 390826 - Epsilon 0.9069150331725548 - \n",
      "Mean Reward 1574.69 - Mean Length 1112.45 - Mean Loss 1.784 - Mean Q Value 14.423 - \n",
      "Episode 371 - Step 391045 - Epsilon 0.9068653809275177 - \n",
      "Mean Reward 1571.3 - Mean Length 1111.62 - Mean Loss 1.778 - Mean Q Value 14.38 - \n",
      "Episode 372 - Step 392113 - Epsilon 0.9066232801624029 - \n",
      "Mean Reward 1576.62 - Mean Length 1113.08 - Mean Loss 1.775 - Mean Q Value 14.357 - \n",
      "Episode 373 - Step 393880 - Epsilon 0.9062228677258926 - \n",
      "Mean Reward 1590.15 - Mean Length 1125.21 - Mean Loss 1.776 - Mean Q Value 14.339 - \n",
      "Episode 374 - Step 394145 - Epsilon 0.9061628324420834 - \n",
      "Mean Reward 1582.33 - Mean Length 1118.72 - Mean Loss 1.775 - Mean Q Value 14.289 - \n",
      "Episode 375 - Step 397413 - Epsilon 0.9054227996599253 - \n",
      "Mean Reward 1584.14 - Mean Length 1144.99 - Mean Loss 1.768 - Mean Q Value 14.236 - \n",
      "Episode 376 - Step 398552 - Epsilon 0.9051650171889842 - \n",
      "Mean Reward 1580.97 - Mean Length 1128.99 - Mean Loss 1.764 - Mean Q Value 14.206 - \n",
      "Episode 377 - Step 399582 - Epsilon 0.9049319671743721 - \n",
      "Mean Reward 1581.65 - Mean Length 1137.14 - Mean Loss 1.762 - Mean Q Value 14.171 - \n",
      "Episode 378 - Step 400258 - Epsilon 0.9047790465749381 - \n",
      "Mean Reward 1580.67 - Mean Length 1137.33 - Mean Loss 1.759 - Mean Q Value 14.13 - \n",
      "Episode 379 - Step 400577 - Epsilon 0.9047068933140934 - \n",
      "Mean Reward 1579.75 - Mean Length 1101.8 - Mean Loss 1.757 - Mean Q Value 14.119 - \n",
      "Episode 380 - Step 401354 - Epsilon 0.9045311710456545 - \n",
      "Mean Reward 1573.69 - Mean Length 1094.56 - Mean Loss 1.762 - Mean Q Value 14.165 - \n",
      "Episode 381 - Step 401674 - Epsilon 0.9044588114373384 - \n",
      "Mean Reward 1576.63 - Mean Length 1085.42 - Mean Loss 1.767 - Mean Q Value 14.209 - \n",
      "Episode 382 - Step 403488 - Epsilon 0.9040487323075077 - \n",
      "Mean Reward 1581.52 - Mean Length 1081.56 - Mean Loss 1.77 - Mean Q Value 14.263 - \n",
      "Episode 383 - Step 404545 - Epsilon 0.9038098689613141 - \n",
      "Mean Reward 1583.36 - Mean Length 1067.86 - Mean Loss 1.775 - Mean Q Value 14.324 - \n",
      "Episode 384 - Step 406080 - Epsilon 0.9034630984215579 - \n",
      "Mean Reward 1587.22 - Mean Length 1069.75 - Mean Loss 1.779 - Mean Q Value 14.376 - \n",
      "Episode 385 - Step 407361 - Epsilon 0.9031738106527626 - \n",
      "Mean Reward 1600.48 - Mean Length 1079.0 - Mean Loss 1.785 - Mean Q Value 14.443 - \n",
      "Episode 386 - Step 408200 - Epsilon 0.9029843897884825 - \n",
      "Mean Reward 1596.99 - Mean Length 1069.91 - Mean Loss 1.789 - Mean Q Value 14.491 - \n",
      "Episode 387 - Step 408441 - Epsilon 0.902929986611102 - \n",
      "Mean Reward 1586.24 - Mean Length 1061.14 - Mean Loss 1.793 - Mean Q Value 14.538 - \n",
      "Episode 388 - Step 409458 - Epsilon 0.9027004458148915 - \n",
      "Mean Reward 1588.57 - Mean Length 1066.36 - Mean Loss 1.797 - Mean Q Value 14.583 - \n",
      "Episode 389 - Step 409850 - Epsilon 0.9026119854947582 - \n",
      "Mean Reward 1590.48 - Mean Length 1063.82 - Mean Loss 1.799 - Mean Q Value 14.636 - \n",
      "Episode 390 - Step 410962 - Epsilon 0.9023610942069011 - \n",
      "Mean Reward 1592.57 - Mean Length 1071.46 - Mean Loss 1.805 - Mean Q Value 14.7 - \n",
      "Episode 391 - Step 411840 - Epsilon 0.9021630476583433 - \n",
      "Mean Reward 1585.49 - Mean Length 1074.94 - Mean Loss 1.805 - Mean Q Value 14.719 - \n",
      "Episode 392 - Step 412110 - Epsilon 0.9021021537002002 - \n",
      "Mean Reward 1580.02 - Mean Length 1071.9 - Mean Loss 1.803 - Mean Q Value 14.721 - \n",
      "Episode 393 - Step 412875 - Epsilon 0.9019296431385654 - \n",
      "Mean Reward 1583.66 - Mean Length 1071.01 - Mean Loss 1.803 - Mean Q Value 14.717 - \n",
      "Episode 394 - Step 413524 - Epsilon 0.9017833169066917 - \n",
      "Mean Reward 1584.56 - Mean Length 1069.81 - Mean Loss 1.803 - Mean Q Value 14.72 - \n",
      "Episode 395 - Step 413760 - Epsilon 0.9017301132538597 - \n",
      "Mean Reward 1585.0 - Mean Length 1062.84 - Mean Loss 1.801 - Mean Q Value 14.714 - \n",
      "Episode 396 - Step 413971 - Epsilon 0.9016825482389715 - \n",
      "Mean Reward 1577.0 - Mean Length 1051.13 - Mean Loss 1.801 - Mean Q Value 14.725 - \n",
      "Episode 397 - Step 414586 - Epsilon 0.9015439251867523 - \n",
      "Mean Reward 1586.36 - Mean Length 1047.67 - Mean Loss 1.802 - Mean Q Value 14.741 - \n",
      "Episode 398 - Step 414709 - Epsilon 0.9015162031338123 - \n",
      "Mean Reward 1573.7 - Mean Length 1043.89 - Mean Loss 1.804 - Mean Q Value 14.78 - \n",
      "Episode 399 - Step 415300 - Epsilon 0.901383013937727 - \n",
      "Mean Reward 1584.16 - Mean Length 1048.72 - Mean Loss 1.805 - Mean Q Value 14.811 - \n",
      "Episode 400 - Step 416651 - Epsilon 0.9010786231935578 - \n",
      "Mean Reward 1585.64 - Mean Length 1057.53 - Mean Loss 1.806 - Mean Q Value 14.839 - \n",
      "Episode 401 - Step 416858 - Epsilon 0.9010319935755237 - \n",
      "Mean Reward 1580.86 - Mean Length 1056.1 - Mean Loss 1.806 - Mean Q Value 14.857 - \n",
      "Episode 402 - Step 417625 - Epsilon 0.9008592372326811 - \n",
      "Mean Reward 1579.58 - Mean Length 1039.84 - Mean Loss 1.807 - Mean Q Value 14.89 - \n",
      "Episode 403 - Step 419757 - Epsilon 0.90037920713855 - \n",
      "Mean Reward 1593.86 - Mean Length 1058.77 - Mean Loss 1.809 - Mean Q Value 14.923 - \n",
      "Episode 404 - Step 420186 - Epsilon 0.9002826466346504 - \n",
      "Mean Reward 1598.9 - Mean Length 1061.03 - Mean Loss 1.807 - Mean Q Value 14.932 - \n",
      "Episode 405 - Step 421464 - Epsilon 0.8999950522387126 - \n",
      "Mean Reward 1604.47 - Mean Length 1051.1 - Mean Loss 1.812 - Mean Q Value 14.972 - \n",
      "Episode 406 - Step 422568 - Epsilon 0.899746687849077 - \n",
      "Mean Reward 1614.36 - Mean Length 1052.95 - Mean Loss 1.818 - Mean Q Value 15.033 - \n",
      "Episode 407 - Step 422954 - Epsilon 0.8996598664720341 - \n",
      "Mean Reward 1609.75 - Mean Length 1046.64 - Mean Loss 1.823 - Mean Q Value 15.077 - \n",
      "Episode 408 - Step 423978 - Epsilon 0.8994295829949431 - \n",
      "Mean Reward 1603.25 - Mean Length 1035.27 - Mean Loss 1.826 - Mean Q Value 15.124 - \n",
      "Episode 409 - Step 425282 - Epsilon 0.8991364167029022 - \n",
      "Mean Reward 1604.73 - Mean Length 1041.56 - Mean Loss 1.829 - Mean Q Value 15.17 - \n",
      "Episode 410 - Step 426473 - Epsilon 0.898868738653878 - \n",
      "Mean Reward 1612.56 - Mean Length 1037.11 - Mean Loss 1.833 - Mean Q Value 15.208 - \n",
      "Episode 411 - Step 428903 - Epsilon 0.8983228416598369 - \n",
      "Mean Reward 1618.45 - Mean Length 1058.32 - Mean Loss 1.835 - Mean Q Value 15.226 - \n",
      "Episode 412 - Step 430378 - Epsilon 0.8979916461384571 - \n",
      "Mean Reward 1611.32 - Mean Length 1066.6 - Mean Loss 1.835 - Mean Q Value 15.231 - \n",
      "Episode 413 - Step 431294 - Epsilon 0.8977860295697572 - \n",
      "Mean Reward 1601.46 - Mean Length 1068.87 - Mean Loss 1.833 - Mean Q Value 15.232 - \n",
      "Episode 414 - Step 431601 - Epsilon 0.8977171271275298 - \n",
      "Mean Reward 1595.44 - Mean Length 1059.8 - Mean Loss 1.83 - Mean Q Value 15.237 - \n",
      "Episode 415 - Step 432386 - Epsilon 0.8975409674055247 - \n",
      "Mean Reward 1595.32 - Mean Length 1062.09 - Mean Loss 1.827 - Mean Q Value 15.233 - \n",
      "Episode 416 - Step 434587 - Epsilon 0.8970472312780365 - \n",
      "Mean Reward 1599.6 - Mean Length 1081.36 - Mean Loss 1.824 - Mean Q Value 15.197 - \n",
      "Episode 417 - Step 434870 - Epsilon 0.8969837674235411 - \n",
      "Mean Reward 1601.9 - Mean Length 1081.97 - Mean Loss 1.817 - Mean Q Value 15.136 - \n",
      "Episode 418 - Step 435326 - Epsilon 0.8968815170896389 - \n",
      "Mean Reward 1604.09 - Mean Length 1068.9 - Mean Loss 1.811 - Mean Q Value 15.091 - \n",
      "Episode 419 - Step 439379 - Epsilon 0.8959732120265463 - \n",
      "Mean Reward 1595.65 - Mean Length 1080.75 - Mean Loss 1.808 - Mean Q Value 15.068 - \n",
      "Episode 420 - Step 440906 - Epsilon 0.8956312394882219 - \n",
      "Mean Reward 1598.98 - Mean Length 1091.64 - Mean Loss 1.803 - Mean Q Value 15.027 - \n",
      "Episode 421 - Step 442741 - Epsilon 0.8952204628347988 - \n",
      "Mean Reward 1601.06 - Mean Length 1097.0 - Mean Loss 1.798 - Mean Q Value 14.982 - \n",
      "Episode 422 - Step 443692 - Epsilon 0.895007649442323 - \n",
      "Mean Reward 1608.53 - Mean Length 1094.87 - Mean Loss 1.796 - Mean Q Value 14.951 - \n",
      "Episode 423 - Step 444768 - Epsilon 0.8947669247334233 - \n",
      "Mean Reward 1607.84 - Mean Length 1083.5 - Mean Loss 1.796 - Mean Q Value 14.948 - \n",
      "Episode 424 - Step 446442 - Epsilon 0.8943925430733982 - \n",
      "Mean Reward 1615.2 - Mean Length 1096.03 - Mean Loss 1.797 - Mean Q Value 14.943 - \n",
      "Episode 425 - Step 447621 - Epsilon 0.894128959685852 - \n",
      "Mean Reward 1618.15 - Mean Length 1105.2 - Mean Loss 1.798 - Mean Q Value 14.96 - \n",
      "Episode 426 - Step 449093 - Epsilon 0.893799980723362 - \n",
      "Mean Reward 1625.9 - Mean Length 1118.26 - Mean Loss 1.801 - Mean Q Value 14.973 - \n",
      "Episode 427 - Step 450759 - Epsilon 0.8934277904988072 - \n",
      "Mean Reward 1623.52 - Mean Length 1131.23 - Mean Loss 1.802 - Mean Q Value 14.977 - \n",
      "Episode 428 - Step 451143 - Epsilon 0.8933420255369704 - \n",
      "Mean Reward 1626.37 - Mean Length 1128.4 - Mean Loss 1.802 - Mean Q Value 14.987 - \n",
      "Episode 429 - Step 451496 - Epsilon 0.8932631915719511 - \n",
      "Mean Reward 1622.89 - Mean Length 1127.55 - Mean Loss 1.803 - Mean Q Value 14.989 - \n",
      "Episode 430 - Step 452584 - Epsilon 0.8930202569940421 - \n",
      "Mean Reward 1617.82 - Mean Length 1132.9 - Mean Loss 1.803 - Mean Q Value 14.978 - \n",
      "Episode 431 - Step 454275 - Epsilon 0.8926428124211322 - \n",
      "Mean Reward 1624.41 - Mean Length 1137.02 - Mean Loss 1.801 - Mean Q Value 14.951 - \n",
      "Episode 432 - Step 455099 - Epsilon 0.892458946917563 - \n",
      "Mean Reward 1624.91 - Mean Length 1142.55 - Mean Loss 1.796 - Mean Q Value 14.916 - \n",
      "Episode 433 - Step 456811 - Epsilon 0.8920770561709424 - \n",
      "Mean Reward 1626.55 - Mean Length 1157.85 - Mean Loss 1.793 - Mean Q Value 14.867 - \n",
      "Episode 434 - Step 457720 - Epsilon 0.8918743546673917 - \n",
      "Mean Reward 1629.14 - Mean Length 1149.25 - Mean Loss 1.79 - Mean Q Value 14.829 - \n",
      "Episode 435 - Step 459215 - Epsilon 0.8915410788704243 - \n",
      "Mean Reward 1639.53 - Mean Length 1149.48 - Mean Loss 1.788 - Mean Q Value 14.818 - \n",
      "Episode 436 - Step 459677 - Epsilon 0.8914381118093914 - \n",
      "Mean Reward 1638.42 - Mean Length 1147.33 - Mean Loss 1.784 - Mean Q Value 14.805 - \n",
      "Episode 437 - Step 460489 - Epsilon 0.8912571682164468 - \n",
      "Mean Reward 1640.8 - Mean Length 1137.48 - Mean Loss 1.785 - Mean Q Value 14.801 - \n",
      "Episode 438 - Step 462247 - Epsilon 0.8908655467071391 - \n",
      "Mean Reward 1645.24 - Mean Length 1132.31 - Mean Loss 1.788 - Mean Q Value 14.837 - \n",
      "Episode 439 - Step 464378 - Epsilon 0.8903910644291947 - \n",
      "Mean Reward 1637.02 - Mean Length 1143.59 - Mean Loss 1.791 - Mean Q Value 14.883 - \n",
      "Episode 440 - Step 465307 - Epsilon 0.890184295090624 - \n",
      "Mean Reward 1629.46 - Mean Length 1125.11 - Mean Loss 1.794 - Mean Q Value 14.926 - \n",
      "Episode 441 - Step 465514 - Epsilon 0.8901382292395527 - \n",
      "Mean Reward 1624.27 - Mean Length 1119.05 - Mean Loss 1.797 - Mean Q Value 14.982 - \n",
      "Episode 442 - Step 467564 - Epsilon 0.8896821502202257 - \n",
      "Mean Reward 1630.16 - Mean Length 1130.93 - Mean Loss 1.801 - Mean Q Value 15.056 - \n",
      "Episode 443 - Step 467889 - Epsilon 0.8896098664730413 - \n",
      "Mean Reward 1628.66 - Mean Length 1128.84 - Mean Loss 1.805 - Mean Q Value 15.107 - \n",
      "Episode 444 - Step 469030 - Epsilon 0.8893561414161362 - \n",
      "Mean Reward 1634.42 - Mean Length 1127.27 - Mean Loss 1.809 - Mean Q Value 15.176 - \n",
      "Episode 445 - Step 469394 - Epsilon 0.8892752136794077 - \n",
      "Mean Reward 1640.27 - Mean Length 1113.12 - Mean Loss 1.812 - Mean Q Value 15.244 - \n",
      "Episode 446 - Step 471572 - Epsilon 0.8887911350673422 - \n",
      "Mean Reward 1645.77 - Mean Length 1104.86 - Mean Loss 1.815 - Mean Q Value 15.306 - \n",
      "Episode 447 - Step 473835 - Epsilon 0.8882884436320194 - \n",
      "Mean Reward 1641.24 - Mean Length 1120.03 - Mean Loss 1.818 - Mean Q Value 15.358 - \n",
      "Episode 448 - Step 474041 - Epsilon 0.8882426979494094 - \n",
      "Mean Reward 1632.78 - Mean Length 1108.69 - Mean Loss 1.82 - Mean Q Value 15.42 - \n",
      "Episode 449 - Step 474325 - Epsilon 0.8881796349487259 - \n",
      "Mean Reward 1628.13 - Mean Length 1081.52 - Mean Loss 1.825 - Mean Q Value 15.493 - \n",
      "Episode 450 - Step 475394 - Epsilon 0.8879423006267962 - \n",
      "Mean Reward 1615.29 - Mean Length 1068.1 - Mean Loss 1.833 - Mean Q Value 15.579 - \n",
      "Episode 451 - Step 476543 - Epsilon 0.887687275798722 - \n",
      "Mean Reward 1613.82 - Mean Length 1070.94 - Mean Loss 1.837 - Mean Q Value 15.62 - \n",
      "Episode 452 - Step 478578 - Epsilon 0.887235779699718 - \n",
      "Mean Reward 1617.61 - Mean Length 1087.79 - Mean Loss 1.84 - Mean Q Value 15.65 - \n",
      "Episode 453 - Step 478775 - Epsilon 0.8871920844081053 - \n",
      "Mean Reward 1613.32 - Mean Length 1083.95 - Mean Loss 1.84 - Mean Q Value 15.663 - \n",
      "Episode 454 - Step 480330 - Epsilon 0.8868472554726068 - \n",
      "Mean Reward 1615.32 - Mean Length 1068.97 - Mean Loss 1.843 - Mean Q Value 15.683 - \n",
      "Episode 455 - Step 480758 - Epsilon 0.886752367880974 - \n",
      "Mean Reward 1609.59 - Mean Length 1061.67 - Mean Loss 1.848 - Mean Q Value 15.721 - \n",
      "Episode 456 - Step 482307 - Epsilon 0.886409039464804 - \n",
      "Mean Reward 1608.48 - Mean Length 1052.33 - Mean Loss 1.852 - Mean Q Value 15.747 - \n",
      "Episode 457 - Step 482829 - Epsilon 0.886293370618237 - \n",
      "Mean Reward 1609.86 - Mean Length 1048.77 - Mean Loss 1.856 - Mean Q Value 15.793 - \n",
      "Episode 458 - Step 483298 - Epsilon 0.8861894587994784 - \n",
      "Mean Reward 1613.97 - Mean Length 1051.66 - Mean Loss 1.865 - Mean Q Value 15.854 - \n",
      "Episode 459 - Step 484087 - Epsilon 0.8860146751454627 - \n",
      "Mean Reward 1620.53 - Mean Length 1055.79 - Mean Loss 1.874 - Mean Q Value 15.921 - \n",
      "Episode 460 - Step 484335 - Epsilon 0.8859597439316147 - \n",
      "Mean Reward 1620.22 - Mean Length 1055.34 - Mean Loss 1.883 - Mean Q Value 15.987 - \n",
      "Episode 461 - Step 485860 - Epsilon 0.8856220361166234 - \n",
      "Mean Reward 1631.57 - Mean Length 1043.34 - Mean Loss 1.889 - Mean Q Value 16.062 - \n",
      "Episode 462 - Step 486238 - Epsilon 0.8855383487780265 - \n",
      "Mean Reward 1624.7 - Mean Length 1038.71 - Mean Loss 1.898 - Mean Q Value 16.159 - \n",
      "Episode 463 - Step 486867 - Epsilon 0.8853991088032871 - \n",
      "Mean Reward 1634.8 - Mean Length 1041.02 - Mean Loss 1.905 - Mean Q Value 16.236 - \n",
      "Episode 464 - Step 487759 - Epsilon 0.8852016867906917 - \n",
      "Mean Reward 1628.72 - Mean Length 1037.74 - Mean Loss 1.913 - Mean Q Value 16.325 - \n",
      "Episode 465 - Step 488450 - Epsilon 0.8850487813877477 - \n",
      "Mean Reward 1619.24 - Mean Length 1037.84 - Mean Loss 1.921 - Mean Q Value 16.411 - \n",
      "Episode 466 - Step 488651 - Epsilon 0.8850043087983006 - \n",
      "Mean Reward 1617.36 - Mean Length 1036.27 - Mean Loss 1.931 - Mean Q Value 16.505 - \n",
      "Episode 467 - Step 489249 - Epsilon 0.8848720105271218 - \n",
      "Mean Reward 1621.31 - Mean Length 1025.41 - Mean Loss 1.938 - Mean Q Value 16.592 - \n",
      "Episode 468 - Step 490216 - Epsilon 0.8846581185470457 - \n",
      "Mean Reward 1626.77 - Mean Length 1013.33 - Mean Loss 1.948 - Mean Q Value 16.702 - \n",
      "Episode 469 - Step 493353 - Epsilon 0.8839645973133475 - \n",
      "Mean Reward 1624.52 - Mean Length 1036.89 - Mean Loss 1.959 - Mean Q Value 16.799 - \n",
      "Episode 470 - Step 494666 - Epsilon 0.8836744835155051 - \n",
      "Mean Reward 1632.69 - Mean Length 1038.4 - Mean Loss 1.963 - Mean Q Value 16.844 - \n",
      "Episode 471 - Step 496060 - Epsilon 0.8833665765753695 - \n",
      "Mean Reward 1634.09 - Mean Length 1050.15 - Mean Loss 1.967 - Mean Q Value 16.88 - \n",
      "Episode 472 - Step 497216 - Epsilon 0.8831113204890777 - \n",
      "Mean Reward 1630.63 - Mean Length 1051.03 - Mean Loss 1.969 - Mean Q Value 16.878 - \n",
      "MarioNet saved to checkpoints/2022-05-18T11-36-14/mario_net_1.chkpt at step 500000\n",
      "Episode 473 - Step 501332 - Epsilon 0.8822030662036046 - \n",
      "Mean Reward 1627.6 - Mean Length 1074.52 - Mean Loss 1.962 - Mean Q Value 16.827 - \n",
      "Episode 474 - Step 503283 - Epsilon 0.8817728765252656 - \n",
      "Mean Reward 1629.07 - Mean Length 1091.38 - Mean Loss 1.956 - Mean Q Value 16.78 - \n",
      "Episode 475 - Step 505994 - Epsilon 0.8811754573567285 - \n",
      "Mean Reward 1628.68 - Mean Length 1085.81 - Mean Loss 1.952 - Mean Q Value 16.744 - \n",
      "Episode 476 - Step 507157 - Epsilon 0.880919292802167 - \n",
      "Mean Reward 1634.7 - Mean Length 1086.05 - Mean Loss 1.949 - Mean Q Value 16.699 - \n",
      "Episode 477 - Step 508224 - Epsilon 0.8806843388897815 - \n",
      "Mean Reward 1645.54 - Mean Length 1086.42 - Mean Loss 1.944 - Mean Q Value 16.66 - \n",
      "Episode 478 - Step 510445 - Epsilon 0.8801954745829482 - \n",
      "Mean Reward 1654.11 - Mean Length 1101.87 - Mean Loss 1.942 - Mean Q Value 16.651 - \n",
      "Episode 479 - Step 511363 - Epsilon 0.8799934928745438 - \n",
      "Mean Reward 1649.83 - Mean Length 1107.86 - Mean Loss 1.941 - Mean Q Value 16.634 - \n",
      "Episode 480 - Step 513818 - Epsilon 0.8794535625088028 - \n",
      "Mean Reward 1655.0 - Mean Length 1124.64 - Mean Loss 1.936 - Mean Q Value 16.602 - \n",
      "Episode 481 - Step 514387 - Epsilon 0.8793284691213602 - \n",
      "Mean Reward 1654.69 - Mean Length 1127.13 - Mean Loss 1.932 - Mean Q Value 16.591 - \n",
      "Episode 482 - Step 515465 - Epsilon 0.8790915219993405 - \n",
      "Mean Reward 1649.81 - Mean Length 1119.77 - Mean Loss 1.93 - Mean Q Value 16.583 - \n",
      "Episode 483 - Step 520280 - Epsilon 0.8780339520999763 - \n",
      "Mean Reward 1648.61 - Mean Length 1157.35 - Mean Loss 1.926 - Mean Q Value 16.556 - \n",
      "Episode 484 - Step 520926 - Epsilon 0.8778921610489037 - \n",
      "Mean Reward 1645.95 - Mean Length 1148.46 - Mean Loss 1.922 - Mean Q Value 16.512 - \n",
      "Episode 485 - Step 522061 - Epsilon 0.8776430944550322 - \n",
      "Mean Reward 1632.69 - Mean Length 1147.0 - Mean Loss 1.916 - Mean Q Value 16.464 - \n",
      "Episode 486 - Step 522576 - Epsilon 0.8775301051663227 - \n",
      "Mean Reward 1627.92 - Mean Length 1143.76 - Mean Loss 1.91 - Mean Q Value 16.411 - \n",
      "Episode 487 - Step 523141 - Epsilon 0.8774061627770942 - \n",
      "Mean Reward 1632.11 - Mean Length 1147.0 - Mean Loss 1.904 - Mean Q Value 16.374 - \n",
      "Episode 488 - Step 523616 - Epsilon 0.8773019769683817 - \n",
      "Mean Reward 1633.11 - Mean Length 1141.58 - Mean Loss 1.899 - Mean Q Value 16.343 - \n",
      "Episode 489 - Step 523743 - Epsilon 0.8772741230693101 - \n",
      "Mean Reward 1623.84 - Mean Length 1138.93 - Mean Loss 1.896 - Mean Q Value 16.324 - \n",
      "Episode 490 - Step 524418 - Epsilon 0.877126095532693 - \n",
      "Mean Reward 1616.99 - Mean Length 1134.56 - Mean Loss 1.891 - Mean Q Value 16.28 - \n",
      "Episode 491 - Step 524940 - Epsilon 0.8770116380314171 - \n",
      "Mean Reward 1621.54 - Mean Length 1131.0 - Mean Loss 1.889 - Mean Q Value 16.266 - \n",
      "Episode 492 - Step 526708 - Epsilon 0.8766240844945508 - \n",
      "Mean Reward 1628.81 - Mean Length 1145.98 - Mean Loss 1.889 - Mean Q Value 16.269 - \n",
      "Episode 493 - Step 528291 - Epsilon 0.8762772291082435 - \n",
      "Mean Reward 1627.32 - Mean Length 1154.16 - Mean Loss 1.888 - Mean Q Value 16.274 - \n",
      "Episode 494 - Step 529295 - Epsilon 0.8760573110970807 - \n",
      "Mean Reward 1629.13 - Mean Length 1157.71 - Mean Loss 1.886 - Mean Q Value 16.272 - \n",
      "Episode 495 - Step 529452 - Epsilon 0.8760229265181193 - \n",
      "Mean Reward 1624.7 - Mean Length 1156.92 - Mean Loss 1.883 - Mean Q Value 16.286 - \n",
      "Episode 496 - Step 530479 - Epsilon 0.8757980364750876 - \n",
      "Mean Reward 1642.34 - Mean Length 1165.08 - Mean Loss 1.884 - Mean Q Value 16.315 - \n",
      "Episode 497 - Step 531400 - Epsilon 0.8755964071654228 - \n",
      "Mean Reward 1641.41 - Mean Length 1168.14 - Mean Loss 1.885 - Mean Q Value 16.332 - \n",
      "Episode 498 - Step 531772 - Epsilon 0.875514980475766 - \n",
      "Mean Reward 1645.7 - Mean Length 1170.63 - Mean Loss 1.887 - Mean Q Value 16.345 - \n",
      "Episode 499 - Step 532195 - Epsilon 0.8754223996502924 - \n",
      "Mean Reward 1647.71 - Mean Length 1168.95 - Mean Loss 1.89 - Mean Q Value 16.357 - \n",
      "Episode 500 - Step 533022 - Epsilon 0.8752414247554428 - \n",
      "Mean Reward 1652.1 - Mean Length 1163.71 - Mean Loss 1.89 - Mean Q Value 16.372 - \n",
      "Episode 501 - Step 533757 - Epsilon 0.8750806138984706 - \n",
      "Mean Reward 1656.92 - Mean Length 1168.99 - Mean Loss 1.89 - Mean Q Value 16.376 - \n",
      "Episode 502 - Step 535567 - Epsilon 0.8746847294467522 - \n",
      "Mean Reward 1651.18 - Mean Length 1179.42 - Mean Loss 1.889 - Mean Q Value 16.362 - \n",
      "Episode 503 - Step 535838 - Epsilon 0.8746254715563005 - \n",
      "Mean Reward 1636.63 - Mean Length 1160.81 - Mean Loss 1.887 - Mean Q Value 16.341 - \n",
      "Episode 504 - Step 536156 - Epsilon 0.8745559415864635 - \n",
      "Mean Reward 1635.06 - Mean Length 1159.7 - Mean Loss 1.889 - Mean Q Value 16.355 - \n",
      "Episode 505 - Step 536810 - Epsilon 0.8744129633609102 - \n",
      "Mean Reward 1625.53 - Mean Length 1153.46 - Mean Loss 1.887 - Mean Q Value 16.386 - \n",
      "Episode 506 - Step 537702 - Epsilon 0.8742179909859092 - \n",
      "Mean Reward 1620.31 - Mean Length 1151.34 - Mean Loss 1.889 - Mean Q Value 16.424 - \n",
      "Episode 507 - Step 538169 - Epsilon 0.8741159319805005 - \n",
      "Mean Reward 1621.02 - Mean Length 1152.15 - Mean Loss 1.889 - Mean Q Value 16.478 - \n",
      "Episode 508 - Step 539292 - Epsilon 0.8738705583478231 - \n",
      "Mean Reward 1625.7 - Mean Length 1153.14 - Mean Loss 1.891 - Mean Q Value 16.527 - \n",
      "Episode 509 - Step 540334 - Epsilon 0.873642944686857 - \n",
      "Mean Reward 1622.66 - Mean Length 1150.52 - Mean Loss 1.895 - Mean Q Value 16.595 - \n",
      "Episode 510 - Step 540932 - Epsilon 0.8735123448128669 - \n",
      "Mean Reward 1621.68 - Mean Length 1144.59 - Mean Loss 1.9 - Mean Q Value 16.674 - \n",
      "Episode 511 - Step 542407 - Epsilon 0.8731902964767282 - \n",
      "Mean Reward 1621.05 - Mean Length 1135.04 - Mean Loss 1.906 - Mean Q Value 16.759 - \n",
      "Episode 512 - Step 542639 - Epsilon 0.8731396529018729 - \n",
      "Mean Reward 1618.43 - Mean Length 1122.61 - Mean Loss 1.91 - Mean Q Value 16.823 - \n",
      "Episode 513 - Step 542993 - Epsilon 0.873062383452146 - \n",
      "Mean Reward 1621.55 - Mean Length 1116.99 - Mean Loss 1.919 - Mean Q Value 16.903 - \n",
      "Episode 514 - Step 543406 - Epsilon 0.8729722444032832 - \n",
      "Mean Reward 1621.82 - Mean Length 1118.05 - Mean Loss 1.93 - Mean Q Value 16.995 - \n",
      "Episode 515 - Step 544881 - Epsilon 0.872650395192478 - \n",
      "Mean Reward 1621.12 - Mean Length 1124.95 - Mean Loss 1.934 - Mean Q Value 17.065 - \n",
      "Episode 516 - Step 546944 - Epsilon 0.8722004417365692 - \n",
      "Mean Reward 1629.28 - Mean Length 1123.57 - Mean Loss 1.941 - Mean Q Value 17.148 - \n",
      "Episode 517 - Step 548591 - Epsilon 0.8718413870852036 - \n",
      "Mean Reward 1637.12 - Mean Length 1137.21 - Mean Loss 1.946 - Mean Q Value 17.222 - \n",
      "Episode 518 - Step 552091 - Epsilon 0.8710788594305829 - \n",
      "Mean Reward 1643.57 - Mean Length 1167.65 - Mean Loss 1.95 - Mean Q Value 17.266 - \n",
      "Episode 519 - Step 554652 - Epsilon 0.8705213296193384 - \n",
      "Mean Reward 1647.57 - Mean Length 1152.73 - Mean Loss 1.95 - Mean Q Value 17.261 - \n",
      "Episode 520 - Step 555247 - Epsilon 0.8703918491857006 - \n",
      "Mean Reward 1643.36 - Mean Length 1143.41 - Mean Loss 1.949 - Mean Q Value 17.254 - \n",
      "Episode 521 - Step 555551 - Epsilon 0.8703257019105133 - \n",
      "Mean Reward 1635.0 - Mean Length 1128.1 - Mean Loss 1.951 - Mean Q Value 17.275 - \n",
      "Episode 522 - Step 558134 - Epsilon 0.8697638704390561 - \n",
      "Mean Reward 1631.42 - Mean Length 1144.42 - Mean Loss 1.95 - Mean Q Value 17.281 - \n",
      "Episode 523 - Step 558618 - Epsilon 0.869658635364414 - \n",
      "Mean Reward 1638.83 - Mean Length 1138.5 - Mean Loss 1.949 - Mean Q Value 17.278 - \n",
      "Episode 524 - Step 559702 - Epsilon 0.8694229897761624 - \n",
      "Mean Reward 1630.83 - Mean Length 1132.6 - Mean Loss 1.948 - Mean Q Value 17.281 - \n",
      "Episode 525 - Step 562826 - Epsilon 0.8687442354242831 - \n",
      "Mean Reward 1630.36 - Mean Length 1152.05 - Mean Loss 1.949 - Mean Q Value 17.294 - \n",
      "Episode 526 - Step 563118 - Epsilon 0.868680819401874 - \n",
      "Mean Reward 1630.73 - Mean Length 1140.25 - Mean Loss 1.947 - Mean Q Value 17.299 - \n",
      "Episode 527 - Step 563416 - Epsilon 0.8686161050833712 - \n",
      "Mean Reward 1630.37 - Mean Length 1126.57 - Mean Loss 1.945 - Mean Q Value 17.307 - \n",
      "Episode 528 - Step 563683 - Epsilon 0.8685581268861458 - \n",
      "Mean Reward 1622.61 - Mean Length 1125.4 - Mean Loss 1.944 - Mean Q Value 17.324 - \n",
      "Episode 529 - Step 565550 - Epsilon 0.8681528219251055 - \n",
      "Mean Reward 1617.12 - Mean Length 1140.54 - Mean Loss 1.945 - Mean Q Value 17.34 - \n",
      "Episode 530 - Step 566024 - Epsilon 0.8680499518980038 - \n",
      "Mean Reward 1616.8 - Mean Length 1134.4 - Mean Loss 1.945 - Mean Q Value 17.332 - \n",
      "Episode 531 - Step 566683 - Epsilon 0.8679069524304379 - \n",
      "Mean Reward 1620.0 - Mean Length 1124.08 - Mean Loss 1.947 - Mean Q Value 17.359 - \n",
      "Episode 532 - Step 568618 - Epsilon 0.8674872039245284 - \n",
      "Mean Reward 1624.22 - Mean Length 1135.19 - Mean Loss 1.949 - Mean Q Value 17.392 - \n",
      "Episode 533 - Step 569400 - Epsilon 0.8673176267316516 - \n",
      "Mean Reward 1627.85 - Mean Length 1125.89 - Mean Loss 1.95 - Mean Q Value 17.418 - \n",
      "Episode 534 - Step 569547 - Epsilon 0.867285753390557 - \n",
      "Mean Reward 1619.39 - Mean Length 1118.27 - Mean Loss 1.952 - Mean Q Value 17.467 - \n",
      "Episode 535 - Step 569815 - Epsilon 0.8672276471843882 - \n",
      "Mean Reward 1611.19 - Mean Length 1106.0 - Mean Loss 1.955 - Mean Q Value 17.499 - \n",
      "Episode 536 - Step 571556 - Epsilon 0.8668502684367351 - \n",
      "Mean Reward 1622.14 - Mean Length 1118.79 - Mean Loss 1.957 - Mean Q Value 17.535 - \n",
      "Episode 537 - Step 572958 - Epsilon 0.8665464906198064 - \n",
      "Mean Reward 1627.23 - Mean Length 1124.69 - Mean Loss 1.96 - Mean Q Value 17.577 - \n",
      "Episode 538 - Step 573991 - Epsilon 0.8663227338544384 - \n",
      "Mean Reward 1628.66 - Mean Length 1117.44 - Mean Loss 1.963 - Mean Q Value 17.618 - \n",
      "Episode 539 - Step 574239 - Epsilon 0.8662690235032562 - \n",
      "Mean Reward 1623.28 - Mean Length 1098.61 - Mean Loss 1.963 - Mean Q Value 17.662 - \n",
      "Episode 540 - Step 575217 - Epsilon 0.8660572465912906 - \n",
      "Mean Reward 1643.46 - Mean Length 1099.1 - Mean Loss 1.968 - Mean Q Value 17.729 - \n",
      "Episode 541 - Step 575854 - Epsilon 0.8659193379387816 - \n",
      "Mean Reward 1651.24 - Mean Length 1103.4 - Mean Loss 1.973 - Mean Q Value 17.821 - \n",
      "Episode 542 - Step 577211 - Epsilon 0.8656256245905741 - \n",
      "Mean Reward 1653.57 - Mean Length 1096.47 - Mean Loss 1.977 - Mean Q Value 17.887 - \n",
      "Episode 543 - Step 577614 - Epsilon 0.8655384171911299 - \n",
      "Mean Reward 1651.26 - Mean Length 1097.25 - Mean Loss 1.984 - Mean Q Value 17.966 - \n",
      "Episode 544 - Step 579128 - Epsilon 0.8652108728509113 - \n",
      "Mean Reward 1658.0 - Mean Length 1100.98 - Mean Loss 1.99 - Mean Q Value 18.047 - \n",
      "Episode 545 - Step 580439 - Epsilon 0.8649273464172893 - \n",
      "Mean Reward 1657.55 - Mean Length 1110.45 - Mean Loss 1.995 - Mean Q Value 18.099 - \n",
      "Episode 546 - Step 580676 - Epsilon 0.8648761009837619 - \n",
      "Mean Reward 1649.86 - Mean Length 1091.04 - Mean Loss 2.0 - Mean Q Value 18.159 - \n",
      "Episode 547 - Step 581250 - Epsilon 0.8647520001521879 - \n",
      "Mean Reward 1645.51 - Mean Length 1074.15 - Mean Loss 2.006 - Mean Q Value 18.251 - \n",
      "Episode 548 - Step 581864 - Epsilon 0.8646192708907865 - \n",
      "Mean Reward 1658.51 - Mean Length 1078.23 - Mean Loss 2.011 - Mean Q Value 18.317 - \n",
      "Episode 549 - Step 583491 - Epsilon 0.8642676584723746 - \n",
      "Mean Reward 1657.65 - Mean Length 1091.66 - Mean Loss 2.017 - Mean Q Value 18.381 - \n",
      "Episode 550 - Step 583825 - Epsilon 0.8641954951267231 - \n",
      "Mean Reward 1656.69 - Mean Length 1084.31 - Mean Loss 2.019 - Mean Q Value 18.437 - \n",
      "Episode 551 - Step 584669 - Epsilon 0.8640131690905092 - \n",
      "Mean Reward 1652.97 - Mean Length 1081.26 - Mean Loss 2.024 - Mean Q Value 18.505 - \n",
      "Episode 552 - Step 585082 - Epsilon 0.8639239643249129 - \n",
      "Mean Reward 1652.86 - Mean Length 1065.04 - Mean Loss 2.029 - Mean Q Value 18.569 - \n",
      "Episode 553 - Step 586011 - Epsilon 0.8637233412573466 - \n",
      "Mean Reward 1659.49 - Mean Length 1072.36 - Mean Loss 2.033 - Mean Q Value 18.632 - \n",
      "Episode 554 - Step 587506 - Epsilon 0.863400584937006 - \n",
      "Mean Reward 1659.72 - Mean Length 1071.76 - Mean Loss 2.036 - Mean Q Value 18.69 - \n",
      "Episode 555 - Step 588165 - Epsilon 0.8632583513896467 - \n",
      "Mean Reward 1659.33 - Mean Length 1074.07 - Mean Loss 2.037 - Mean Q Value 18.724 - \n",
      "Episode 556 - Step 589563 - Epsilon 0.8629566952755638 - \n",
      "Mean Reward 1658.4 - Mean Length 1072.56 - Mean Loss 2.038 - Mean Q Value 18.757 - \n",
      "Episode 557 - Step 589943 - Epsilon 0.862874718273223 - \n",
      "Mean Reward 1650.87 - Mean Length 1071.14 - Mean Loss 2.041 - Mean Q Value 18.788 - \n",
      "Episode 558 - Step 590370 - Epsilon 0.8627826113018178 - \n",
      "Mean Reward 1651.98 - Mean Length 1070.72 - Mean Loss 2.041 - Mean Q Value 18.814 - \n",
      "Episode 559 - Step 591341 - Epsilon 0.862573196215503 - \n",
      "Mean Reward 1647.7 - Mean Length 1072.54 - Mean Loss 2.039 - Mean Q Value 18.821 - \n",
      "Episode 560 - Step 592860 - Epsilon 0.8622456961912585 - \n",
      "Mean Reward 1653.33 - Mean Length 1085.25 - Mean Loss 2.036 - Mean Q Value 18.825 - \n",
      "Episode 561 - Step 595648 - Epsilon 0.8616449202605667 - \n",
      "Mean Reward 1653.89 - Mean Length 1097.88 - Mean Loss 2.033 - Mean Q Value 18.808 - \n",
      "Episode 562 - Step 596062 - Epsilon 0.8615557446150788 - \n",
      "Mean Reward 1658.57 - Mean Length 1098.24 - Mean Loss 2.026 - Mean Q Value 18.748 - \n",
      "Episode 563 - Step 596645 - Epsilon 0.8614301820001861 - \n",
      "Mean Reward 1650.08 - Mean Length 1097.78 - Mean Loss 2.02 - Mean Q Value 18.7 - \n",
      "Episode 564 - Step 597654 - Epsilon 0.8612129136137148 - \n",
      "Mean Reward 1645.03 - Mean Length 1098.95 - Mean Loss 2.012 - Mean Q Value 18.645 - \n",
      "Episode 565 - Step 598128 - Epsilon 0.8611108659171266 - \n",
      "Mean Reward 1646.08 - Mean Length 1096.78 - Mean Loss 2.006 - Mean Q Value 18.573 - \n",
      "Episode 566 - Step 598668 - Epsilon 0.8609946237822019 - \n",
      "Mean Reward 1657.24 - Mean Length 1100.17 - Mean Loss 1.994 - Mean Q Value 18.502 - \n",
      "Episode 567 - Step 600287 - Epsilon 0.8606462066802916 - \n",
      "Mean Reward 1658.04 - Mean Length 1110.38 - Mean Loss 1.986 - Mean Q Value 18.431 - \n",
      "Episode 568 - Step 600927 - Epsilon 0.8605085142856769 - \n",
      "Mean Reward 1651.27 - Mean Length 1107.11 - Mean Loss 1.974 - Mean Q Value 18.325 - \n",
      "Episode 569 - Step 601623 - Epsilon 0.8603587988110802 - \n",
      "Mean Reward 1655.25 - Mean Length 1082.7 - Mean Loss 1.964 - Mean Q Value 18.27 - \n",
      "Episode 570 - Step 601979 - Epsilon 0.8602822302757553 - \n",
      "Mean Reward 1652.32 - Mean Length 1073.13 - Mean Loss 1.961 - Mean Q Value 18.262 - \n",
      "Episode 571 - Step 602790 - Epsilon 0.8601078257126025 - \n",
      "Mean Reward 1651.35 - Mean Length 1067.3 - Mean Loss 1.958 - Mean Q Value 18.273 - \n",
      "Episode 572 - Step 603445 - Epsilon 0.8599669945693829 - \n",
      "Mean Reward 1655.78 - Mean Length 1062.29 - Mean Loss 1.957 - Mean Q Value 18.302 - \n",
      "Episode 573 - Step 604356 - Epsilon 0.8597711593634434 - \n",
      "Mean Reward 1654.58 - Mean Length 1030.24 - Mean Loss 1.967 - Mean Q Value 18.423 - \n",
      "Episode 574 - Step 604922 - Epsilon 0.859649510336041 - \n",
      "Mean Reward 1654.68 - Mean Length 1016.39 - Mean Loss 1.978 - Mean Q Value 18.561 - \n",
      "Episode 575 - Step 605218 - Epsilon 0.8595858986179783 - \n",
      "Mean Reward 1645.95 - Mean Length 992.24 - Mean Loss 1.988 - Mean Q Value 18.729 - \n",
      "Episode 576 - Step 605643 - Epsilon 0.8594945724566101 - \n",
      "Mean Reward 1642.81 - Mean Length 984.86 - Mean Loss 2.001 - Mean Q Value 18.906 - \n",
      "Episode 577 - Step 606020 - Epsilon 0.8594135689003718 - \n",
      "Mean Reward 1640.08 - Mean Length 977.96 - Mean Loss 2.011 - Mean Q Value 19.077 - \n",
      "Episode 578 - Step 606563 - Epsilon 0.8592969114121013 - \n",
      "Mean Reward 1637.37 - Mean Length 961.18 - Mean Loss 2.021 - Mean Q Value 19.227 - \n",
      "Episode 579 - Step 608801 - Epsilon 0.8588162692022159 - \n",
      "Mean Reward 1640.74 - Mean Length 974.38 - Mean Loss 2.029 - Mean Q Value 19.375 - \n",
      "Episode 580 - Step 610680 - Epsilon 0.8584129349498346 - \n",
      "Mean Reward 1632.89 - Mean Length 968.62 - Mean Loss 2.036 - Mean Q Value 19.504 - \n",
      "Episode 581 - Step 612586 - Epsilon 0.8580039985719863 - \n",
      "Mean Reward 1633.96 - Mean Length 981.99 - Mean Loss 2.042 - Mean Q Value 19.593 - \n",
      "Episode 582 - Step 612988 - Epsilon 0.8579177734922199 - \n",
      "Mean Reward 1638.59 - Mean Length 975.23 - Mean Loss 2.048 - Mean Q Value 19.675 - \n",
      "Episode 583 - Step 613827 - Epsilon 0.8577378440874698 - \n",
      "Mean Reward 1634.46 - Mean Length 935.47 - Mean Loss 2.055 - Mean Q Value 19.766 - \n",
      "Episode 584 - Step 615028 - Epsilon 0.8574803469262556 - \n",
      "Mean Reward 1633.5 - Mean Length 941.02 - Mean Loss 2.063 - Mean Q Value 19.865 - \n",
      "Episode 585 - Step 615283 - Epsilon 0.8574256842896887 - \n",
      "Mean Reward 1629.78 - Mean Length 932.22 - Mean Loss 2.067 - Mean Q Value 19.936 - \n",
      "Episode 586 - Step 615416 - Epsilon 0.8573971753560822 - \n",
      "Mean Reward 1622.38 - Mean Length 928.4 - Mean Loss 2.073 - Mean Q Value 20.013 - \n",
      "Episode 587 - Step 615930 - Epsilon 0.8572870068837378 - \n",
      "Mean Reward 1622.49 - Mean Length 927.89 - Mean Loss 2.077 - Mean Q Value 20.077 - \n",
      "Episode 588 - Step 618605 - Epsilon 0.8567138877842232 - \n",
      "Mean Reward 1620.09 - Mean Length 949.89 - Mean Loss 2.079 - Mean Q Value 20.105 - \n",
      "Episode 589 - Step 620696 - Epsilon 0.856266157579033 - \n",
      "Mean Reward 1635.49 - Mean Length 969.53 - Mean Loss 2.08 - Mean Q Value 20.093 - \n",
      "Episode 590 - Step 621763 - Epsilon 0.8560377790142674 - \n",
      "Mean Reward 1639.04 - Mean Length 973.45 - Mean Loss 2.083 - Mean Q Value 20.116 - \n",
      "Episode 591 - Step 623426 - Epsilon 0.8556819552351078 - \n",
      "Mean Reward 1638.86 - Mean Length 984.86 - Mean Loss 2.084 - Mean Q Value 20.116 - \n",
      "Episode 592 - Step 623801 - Epsilon 0.8556017388019698 - \n",
      "Mean Reward 1635.09 - Mean Length 970.93 - Mean Loss 2.087 - Mean Q Value 20.099 - \n",
      "Episode 593 - Step 624245 - Epsilon 0.8555067722678189 - \n",
      "Mean Reward 1633.22 - Mean Length 959.54 - Mean Loss 2.086 - Mean Q Value 20.081 - \n",
      "Episode 594 - Step 626314 - Epsilon 0.8550643757591622 - \n",
      "Mean Reward 1628.24 - Mean Length 970.19 - Mean Loss 2.083 - Mean Q Value 20.035 - \n",
      "Episode 595 - Step 626706 - Epsilon 0.8549805835457377 - \n",
      "Mean Reward 1636.21 - Mean Length 972.54 - Mean Loss 2.08 - Mean Q Value 19.969 - \n",
      "Episode 596 - Step 626826 - Epsilon 0.8549549345097591 - \n",
      "Mean Reward 1614.85 - Mean Length 963.47 - Mean Loss 2.073 - Mean Q Value 19.885 - \n",
      "Episode 597 - Step 627659 - Epsilon 0.8547769086599545 - \n",
      "Mean Reward 1605.24 - Mean Length 962.59 - Mean Loss 2.069 - Mean Q Value 19.805 - \n",
      "Episode 598 - Step 627803 - Epsilon 0.8547461372412809 - \n",
      "Mean Reward 1600.55 - Mean Length 960.31 - Mean Loss 2.061 - Mean Q Value 19.721 - \n",
      "Episode 599 - Step 628097 - Epsilon 0.8546833157010519 - \n",
      "Mean Reward 1595.3 - Mean Length 959.02 - Mean Loss 2.05 - Mean Q Value 19.643 - \n",
      "Episode 600 - Step 629574 - Epsilon 0.8543677821062212 - \n",
      "Mean Reward 1595.15 - Mean Length 965.52 - Mean Loss 2.046 - Mean Q Value 19.574 - \n",
      "Episode 601 - Step 631104 - Epsilon 0.8540410488803907 - \n",
      "Mean Reward 1600.06 - Mean Length 973.47 - Mean Loss 2.043 - Mean Q Value 19.528 - \n",
      "Episode 602 - Step 632049 - Epsilon 0.8538393054892237 - \n",
      "Mean Reward 1605.69 - Mean Length 964.82 - Mean Loss 2.041 - Mean Q Value 19.504 - \n",
      "Episode 603 - Step 634143 - Epsilon 0.8533924375347706 - \n",
      "Mean Reward 1604.08 - Mean Length 983.05 - Mean Loss 2.039 - Mean Q Value 19.493 - \n",
      "Episode 604 - Step 634285 - Epsilon 0.8533621426371847 - \n",
      "Mean Reward 1596.01 - Mean Length 981.29 - Mean Loss 2.04 - Mean Q Value 19.452 - \n",
      "Episode 605 - Step 635575 - Epsilon 0.8530869776844846 - \n",
      "Mean Reward 1595.52 - Mean Length 987.65 - Mean Loss 2.036 - Mean Q Value 19.376 - \n",
      "Episode 606 - Step 637026 - Epsilon 0.8527775764656493 - \n",
      "Mean Reward 1604.58 - Mean Length 993.24 - Mean Loss 2.031 - Mean Q Value 19.294 - \n",
      "Episode 607 - Step 637384 - Epsilon 0.852701256278384 - \n",
      "Mean Reward 1603.14 - Mean Length 992.15 - Mean Loss 2.027 - Mean Q Value 19.206 - \n",
      "Episode 608 - Step 638144 - Epsilon 0.8525392584097051 - \n",
      "Mean Reward 1599.39 - Mean Length 988.52 - Mean Loss 2.024 - Mean Q Value 19.122 - \n",
      "Episode 609 - Step 640289 - Epsilon 0.852082206733119 - \n",
      "Mean Reward 1598.18 - Mean Length 999.55 - Mean Loss 2.017 - Mean Q Value 19.032 - \n",
      "Episode 610 - Step 641866 - Epsilon 0.8517463394932636 - \n",
      "Mean Reward 1596.78 - Mean Length 1009.34 - Mean Loss 2.007 - Mean Q Value 18.929 - \n",
      "Episode 611 - Step 642457 - Epsilon 0.851620503252237 - \n",
      "Mean Reward 1592.91 - Mean Length 1000.5 - Mean Loss 1.999 - Mean Q Value 18.83 - \n",
      "Episode 612 - Step 642606 - Epsilon 0.8515887809753523 - \n",
      "Mean Reward 1589.24 - Mean Length 999.67 - Mean Loss 1.994 - Mean Q Value 18.777 - \n",
      "Episode 613 - Step 643606 - Epsilon 0.8513759103634054 - \n",
      "Mean Reward 1588.68 - Mean Length 1006.13 - Mean Loss 1.988 - Mean Q Value 18.718 - \n",
      "Episode 614 - Step 644613 - Epsilon 0.8511616034281698 - \n",
      "Mean Reward 1588.65 - Mean Length 1012.07 - Mean Loss 1.98 - Mean Q Value 18.654 - \n",
      "Episode 615 - Step 645434 - Epsilon 0.8509869204146641 - \n",
      "Mean Reward 1588.5 - Mean Length 1005.53 - Mean Loss 1.976 - Mean Q Value 18.609 - \n",
      "Episode 616 - Step 645873 - Epsilon 0.8508935297133706 - \n",
      "Mean Reward 1579.26 - Mean Length 989.29 - Mean Loss 1.975 - Mean Q Value 18.577 - \n",
      "Episode 617 - Step 646006 - Epsilon 0.8508652379703182 - \n",
      "Mean Reward 1563.51 - Mean Length 974.15 - Mean Loss 1.971 - Mean Q Value 18.579 - \n",
      "Episode 618 - Step 646253 - Epsilon 0.8508126986574661 - \n",
      "Mean Reward 1557.67 - Mean Length 941.62 - Mean Loss 1.972 - Mean Q Value 18.614 - \n",
      "Episode 619 - Step 647544 - Epsilon 0.8505381431333977 - \n",
      "Mean Reward 1558.91 - Mean Length 928.92 - Mean Loss 1.977 - Mean Q Value 18.679 - \n",
      "Episode 620 - Step 647916 - Epsilon 0.850459046754227 - \n",
      "Mean Reward 1556.42 - Mean Length 926.69 - Mean Loss 1.98 - Mean Q Value 18.745 - \n",
      "Episode 621 - Step 648482 - Epsilon 0.8503387152977044 - \n",
      "Mean Reward 1565.61 - Mean Length 929.31 - Mean Loss 1.985 - Mean Q Value 18.804 - \n",
      "Episode 622 - Step 649240 - Epsilon 0.8501775913579682 - \n",
      "Mean Reward 1571.09 - Mean Length 911.06 - Mean Loss 1.99 - Mean Q Value 18.895 - \n",
      "Episode 623 - Step 649507 - Epsilon 0.8501208438906119 - \n",
      "Mean Reward 1561.56 - Mean Length 908.89 - Mean Loss 1.998 - Mean Q Value 18.99 - \n",
      "Episode 624 - Step 650667 - Epsilon 0.8498743445591679 - \n",
      "Mean Reward 1569.68 - Mean Length 909.65 - Mean Loss 2.011 - Mean Q Value 19.131 - \n",
      "Episode 625 - Step 652012 - Epsilon 0.8495886223147993 - \n",
      "Mean Reward 1575.06 - Mean Length 891.86 - Mean Loss 2.019 - Mean Q Value 19.26 - \n",
      "Episode 626 - Step 654148 - Epsilon 0.8491350630448286 - \n",
      "Mean Reward 1574.04 - Mean Length 910.3 - Mean Loss 2.028 - Mean Q Value 19.372 - \n",
      "Episode 627 - Step 657628 - Epsilon 0.8483966367096082 - \n",
      "Mean Reward 1585.19 - Mean Length 942.12 - Mean Loss 2.033 - Mean Q Value 19.423 - \n",
      "Episode 628 - Step 659698 - Epsilon 0.8479577049785644 - \n",
      "Mean Reward 1594.85 - Mean Length 960.15 - Mean Loss 2.032 - Mean Q Value 19.397 - \n",
      "Episode 629 - Step 660722 - Epsilon 0.8477406555624382 - \n",
      "Mean Reward 1608.18 - Mean Length 951.72 - Mean Loss 2.028 - Mean Q Value 19.346 - \n",
      "Episode 630 - Step 662953 - Epsilon 0.8472679599878777 - \n",
      "Mean Reward 1606.76 - Mean Length 969.29 - Mean Loss 2.026 - Mean Q Value 19.318 - \n",
      "Episode 631 - Step 665775 - Epsilon 0.8466704231732275 - \n",
      "Mean Reward 1601.84 - Mean Length 990.92 - Mean Loss 2.02 - Mean Q Value 19.253 - \n",
      "Episode 632 - Step 666332 - Epsilon 0.846552532510376 - \n",
      "Mean Reward 1604.78 - Mean Length 977.14 - Mean Loss 2.015 - Mean Q Value 19.188 - \n",
      "Episode 633 - Step 668615 - Epsilon 0.8460695004503826 - \n",
      "Mean Reward 1607.29 - Mean Length 992.15 - Mean Loss 2.012 - Mean Q Value 19.137 - \n",
      "Episode 634 - Step 668893 - Epsilon 0.8460107006560595 - \n",
      "Mean Reward 1610.48 - Mean Length 993.46 - Mean Loss 2.009 - Mean Q Value 19.079 - \n",
      "Episode 635 - Step 669410 - Epsilon 0.8459013608255553 - \n",
      "Mean Reward 1610.32 - Mean Length 995.95 - Mean Loss 2.008 - Mean Q Value 19.038 - \n",
      "Episode 636 - Step 671740 - Epsilon 0.8454087667031953 - \n",
      "Mean Reward 1606.3 - Mean Length 1001.84 - Mean Loss 2.004 - Mean Q Value 18.975 - \n",
      "Episode 637 - Step 675274 - Epsilon 0.84466217781859 - \n",
      "Mean Reward 1599.01 - Mean Length 1023.16 - Mean Loss 1.996 - Mean Q Value 18.891 - \n",
      "Episode 638 - Step 675977 - Epsilon 0.8445137414664907 - \n",
      "Mean Reward 1600.5 - Mean Length 1019.86 - Mean Loss 1.988 - Mean Q Value 18.814 - \n",
      "Episode 639 - Step 676492 - Epsilon 0.8444050173079389 - \n",
      "Mean Reward 1605.29 - Mean Length 1022.53 - Mean Loss 1.985 - Mean Q Value 18.749 - \n",
      "Episode 640 - Step 676700 - Epsilon 0.8443611093831606 - \n",
      "Mean Reward 1585.43 - Mean Length 1014.83 - Mean Loss 1.978 - Mean Q Value 18.659 - \n",
      "Episode 641 - Step 678802 - Epsilon 0.8439175141295291 - \n",
      "Mean Reward 1591.68 - Mean Length 1029.48 - Mean Loss 1.973 - Mean Q Value 18.566 - \n",
      "Episode 642 - Step 679056 - Epsilon 0.8438639270620834 - \n",
      "Mean Reward 1584.71 - Mean Length 1018.45 - Mean Loss 1.965 - Mean Q Value 18.471 - \n",
      "Episode 643 - Step 679694 - Epsilon 0.8437293414823623 - \n",
      "Mean Reward 1588.42 - Mean Length 1020.8 - Mean Loss 1.957 - Mean Q Value 18.392 - \n",
      "Episode 644 - Step 681367 - Epsilon 0.843376525428937 - \n",
      "Mean Reward 1575.13 - Mean Length 1022.39 - Mean Loss 1.949 - Mean Q Value 18.286 - \n",
      "Episode 645 - Step 681580 - Epsilon 0.8433316168190405 - \n",
      "Mean Reward 1571.69 - Mean Length 1011.41 - Mean Loss 1.944 - Mean Q Value 18.21 - \n",
      "Episode 646 - Step 682349 - Epsilon 0.8431695018792182 - \n",
      "Mean Reward 1576.34 - Mean Length 1016.73 - Mean Loss 1.938 - Mean Q Value 18.154 - \n",
      "Episode 647 - Step 682693 - Epsilon 0.8430969924109343 - \n",
      "Mean Reward 1580.03 - Mean Length 1014.43 - Mean Loss 1.933 - Mean Q Value 18.094 - \n",
      "Episode 648 - Step 684968 - Epsilon 0.8426176172717362 - \n",
      "Mean Reward 1577.08 - Mean Length 1031.04 - Mean Loss 1.93 - Mean Q Value 18.059 - \n",
      "Episode 649 - Step 685347 - Epsilon 0.8425377830247172 - \n",
      "Mean Reward 1579.74 - Mean Length 1018.56 - Mean Loss 1.926 - Mean Q Value 18.039 - \n",
      "Episode 650 - Step 686840 - Epsilon 0.8422233644398744 - \n",
      "Mean Reward 1587.42 - Mean Length 1030.15 - Mean Loss 1.922 - Mean Q Value 18.023 - \n",
      "Episode 651 - Step 689078 - Epsilon 0.8417522722088571 - \n",
      "Mean Reward 1584.14 - Mean Length 1044.09 - Mean Loss 1.916 - Mean Q Value 17.985 - \n",
      "Episode 652 - Step 689558 - Epsilon 0.841651267983927 - \n",
      "Mean Reward 1581.21 - Mean Length 1044.76 - Mean Loss 1.91 - Mean Q Value 17.926 - \n",
      "Episode 653 - Step 690208 - Epsilon 0.8415145107475925 - \n",
      "Mean Reward 1579.36 - Mean Length 1041.97 - Mean Loss 1.906 - Mean Q Value 17.876 - \n",
      "Episode 654 - Step 690570 - Epsilon 0.8414383571208434 - \n",
      "Mean Reward 1573.31 - Mean Length 1030.64 - Mean Loss 1.906 - Mean Q Value 17.852 - \n",
      "Episode 655 - Step 691769 - Epsilon 0.8411861737396095 - \n",
      "Mean Reward 1579.22 - Mean Length 1036.04 - Mean Loss 1.906 - Mean Q Value 17.85 - \n",
      "Episode 656 - Step 692909 - Epsilon 0.8409464698095298 - \n",
      "Mean Reward 1583.09 - Mean Length 1033.46 - Mean Loss 1.904 - Mean Q Value 17.83 - \n",
      "Episode 657 - Step 693194 - Epsilon 0.8408865545005666 - \n",
      "Mean Reward 1585.12 - Mean Length 1032.51 - Mean Loss 1.901 - Mean Q Value 17.786 - \n",
      "Episode 658 - Step 693596 - Epsilon 0.8408020496377043 - \n",
      "Mean Reward 1583.84 - Mean Length 1032.26 - Mean Loss 1.899 - Mean Q Value 17.751 - \n",
      "Episode 659 - Step 694927 - Epsilon 0.840522319263403 - \n",
      "Mean Reward 1587.22 - Mean Length 1035.86 - Mean Loss 1.899 - Mean Q Value 17.75 - \n",
      "Episode 660 - Step 696053 - Epsilon 0.8402857455002453 - \n",
      "Mean Reward 1584.85 - Mean Length 1031.93 - Mean Loss 1.897 - Mean Q Value 17.74 - \n",
      "Episode 661 - Step 696464 - Epsilon 0.8401994105646272 - \n",
      "Mean Reward 1580.22 - Mean Length 1008.16 - Mean Loss 1.896 - Mean Q Value 17.742 - \n",
      "Episode 662 - Step 698506 - Epsilon 0.839770598175547 - \n",
      "Mean Reward 1587.61 - Mean Length 1024.44 - Mean Loss 1.898 - Mean Q Value 17.791 - \n",
      "Episode 663 - Step 698635 - Epsilon 0.8397435160070689 - \n",
      "Mean Reward 1581.12 - Mean Length 1019.9 - Mean Loss 1.902 - Mean Q Value 17.84 - \n",
      "Episode 664 - Step 699065 - Epsilon 0.8396532484197723 - \n",
      "Mean Reward 1578.95 - Mean Length 1014.11 - Mean Loss 1.904 - Mean Q Value 17.893 - \n",
      "Episode 665 - Step 699775 - Epsilon 0.8395042231759092 - \n",
      "Mean Reward 1582.25 - Mean Length 1016.47 - Mean Loss 1.907 - Mean Q Value 17.96 - \n",
      "Episode 666 - Step 700418 - Epsilon 0.839369283701198 - \n",
      "Mean Reward 1580.45 - Mean Length 1017.5 - Mean Loss 1.911 - Mean Q Value 18.015 - \n",
      "Episode 667 - Step 702326 - Epsilon 0.8389689999780393 - \n",
      "Mean Reward 1578.99 - Mean Length 1020.39 - Mean Loss 1.913 - Mean Q Value 18.053 - \n",
      "Episode 668 - Step 703175 - Epsilon 0.8387909476819798 - \n",
      "Mean Reward 1579.96 - Mean Length 1022.48 - Mean Loss 1.918 - Mean Q Value 18.112 - \n",
      "Episode 669 - Step 703925 - Epsilon 0.8386336891030629 - \n",
      "Mean Reward 1581.05 - Mean Length 1023.02 - Mean Loss 1.92 - Mean Q Value 18.126 - \n",
      "Episode 670 - Step 705121 - Epsilon 0.8383829750822618 - \n",
      "Mean Reward 1575.77 - Mean Length 1031.42 - Mean Loss 1.92 - Mean Q Value 18.134 - \n",
      "Episode 671 - Step 705370 - Epsilon 0.8383307873598917 - \n",
      "Mean Reward 1574.63 - Mean Length 1025.8 - Mean Loss 1.92 - Mean Q Value 18.153 - \n",
      "Episode 672 - Step 706091 - Epsilon 0.8381796918344553 - \n",
      "Mean Reward 1568.8 - Mean Length 1026.46 - Mean Loss 1.921 - Mean Q Value 18.175 - \n",
      "Episode 673 - Step 707349 - Epsilon 0.8379161257363323 - \n",
      "Mean Reward 1563.83 - Mean Length 1029.93 - Mean Loss 1.916 - Mean Q Value 18.142 - \n",
      "Episode 674 - Step 707922 - Epsilon 0.8377961028331463 - \n",
      "Mean Reward 1564.7 - Mean Length 1030.0 - Mean Loss 1.913 - Mean Q Value 18.108 - \n",
      "Episode 675 - Step 708976 - Epsilon 0.8375753726149111 - \n",
      "Mean Reward 1569.34 - Mean Length 1037.58 - Mean Loss 1.911 - Mean Q Value 18.044 - \n",
      "Episode 676 - Step 710115 - Epsilon 0.8373369069508766 - \n",
      "Mean Reward 1573.43 - Mean Length 1044.72 - Mean Loss 1.904 - Mean Q Value 17.977 - \n",
      "Episode 677 - Step 711398 - Epsilon 0.8370683741726141 - \n",
      "Mean Reward 1569.05 - Mean Length 1053.78 - Mean Loss 1.902 - Mean Q Value 17.892 - \n",
      "Episode 678 - Step 712700 - Epsilon 0.8367959527217269 - \n",
      "Mean Reward 1574.08 - Mean Length 1061.37 - Mean Loss 1.895 - Mean Q Value 17.808 - \n",
      "Episode 679 - Step 713727 - Epsilon 0.8365811329126589 - \n",
      "Mean Reward 1582.37 - Mean Length 1049.26 - Mean Loss 1.891 - Mean Q Value 17.73 - \n",
      "Episode 680 - Step 714008 - Epsilon 0.8365223651449595 - \n",
      "Mean Reward 1580.04 - Mean Length 1033.28 - Mean Loss 1.889 - Mean Q Value 17.691 - \n",
      "Episode 681 - Step 714930 - Epsilon 0.8363295689362836 - \n",
      "Mean Reward 1587.32 - Mean Length 1023.44 - Mean Loss 1.888 - Mean Q Value 17.682 - \n",
      "Episode 682 - Step 715069 - Epsilon 0.8363005069850807 - \n",
      "Mean Reward 1576.6 - Mean Length 1020.81 - Mean Loss 1.888 - Mean Q Value 17.681 - \n",
      "Episode 683 - Step 715674 - Epsilon 0.8361740260829308 - \n",
      "Mean Reward 1578.15 - Mean Length 1018.47 - Mean Loss 1.888 - Mean Q Value 17.692 - \n",
      "Episode 684 - Step 716174 - Epsilon 0.8360695108489298 - \n",
      "Mean Reward 1573.82 - Mean Length 1011.46 - Mean Loss 1.891 - Mean Q Value 17.717 - \n",
      "Episode 685 - Step 718619 - Epsilon 0.8355586194535812 - \n",
      "Mean Reward 1576.45 - Mean Length 1033.36 - Mean Loss 1.894 - Mean Q Value 17.749 - \n",
      "Episode 686 - Step 719666 - Epsilon 0.8353399405784206 - \n",
      "Mean Reward 1589.29 - Mean Length 1042.5 - Mean Loss 1.894 - Mean Q Value 17.755 - \n",
      "Episode 687 - Step 720031 - Epsilon 0.8352637192769545 - \n",
      "Mean Reward 1589.04 - Mean Length 1041.01 - Mean Loss 1.895 - Mean Q Value 17.76 - \n",
      "Episode 688 - Step 723238 - Epsilon 0.8345943148895983 - \n",
      "Mean Reward 1593.42 - Mean Length 1046.33 - Mean Loss 1.896 - Mean Q Value 17.796 - \n",
      "Episode 689 - Step 724631 - Epsilon 0.8343037179861963 - \n",
      "Mean Reward 1586.08 - Mean Length 1039.35 - Mean Loss 1.894 - Mean Q Value 17.824 - \n",
      "Episode 690 - Step 727826 - Epsilon 0.8336375838808164 - \n",
      "Mean Reward 1585.48 - Mean Length 1060.63 - Mean Loss 1.888 - Mean Q Value 17.788 - \n",
      "Episode 691 - Step 728265 - Epsilon 0.8335460971649626 - \n",
      "Mean Reward 1581.84 - Mean Length 1048.39 - Mean Loss 1.883 - Mean Q Value 17.757 - \n",
      "Episode 692 - Step 730054 - Epsilon 0.833173376982127 - \n",
      "Mean Reward 1587.09 - Mean Length 1062.53 - Mean Loss 1.876 - Mean Q Value 17.724 - \n",
      "Episode 693 - Step 731780 - Epsilon 0.8328139401790495 - \n",
      "Mean Reward 1588.71 - Mean Length 1075.35 - Mean Loss 1.869 - Mean Q Value 17.664 - \n",
      "Episode 694 - Step 732078 - Epsilon 0.8327518978438483 - \n",
      "Mean Reward 1586.55 - Mean Length 1057.64 - Mean Loss 1.866 - Mean Q Value 17.624 - \n",
      "Episode 695 - Step 732225 - Epsilon 0.8327212947701079 - \n",
      "Mean Reward 1578.84 - Mean Length 1055.19 - Mean Loss 1.863 - Mean Q Value 17.607 - \n",
      "Episode 696 - Step 733472 - Epsilon 0.8324617343350674 - \n",
      "Mean Reward 1591.02 - Mean Length 1066.46 - Mean Loss 1.86 - Mean Q Value 17.593 - \n",
      "Episode 697 - Step 734399 - Epsilon 0.8322688336572244 - \n",
      "Mean Reward 1594.0 - Mean Length 1067.4 - Mean Loss 1.857 - Mean Q Value 17.579 - \n",
      "Episode 698 - Step 737788 - Epsilon 0.8315639924302315 - \n",
      "Mean Reward 1607.24 - Mean Length 1099.85 - Mean Loss 1.854 - Mean Q Value 17.563 - \n",
      "Episode 699 - Step 738302 - Epsilon 0.8314571433090362 - \n",
      "Mean Reward 1606.98 - Mean Length 1102.05 - Mean Loss 1.851 - Mean Q Value 17.523 - \n",
      "Episode 700 - Step 738734 - Epsilon 0.8313673507752065 - \n",
      "Mean Reward 1602.54 - Mean Length 1091.6 - Mean Loss 1.845 - Mean Q Value 17.474 - \n",
      "Episode 701 - Step 739446 - Epsilon 0.8312193805379938 - \n",
      "Mean Reward 1600.04 - Mean Length 1083.42 - Mean Loss 1.838 - Mean Q Value 17.413 - \n",
      "Episode 702 - Step 740763 - Epsilon 0.8309457465721732 - \n",
      "Mean Reward 1594.55 - Mean Length 1087.14 - Mean Loss 1.833 - Mean Q Value 17.369 - \n",
      "Episode 703 - Step 741413 - Epsilon 0.830810728841947 - \n",
      "Mean Reward 1608.89 - Mean Length 1072.7 - Mean Loss 1.829 - Mean Q Value 17.339 - \n",
      "Episode 704 - Step 741568 - Epsilon 0.8307785355459248 - \n",
      "Mean Reward 1613.97 - Mean Length 1072.83 - Mean Loss 1.823 - Mean Q Value 17.338 - \n",
      "Episode 705 - Step 742685 - Epsilon 0.8305465730001248 - \n",
      "Mean Reward 1615.93 - Mean Length 1071.1 - Mean Loss 1.82 - Mean Q Value 17.336 - \n",
      "Episode 706 - Step 744353 - Epsilon 0.8302003072372808 - \n",
      "Mean Reward 1613.44 - Mean Length 1073.27 - Mean Loss 1.815 - Mean Q Value 17.317 - \n",
      "Episode 707 - Step 745370 - Epsilon 0.8299892556138303 - \n",
      "Mean Reward 1615.69 - Mean Length 1079.86 - Mean Loss 1.812 - Mean Q Value 17.301 - \n",
      "Episode 708 - Step 745955 - Epsilon 0.829867878545922 - \n",
      "Mean Reward 1613.62 - Mean Length 1078.11 - Mean Loss 1.809 - Mean Q Value 17.305 - \n",
      "Episode 709 - Step 746875 - Epsilon 0.8296770308582992 - \n",
      "Mean Reward 1618.9 - Mean Length 1065.86 - Mean Loss 1.808 - Mean Q Value 17.335 - \n",
      "Episode 710 - Step 746995 - Epsilon 0.8296521409176094 - \n",
      "Mean Reward 1608.51 - Mean Length 1051.29 - Mean Loss 1.808 - Mean Q Value 17.4 - \n",
      "Episode 711 - Step 748408 - Epsilon 0.8293591180203762 - \n",
      "Mean Reward 1604.93 - Mean Length 1059.51 - Mean Loss 1.812 - Mean Q Value 17.46 - \n",
      "Episode 712 - Step 749321 - Epsilon 0.8291698383803614 - \n",
      "Mean Reward 1614.23 - Mean Length 1067.15 - Mean Loss 1.813 - Mean Q Value 17.485 - \n",
      "Episode 713 - Step 752042 - Epsilon 0.8286059873288238 - \n",
      "Mean Reward 1627.92 - Mean Length 1084.36 - Mean Loss 1.813 - Mean Q Value 17.499 - \n",
      "Episode 714 - Step 753066 - Epsilon 0.8283938913189731 - \n",
      "Mean Reward 1628.08 - Mean Length 1084.53 - Mean Loss 1.811 - Mean Q Value 17.479 - \n",
      "Episode 715 - Step 753312 - Epsilon 0.8283429466548455 - \n",
      "Mean Reward 1628.66 - Mean Length 1078.78 - Mean Loss 1.809 - Mean Q Value 17.466 - \n",
      "Episode 716 - Step 753961 - Epsilon 0.8282085588974355 - \n",
      "Mean Reward 1637.2 - Mean Length 1080.88 - Mean Loss 1.806 - Mean Q Value 17.448 - \n",
      "Episode 717 - Step 754215 - Epsilon 0.8281559693171012 - \n",
      "Mean Reward 1641.55 - Mean Length 1082.09 - Mean Loss 1.807 - Mean Q Value 17.441 - \n",
      "Episode 718 - Step 754878 - Epsilon 0.8280187138233744 - \n",
      "Mean Reward 1645.36 - Mean Length 1086.25 - Mean Loss 1.805 - Mean Q Value 17.412 - \n",
      "Episode 719 - Step 755399 - Epsilon 0.8279108713957942 - \n",
      "Mean Reward 1649.91 - Mean Length 1078.55 - Mean Loss 1.806 - Mean Q Value 17.428 - \n",
      "Episode 720 - Step 756143 - Epsilon 0.8277568942747604 - \n",
      "Mean Reward 1659.68 - Mean Length 1082.27 - Mean Loss 1.807 - Mean Q Value 17.452 - \n",
      "Episode 721 - Step 756834 - Epsilon 0.8276139116038648 - \n",
      "Mean Reward 1661.02 - Mean Length 1083.52 - Mean Loss 1.805 - Mean Q Value 17.471 - \n",
      "Episode 722 - Step 757217 - Epsilon 0.8275346713555999 - \n",
      "Mean Reward 1650.93 - Mean Length 1079.77 - Mean Loss 1.803 - Mean Q Value 17.463 - \n",
      "Episode 723 - Step 757775 - Epsilon 0.8274192383061431 - \n",
      "Mean Reward 1655.62 - Mean Length 1082.68 - Mean Loss 1.801 - Mean Q Value 17.469 - \n",
      "Episode 724 - Step 760612 - Epsilon 0.8268325991994492 - \n",
      "Mean Reward 1655.46 - Mean Length 1099.45 - Mean Loss 1.794 - Mean Q Value 17.434 - \n",
      "Episode 725 - Step 761379 - Epsilon 0.8266740692282611 - \n",
      "Mean Reward 1650.36 - Mean Length 1093.67 - Mean Loss 1.789 - Mean Q Value 17.383 - \n",
      "Episode 726 - Step 761697 - Epsilon 0.8266083512438585 - \n",
      "Mean Reward 1652.62 - Mean Length 1075.49 - Mean Loss 1.786 - Mean Q Value 17.376 - \n",
      "Episode 727 - Step 763921 - Epsilon 0.8261488846866093 - \n",
      "Mean Reward 1642.49 - Mean Length 1062.93 - Mean Loss 1.786 - Mean Q Value 17.401 - \n",
      "Episode 728 - Step 764260 - Epsilon 0.82607887152672 - \n",
      "Mean Reward 1640.11 - Mean Length 1045.62 - Mean Loss 1.788 - Mean Q Value 17.448 - \n",
      "Episode 729 - Step 765176 - Epsilon 0.825889721100022 - \n",
      "Mean Reward 1635.64 - Mean Length 1044.54 - Mean Loss 1.794 - Mean Q Value 17.516 - \n",
      "Episode 730 - Step 769101 - Epsilon 0.8250797141844659 - \n",
      "Mean Reward 1641.45 - Mean Length 1061.48 - Mean Loss 1.794 - Mean Q Value 17.541 - \n",
      "Episode 731 - Step 771291 - Epsilon 0.8246281066232861 - \n",
      "Mean Reward 1642.34 - Mean Length 1055.16 - Mean Loss 1.793 - Mean Q Value 17.554 - \n",
      "Episode 732 - Step 771768 - Epsilon 0.8245297755723716 - \n",
      "Mean Reward 1635.41 - Mean Length 1054.36 - Mean Loss 1.791 - Mean Q Value 17.552 - \n",
      "Episode 733 - Step 773288 - Epsilon 0.8242165137419724 - \n",
      "Mean Reward 1633.34 - Mean Length 1046.73 - Mean Loss 1.787 - Mean Q Value 17.53 - \n",
      "Episode 734 - Step 774041 - Epsilon 0.8240613695672495 - \n",
      "Mean Reward 1638.71 - Mean Length 1051.48 - Mean Loss 1.785 - Mean Q Value 17.518 - \n",
      "Episode 735 - Step 775807 - Epsilon 0.8236976267292074 - \n",
      "Mean Reward 1643.09 - Mean Length 1063.97 - Mean Loss 1.779 - Mean Q Value 17.481 - \n",
      "Episode 736 - Step 776045 - Epsilon 0.823648618172303 - \n",
      "Mean Reward 1633.23 - Mean Length 1043.05 - Mean Loss 1.774 - Mean Q Value 17.444 - \n",
      "Episode 737 - Step 776847 - Epsilon 0.8234834931580319 - \n",
      "Mean Reward 1630.66 - Mean Length 1015.73 - Mean Loss 1.773 - Mean Q Value 17.431 - \n",
      "Episode 738 - Step 778945 - Epsilon 0.8230516892623053 - \n",
      "Mean Reward 1625.92 - Mean Length 1029.68 - Mean Loss 1.772 - Mean Q Value 17.435 - \n",
      "Episode 739 - Step 780119 - Epsilon 0.8228101590075813 - \n",
      "Mean Reward 1631.58 - Mean Length 1036.27 - Mean Loss 1.772 - Mean Q Value 17.436 - \n",
      "Episode 740 - Step 781016 - Epsilon 0.8226256644935576 - \n",
      "Mean Reward 1638.4 - Mean Length 1043.16 - Mean Loss 1.771 - Mean Q Value 17.463 - \n",
      "Episode 741 - Step 781992 - Epsilon 0.8224249682922334 - \n",
      "Mean Reward 1633.18 - Mean Length 1031.9 - Mean Loss 1.769 - Mean Q Value 17.459 - \n",
      "Episode 742 - Step 784658 - Epsilon 0.8218770046114744 - \n",
      "Mean Reward 1630.6 - Mean Length 1056.02 - Mean Loss 1.767 - Mean Q Value 17.453 - \n",
      "Episode 743 - Step 785070 - Epsilon 0.8217923556289043 - \n",
      "Mean Reward 1629.02 - Mean Length 1053.76 - Mean Loss 1.762 - Mean Q Value 17.419 - \n",
      "Episode 744 - Step 785318 - Epsilon 0.8217414060759318 - \n",
      "Mean Reward 1627.91 - Mean Length 1039.51 - Mean Loss 1.759 - Mean Q Value 17.43 - \n",
      "Episode 745 - Step 787394 - Epsilon 0.8213150328862349 - \n",
      "Mean Reward 1633.69 - Mean Length 1058.14 - Mean Loss 1.756 - Mean Q Value 17.422 - \n",
      "Episode 746 - Step 787962 - Epsilon 0.8211984144170793 - \n",
      "Mean Reward 1636.73 - Mean Length 1056.13 - Mean Loss 1.752 - Mean Q Value 17.383 - \n",
      "Episode 747 - Step 788261 - Epsilon 0.8211370321221122 - \n",
      "Mean Reward 1636.69 - Mean Length 1055.68 - Mean Loss 1.747 - Mean Q Value 17.34 - \n",
      "Episode 748 - Step 788388 - Epsilon 0.8211109614319543 - \n",
      "Mean Reward 1622.94 - Mean Length 1034.2 - Mean Loss 1.744 - Mean Q Value 17.293 - \n",
      "Episode 749 - Step 789899 - Epsilon 0.8208008453043362 - \n",
      "Mean Reward 1628.99 - Mean Length 1045.52 - Mean Loss 1.741 - Mean Q Value 17.246 - \n",
      "Episode 750 - Step 790367 - Epsilon 0.8207048172111715 - \n",
      "Mean Reward 1627.56 - Mean Length 1035.27 - Mean Loss 1.738 - Mean Q Value 17.196 - \n",
      "Episode 751 - Step 790766 - Epsilon 0.8206229559783048 - \n",
      "Mean Reward 1631.72 - Mean Length 1016.88 - Mean Loss 1.738 - Mean Q Value 17.187 - \n",
      "Episode 752 - Step 792735 - Epsilon 0.8202191036839872 - \n",
      "Mean Reward 1638.95 - Mean Length 1031.77 - Mean Loss 1.74 - Mean Q Value 17.192 - \n",
      "Episode 753 - Step 795193 - Epsilon 0.8197152338115925 - \n",
      "Mean Reward 1639.25 - Mean Length 1049.85 - Mean Loss 1.74 - Mean Q Value 17.198 - \n",
      "Episode 754 - Step 795876 - Epsilon 0.8195752793668557 - \n",
      "Mean Reward 1644.38 - Mean Length 1053.06 - Mean Loss 1.736 - Mean Q Value 17.167 - \n",
      "Episode 755 - Step 796655 - Epsilon 0.8194156826024662 - \n",
      "Mean Reward 1646.36 - Mean Length 1048.86 - Mean Loss 1.734 - Mean Q Value 17.136 - \n",
      "Episode 756 - Step 797089 - Epsilon 0.8193267808127886 - \n",
      "Mean Reward 1640.98 - Mean Length 1041.8 - Mean Loss 1.733 - Mean Q Value 17.143 - \n",
      "Episode 757 - Step 797754 - Epsilon 0.8191905790405207 - \n",
      "Mean Reward 1640.87 - Mean Length 1045.6 - Mean Loss 1.732 - Mean Q Value 17.171 - \n",
      "Episode 758 - Step 798401 - Epsilon 0.8190580856634712 - \n",
      "Mean Reward 1643.28 - Mean Length 1048.05 - Mean Loss 1.731 - Mean Q Value 17.169 - \n",
      "Episode 759 - Step 798703 - Epsilon 0.8189962491046243 - \n",
      "Mean Reward 1639.72 - Mean Length 1037.76 - Mean Loss 1.729 - Mean Q Value 17.132 - \n",
      "Episode 760 - Step 799477 - Epsilon 0.8188377886421362 - \n",
      "Mean Reward 1648.43 - Mean Length 1034.24 - Mean Loss 1.727 - Mean Q Value 17.131 - \n",
      "Episode 761 - Step 799746 - Epsilon 0.8187827236455405 - \n",
      "Mean Reward 1640.28 - Mean Length 1032.82 - Mean Loss 1.731 - Mean Q Value 17.157 - \n",
      "Episode 762 - Step 801116 - Epsilon 0.8185023385462941 - \n",
      "Mean Reward 1640.6 - Mean Length 1026.1 - Mean Loss 1.733 - Mean Q Value 17.16 - \n",
      "Episode 763 - Step 801375 - Epsilon 0.8184493422290149 - \n",
      "Mean Reward 1644.06 - Mean Length 1027.4 - Mean Loss 1.729 - Mean Q Value 17.14 - \n",
      "Episode 764 - Step 803430 - Epsilon 0.8180289718187345 - \n",
      "Mean Reward 1649.36 - Mean Length 1043.65 - Mean Loss 1.728 - Mean Q Value 17.113 - \n",
      "Episode 765 - Step 803962 - Epsilon 0.817920181186606 - \n",
      "Mean Reward 1650.47 - Mean Length 1041.87 - Mean Loss 1.725 - Mean Q Value 17.086 - \n",
      "Episode 766 - Step 804259 - Epsilon 0.8178594528601207 - \n",
      "Mean Reward 1645.67 - Mean Length 1038.41 - Mean Loss 1.725 - Mean Q Value 17.082 - \n",
      "Episode 767 - Step 804549 - Epsilon 0.8178001601917537 - \n",
      "Mean Reward 1638.76 - Mean Length 1022.23 - Mean Loss 1.725 - Mean Q Value 17.089 - \n",
      "Episode 768 - Step 806821 - Epsilon 0.817335781538675 - \n",
      "Mean Reward 1637.91 - Mean Length 1036.46 - Mean Loss 1.723 - Mean Q Value 17.091 - \n",
      "Episode 769 - Step 807173 - Epsilon 0.8172638591455312 - \n",
      "Mean Reward 1636.18 - Mean Length 1032.48 - Mean Loss 1.721 - Mean Q Value 17.093 - \n",
      "Episode 770 - Step 808377 - Epsilon 0.8170178997119035 - \n",
      "Mean Reward 1639.44 - Mean Length 1032.56 - Mean Loss 1.721 - Mean Q Value 17.102 - \n",
      "Episode 771 - Step 809342 - Epsilon 0.816820817892886 - \n",
      "Mean Reward 1648.94 - Mean Length 1039.72 - Mean Loss 1.718 - Mean Q Value 17.081 - \n",
      "Episode 772 - Step 810611 - Epsilon 0.8165617225571583 - \n",
      "Mean Reward 1643.02 - Mean Length 1045.2 - Mean Loss 1.712 - Mean Q Value 17.019 - \n",
      "Episode 773 - Step 810917 - Epsilon 0.8164992579668673 - \n",
      "Mean Reward 1638.7 - Mean Length 1035.68 - Mean Loss 1.707 - Mean Q Value 16.97 - \n",
      "Episode 774 - Step 811319 - Epsilon 0.8164172039044588 - \n",
      "Mean Reward 1639.09 - Mean Length 1033.97 - Mean Loss 1.704 - Mean Q Value 16.941 - \n",
      "Episode 775 - Step 812930 - Epsilon 0.8160884580400882 - \n",
      "Mean Reward 1642.89 - Mean Length 1039.54 - Mean Loss 1.7 - Mean Q Value 16.912 - \n",
      "Episode 776 - Step 813369 - Epsilon 0.81599889723535 - \n",
      "Mean Reward 1637.53 - Mean Length 1032.54 - Mean Loss 1.699 - Mean Q Value 16.898 - \n",
      "Episode 777 - Step 815085 - Epsilon 0.8156489087425369 - \n",
      "Mean Reward 1640.9 - Mean Length 1036.87 - Mean Loss 1.696 - Mean Q Value 16.886 - \n",
      "Episode 778 - Step 815898 - Epsilon 0.8154831449274104 - \n",
      "Mean Reward 1632.64 - Mean Length 1031.98 - Mean Loss 1.695 - Mean Q Value 16.861 - \n",
      "Episode 779 - Step 817576 - Epsilon 0.8151411214497511 - \n",
      "Mean Reward 1646.68 - Mean Length 1038.49 - Mean Loss 1.693 - Mean Q Value 16.841 - \n",
      "Episode 780 - Step 818717 - Epsilon 0.8149086355756384 - \n",
      "Mean Reward 1655.32 - Mean Length 1047.09 - Mean Loss 1.692 - Mean Q Value 16.814 - \n",
      "Episode 781 - Step 819873 - Epsilon 0.8146731609782081 - \n",
      "Mean Reward 1651.62 - Mean Length 1049.43 - Mean Loss 1.69 - Mean Q Value 16.785 - \n",
      "Episode 782 - Step 820165 - Epsilon 0.8146136920006588 - \n",
      "Mean Reward 1656.66 - Mean Length 1050.96 - Mean Loss 1.687 - Mean Q Value 16.754 - \n",
      "Episode 783 - Step 821523 - Epsilon 0.8143371775586054 - \n",
      "Mean Reward 1663.48 - Mean Length 1058.49 - Mean Loss 1.686 - Mean Q Value 16.73 - \n",
      "Episode 784 - Step 823124 - Epsilon 0.8140113042822464 - \n",
      "Mean Reward 1669.84 - Mean Length 1069.5 - Mean Loss 1.68 - Mean Q Value 16.689 - \n",
      "Episode 785 - Step 824392 - Epsilon 0.8137533035617778 - \n",
      "Mean Reward 1676.49 - Mean Length 1057.73 - Mean Loss 1.677 - Mean Q Value 16.644 - \n",
      "Episode 786 - Step 825902 - Epsilon 0.8134461696264256 - \n",
      "Mean Reward 1676.12 - Mean Length 1062.36 - Mean Loss 1.675 - Mean Q Value 16.628 - \n",
      "Episode 787 - Step 826850 - Epsilon 0.813253405703529 - \n",
      "Mean Reward 1675.13 - Mean Length 1068.19 - Mean Loss 1.671 - Mean Q Value 16.593 - \n",
      "Episode 788 - Step 830030 - Epsilon 0.8126071260953033 - \n",
      "Mean Reward 1680.05 - Mean Length 1067.92 - Mean Loss 1.667 - Mean Q Value 16.532 - \n",
      "Episode 789 - Step 830541 - Epsilon 0.8125033221525727 - \n",
      "Mean Reward 1683.37 - Mean Length 1059.1 - Mean Loss 1.665 - Mean Q Value 16.472 - \n",
      "Episode 790 - Step 830708 - Epsilon 0.8124694008427399 - \n",
      "Mean Reward 1672.79 - Mean Length 1028.82 - Mean Loss 1.666 - Mean Q Value 16.473 - \n",
      "Episode 791 - Step 831119 - Epsilon 0.8123859238900594 - \n",
      "Mean Reward 1673.05 - Mean Length 1028.54 - Mean Loss 1.666 - Mean Q Value 16.481 - \n",
      "Episode 792 - Step 831835 - Epsilon 0.8122405198055409 - \n",
      "Mean Reward 1671.5 - Mean Length 1017.81 - Mean Loss 1.669 - Mean Q Value 16.509 - \n",
      "Episode 793 - Step 832905 - Epsilon 0.8120232744971618 - \n",
      "Mean Reward 1673.83 - Mean Length 1011.25 - Mean Loss 1.675 - Mean Q Value 16.569 - \n",
      "Episode 794 - Step 834597 - Epsilon 0.8116798612461013 - \n",
      "Mean Reward 1679.01 - Mean Length 1025.19 - Mean Loss 1.677 - Mean Q Value 16.619 - \n",
      "Episode 795 - Step 834885 - Epsilon 0.8116214223926023 - \n",
      "Mean Reward 1682.29 - Mean Length 1026.6 - Mean Loss 1.68 - Mean Q Value 16.648 - \n",
      "Episode 796 - Step 835284 - Epsilon 0.8115404671832964 - \n",
      "Mean Reward 1677.29 - Mean Length 1018.12 - Mean Loss 1.683 - Mean Q Value 16.694 - \n",
      "Episode 797 - Step 835434 - Epsilon 0.8115100349825761 - \n",
      "Mean Reward 1666.42 - Mean Length 1010.35 - Mean Loss 1.686 - Mean Q Value 16.731 - \n",
      "Episode 798 - Step 836754 - Epsilon 0.811242280819392 - \n",
      "Mean Reward 1680.06 - Mean Length 989.66 - Mean Loss 1.689 - Mean Q Value 16.781 - \n",
      "Episode 799 - Step 838296 - Epsilon 0.8109296071527292 - \n",
      "Mean Reward 1678.33 - Mean Length 999.94 - Mean Loss 1.694 - Mean Q Value 16.851 - \n",
      "Episode 800 - Step 839538 - Epsilon 0.8106778525651273 - \n",
      "Mean Reward 1676.44 - Mean Length 1008.04 - Mean Loss 1.698 - Mean Q Value 16.929 - \n",
      "Episode 801 - Step 839662 - Epsilon 0.8106527219380796 - \n",
      "Mean Reward 1665.39 - Mean Length 1002.16 - Mean Loss 1.705 - Mean Q Value 17.007 - \n",
      "Episode 802 - Step 841074 - Epsilon 0.8103666119928089 - \n",
      "Mean Reward 1662.4 - Mean Length 1003.11 - Mean Loss 1.709 - Mean Q Value 17.061 - \n",
      "Episode 803 - Step 842046 - Epsilon 0.8101697168051895 - \n",
      "Mean Reward 1653.4 - Mean Length 1006.33 - Mean Loss 1.71 - Mean Q Value 17.076 - \n",
      "Episode 804 - Step 843738 - Epsilon 0.8098270874433231 - \n",
      "Mean Reward 1663.27 - Mean Length 1021.7 - Mean Loss 1.708 - Mean Q Value 17.07 - \n",
      "Episode 805 - Step 844869 - Epsilon 0.8095981411745022 - \n",
      "Mean Reward 1659.22 - Mean Length 1021.84 - Mean Loss 1.706 - Mean Q Value 17.071 - \n",
      "Episode 806 - Step 847030 - Epsilon 0.8091608738514857 - \n",
      "Mean Reward 1666.42 - Mean Length 1026.77 - Mean Loss 1.705 - Mean Q Value 17.056 - \n",
      "Episode 807 - Step 847245 - Epsilon 0.8091173826179114 - \n",
      "Mean Reward 1659.95 - Mean Length 1018.75 - Mean Loss 1.702 - Mean Q Value 17.005 - \n",
      "Episode 808 - Step 847628 - Epsilon 0.8090399133277328 - \n",
      "Mean Reward 1662.58 - Mean Length 1016.73 - Mean Loss 1.7 - Mean Q Value 16.982 - \n",
      "Episode 809 - Step 849421 - Epsilon 0.8086773424084891 - \n",
      "Mean Reward 1662.5 - Mean Length 1025.46 - Mean Loss 1.698 - Mean Q Value 16.945 - \n",
      "Episode 810 - Step 850478 - Epsilon 0.8084636776257226 - \n",
      "Mean Reward 1686.49 - Mean Length 1034.83 - Mean Loss 1.697 - Mean Q Value 16.887 - \n",
      "Episode 811 - Step 851551 - Epsilon 0.8082468363022061 - \n",
      "Mean Reward 1689.02 - Mean Length 1031.43 - Mean Loss 1.693 - Mean Q Value 16.848 - \n",
      "Episode 812 - Step 852228 - Epsilon 0.8081100520837328 - \n",
      "Mean Reward 1689.59 - Mean Length 1029.07 - Mean Loss 1.691 - Mean Q Value 16.822 - \n",
      "Episode 813 - Step 853581 - Epsilon 0.807836755048382 - \n",
      "Mean Reward 1674.53 - Mean Length 1015.39 - Mean Loss 1.691 - Mean Q Value 16.817 - \n",
      "Episode 814 - Step 854064 - Epsilon 0.807739214637125 - \n",
      "Mean Reward 1682.47 - Mean Length 1009.98 - Mean Loss 1.691 - Mean Q Value 16.83 - \n",
      "Episode 815 - Step 854287 - Epsilon 0.8076941844255027 - \n",
      "Mean Reward 1678.48 - Mean Length 1009.75 - Mean Loss 1.69 - Mean Q Value 16.837 - \n",
      "Episode 816 - Step 855477 - Epsilon 0.8074539311150227 - \n",
      "Mean Reward 1678.6 - Mean Length 1015.16 - Mean Loss 1.689 - Mean Q Value 16.857 - \n",
      "Episode 817 - Step 856387 - Epsilon 0.8072702562165195 - \n",
      "Mean Reward 1686.97 - Mean Length 1021.72 - Mean Loss 1.69 - Mean Q Value 16.87 - \n",
      "Episode 818 - Step 856809 - Epsilon 0.8071850936862339 - \n",
      "Mean Reward 1682.9 - Mean Length 1019.31 - Mean Loss 1.696 - Mean Q Value 16.903 - \n",
      "Episode 819 - Step 857468 - Epsilon 0.8070521208793441 - \n",
      "Mean Reward 1679.52 - Mean Length 1020.69 - Mean Loss 1.694 - Mean Q Value 16.911 - \n",
      "Episode 820 - Step 857964 - Epsilon 0.8069520526081937 - \n",
      "Mean Reward 1671.32 - Mean Length 1018.21 - Mean Loss 1.694 - Mean Q Value 16.914 - \n",
      "Episode 821 - Step 858183 - Epsilon 0.8069078731872069 - \n",
      "Mean Reward 1660.64 - Mean Length 1013.49 - Mean Loss 1.695 - Mean Q Value 16.913 - \n",
      "Episode 822 - Step 858540 - Epsilon 0.8068358598641554 - \n",
      "Mean Reward 1662.12 - Mean Length 1013.23 - Mean Loss 1.695 - Mean Q Value 16.923 - \n",
      "Episode 823 - Step 859558 - Epsilon 0.8066305462393942 - \n",
      "Mean Reward 1659.95 - Mean Length 1017.83 - Mean Loss 1.695 - Mean Q Value 16.927 - \n",
      "Episode 824 - Step 859935 - Epsilon 0.8065545248834605 - \n",
      "Mean Reward 1656.38 - Mean Length 993.23 - Mean Loss 1.697 - Mean Q Value 16.939 - \n",
      "Episode 825 - Step 863314 - Epsilon 0.8058734755620492 - \n",
      "Mean Reward 1653.15 - Mean Length 1019.35 - Mean Loss 1.698 - Mean Q Value 16.945 - \n",
      "Episode 826 - Step 863741 - Epsilon 0.8057874531492966 - \n",
      "Mean Reward 1652.85 - Mean Length 1020.44 - Mean Loss 1.696 - Mean Q Value 16.906 - \n",
      "Episode 827 - Step 865977 - Epsilon 0.8053371437798293 - \n",
      "Mean Reward 1656.86 - Mean Length 1020.56 - Mean Loss 1.691 - Mean Q Value 16.853 - \n",
      "Episode 828 - Step 868978 - Epsilon 0.8047331661075816 - \n",
      "Mean Reward 1657.42 - Mean Length 1047.18 - Mean Loss 1.686 - Mean Q Value 16.789 - \n",
      "Episode 829 - Step 869485 - Epsilon 0.8046311726299895 - \n",
      "Mean Reward 1653.92 - Mean Length 1043.09 - Mean Loss 1.675 - Mean Q Value 16.709 - \n",
      "Episode 830 - Step 869945 - Epsilon 0.8045386453539789 - \n",
      "Mean Reward 1653.74 - Mean Length 1008.44 - Mean Loss 1.672 - Mean Q Value 16.672 - \n",
      "Episode 831 - Step 871280 - Epsilon 0.8042701753509192 - \n",
      "Mean Reward 1653.83 - Mean Length 999.89 - Mean Loss 1.67 - Mean Q Value 16.653 - \n",
      "Episode 832 - Step 872171 - Epsilon 0.8040910440984272 - \n",
      "Mean Reward 1659.63 - Mean Length 1004.03 - Mean Loss 1.669 - Mean Q Value 16.645 - \n",
      "Episode 833 - Step 873258 - Epsilon 0.8038725620174498 - \n",
      "Mean Reward 1660.38 - Mean Length 999.7 - Mean Loss 1.672 - Mean Q Value 16.664 - \n",
      "Episode 834 - Step 874392 - Epsilon 0.803644696419076 - \n",
      "Mean Reward 1661.85 - Mean Length 1003.51 - Mean Loss 1.673 - Mean Q Value 16.675 - \n",
      "Episode 835 - Step 876050 - Epsilon 0.803311654678403 - \n",
      "Mean Reward 1659.17 - Mean Length 1002.43 - Mean Loss 1.676 - Mean Q Value 16.703 - \n",
      "Episode 836 - Step 876449 - Epsilon 0.80323152832719 - \n",
      "Mean Reward 1666.77 - Mean Length 1004.04 - Mean Loss 1.68 - Mean Q Value 16.748 - \n",
      "Episode 837 - Step 877611 - Epsilon 0.802998223428192 - \n",
      "Mean Reward 1662.65 - Mean Length 1007.64 - Mean Loss 1.685 - Mean Q Value 16.782 - \n",
      "Episode 838 - Step 877978 - Epsilon 0.8029245517117154 - \n",
      "Mean Reward 1666.03 - Mean Length 990.33 - Mean Loss 1.689 - Mean Q Value 16.819 - \n",
      "Episode 839 - Step 878124 - Epsilon 0.8028952454967544 - \n",
      "Mean Reward 1651.41 - Mean Length 980.05 - Mean Loss 1.695 - Mean Q Value 16.876 - \n",
      "Episode 840 - Step 879305 - Epsilon 0.802658225637639 - \n",
      "Mean Reward 1646.77 - Mean Length 982.89 - Mean Loss 1.699 - Mean Q Value 16.915 - \n",
      "Episode 841 - Step 881245 - Epsilon 0.802269030736894 - \n",
      "Mean Reward 1650.48 - Mean Length 992.53 - Mean Loss 1.704 - Mean Q Value 16.943 - \n",
      "Episode 842 - Step 884338 - Epsilon 0.8016489159140671 - \n",
      "Mean Reward 1661.3 - Mean Length 996.8 - Mean Loss 1.707 - Mean Q Value 16.962 - \n",
      "Episode 843 - Step 884741 - Epsilon 0.8015681538441393 - \n",
      "Mean Reward 1665.85 - Mean Length 996.71 - Mean Loss 1.711 - Mean Q Value 16.991 - \n",
      "Episode 844 - Step 885379 - Epsilon 0.8014403139031099 - \n",
      "Mean Reward 1670.76 - Mean Length 1000.61 - Mean Loss 1.713 - Mean Q Value 16.993 - \n",
      "Episode 845 - Step 886794 - Epsilon 0.8011568544964336 - \n",
      "Mean Reward 1672.77 - Mean Length 994.0 - Mean Loss 1.716 - Mean Q Value 17.013 - \n",
      "Episode 846 - Step 887435 - Epsilon 0.8010284793807676 - \n",
      "Mean Reward 1673.56 - Mean Length 994.73 - Mean Loss 1.718 - Mean Q Value 17.047 - \n",
      "Episode 847 - Step 888806 - Epsilon 0.8007539738811756 - \n",
      "Mean Reward 1675.58 - Mean Length 1005.45 - Mean Loss 1.723 - Mean Q Value 17.067 - \n",
      "Episode 848 - Step 890865 - Epsilon 0.80034189179028 - \n",
      "Mean Reward 1689.22 - Mean Length 1024.77 - Mean Loss 1.723 - Mean Q Value 17.062 - \n",
      "Episode 849 - Step 891581 - Epsilon 0.8001986433948383 - \n",
      "Mean Reward 1685.16 - Mean Length 1016.82 - Mean Loss 1.724 - Mean Q Value 17.047 - \n",
      "Episode 850 - Step 891839 - Epsilon 0.8001470322403583 - \n",
      "Mean Reward 1677.47 - Mean Length 1014.72 - Mean Loss 1.726 - Mean Q Value 17.049 - \n",
      "Episode 851 - Step 893052 - Epsilon 0.7999044244097424 - \n",
      "Mean Reward 1684.31 - Mean Length 1022.86 - Mean Loss 1.725 - Mean Q Value 17.036 - \n",
      "Episode 852 - Step 893361 - Epsilon 0.7998426341719029 - \n",
      "Mean Reward 1676.75 - Mean Length 1006.26 - Mean Loss 1.727 - Mean Q Value 17.043 - \n",
      "Episode 853 - Step 893689 - Epsilon 0.7997770497566913 - \n",
      "Mean Reward 1677.26 - Mean Length 984.96 - Mean Loss 1.73 - Mean Q Value 17.04 - \n",
      "Episode 854 - Step 894311 - Epsilon 0.799652694078797 - \n",
      "Mean Reward 1670.08 - Mean Length 984.35 - Mean Loss 1.734 - Mean Q Value 17.089 - \n",
      "Episode 855 - Step 896475 - Epsilon 0.7992201989176625 - \n",
      "Mean Reward 1668.26 - Mean Length 998.2 - Mean Loss 1.734 - Mean Q Value 17.118 - \n",
      "Episode 856 - Step 896839 - Epsilon 0.7991474731795312 - \n",
      "Mean Reward 1669.19 - Mean Length 997.5 - Mean Loss 1.738 - Mean Q Value 17.146 - \n",
      "Episode 857 - Step 899333 - Epsilon 0.7986493599705997 - \n",
      "Mean Reward 1673.57 - Mean Length 1015.79 - Mean Loss 1.739 - Mean Q Value 17.136 - \n",
      "Episode 858 - Step 900020 - Epsilon 0.7985122037044927 - \n",
      "Mean Reward 1679.94 - Mean Length 1016.19 - Mean Loss 1.738 - Mean Q Value 17.121 - \n",
      "Episode 859 - Step 900714 - Epsilon 0.7983736738376261 - \n",
      "Mean Reward 1682.9 - Mean Length 1020.11 - Mean Loss 1.742 - Mean Q Value 17.136 - \n",
      "Episode 860 - Step 901030 - Epsilon 0.7983106048007607 - \n",
      "Mean Reward 1679.18 - Mean Length 1015.53 - Mean Loss 1.746 - Mean Q Value 17.14 - \n",
      "Episode 861 - Step 902648 - Epsilon 0.7979877534218108 - \n",
      "Mean Reward 1682.71 - Mean Length 1029.02 - Mean Loss 1.744 - Mean Q Value 17.118 - \n",
      "Episode 862 - Step 903137 - Epsilon 0.7978902053694942 - \n",
      "Mean Reward 1674.44 - Mean Length 1020.21 - Mean Loss 1.743 - Mean Q Value 17.095 - \n",
      "Episode 863 - Step 904058 - Epsilon 0.7977065122752 - \n",
      "Mean Reward 1685.1 - Mean Length 1026.83 - Mean Loss 1.745 - Mean Q Value 17.074 - \n",
      "Episode 864 - Step 904206 - Epsilon 0.7976769976765758 - \n",
      "Mean Reward 1671.37 - Mean Length 1007.76 - Mean Loss 1.746 - Mean Q Value 17.036 - \n",
      "Episode 865 - Step 905400 - Epsilon 0.7974389265968533 - \n",
      "Mean Reward 1673.37 - Mean Length 1014.38 - Mean Loss 1.743 - Mean Q Value 16.993 - \n",
      "Episode 866 - Step 906217 - Epsilon 0.7972760663083885 - \n",
      "Mean Reward 1672.15 - Mean Length 1019.58 - Mean Loss 1.742 - Mean Q Value 16.951 - \n"
     ]
    }
   ],
   "source": [
    "use_cuda = torch.cuda.is_available()\n",
    "print(f\"Using CUDA: {use_cuda}\")\n",
    "print()\n",
    "\n",
    "save_dir = Path(\"checkpoints\") / datetime.datetime.now().strftime(\"%Y-%m-%dT%H-%M-%S\")\n",
    "save_dir.mkdir(parents=True)\n",
    "\n",
    "mario = Mario(state_dim=(4, 84, 84), action_dim=env.action_space.n, save_dir=save_dir)\n",
    "\n",
    "logger = MetricLogger(save_dir)\n",
    "\n",
    "episodes = 300000\n",
    "for e in range(episodes):\n",
    "\n",
    "    state = env.reset()\n",
    "\n",
    "    # Play the game!\n",
    "    while True:\n",
    "\n",
    "        # Run agent on the state\n",
    "        action = mario.act(state)\n",
    "\n",
    "        # Agent performs action\n",
    "        next_state, reward, done, info = env.step(action)\n",
    "\n",
    "        # Remember\n",
    "        mario.cache(state, next_state, action, reward, done)\n",
    "\n",
    "        # Learn\n",
    "        q, loss = mario.learn()\n",
    "        # Logging\n",
    "        logger.log_step(reward, loss, q)\n",
    "\n",
    "        # Update state\n",
    "        state = next_state\n",
    "\n",
    "        # Check if end of game\n",
    "        if done or info[\"flag_get\"]:\n",
    "            break\n",
    "\n",
    "    logger.log_episode()\n",
    "    logger.record(episode=e, epsilon=mario.exploration_rate, step=mario.curr_step)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "Copy of Copy of Super_Mario_Bros_DQN.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
